{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QnA synthetic dataset from a Complex PDF using Azure AI Document Intelligence\n",
    "\n",
    "### Overview\n",
    "\n",
    "We process the PDF by dividing it into three parts.\n",
    "\n",
    "-   **Text-heavy** - Text-heavy PDF can be processed with open source without the need to use toolkits like Azure AI Document Intelligence.\n",
    "-   **Image-heavy** - Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "-   **Mixed** - After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n",
    "\n",
    "![summary](../imgs/summary-creating-qna-pdf.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: pythonundefinedundefinedundefinedjvsc74a57bd02139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "# auto-reload all the modules. Great way to make writing and testing your modules much easier. Refer to: https://nocomplexity.com/documents/jupyterlab/tip-autoloadmodule.html\n",
    "import os, sys\n",
    "lab_prep_dir = os.getcwd().split(\"SLMWorkshopCN\")[0] + \"SLMWorkshopCN/0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(lab_prep_dir))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aoai_api_endpoint: https://wstus2aisrv.cognitiveservices.azure.com/\n",
      "aoai_api_version: 2024-08-01-preview\n",
      "aoai_deployment_name: gpt-4o\n",
      "doc_intelligence_endpoint: https://wstus2docintel.cognitiveservices.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()#ç›¸å…³æœºå¯†é…ç½®å†™åœ¨.envä¸­ï¼Œå¹¶.gitignoreæ‰.env\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\")\n",
    "\n",
    "if not aoai_api_version:\n",
    "    aoai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "if not aoai_deployment_name:\n",
    "    aoai_deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "    \n",
    "print(f\"aoai_api_endpoint: {aoai_api_endpoint}\")\n",
    "# print(f\"aoai_api_key: {aoai_api_key}\")\n",
    "print(f\"aoai_api_version: {aoai_api_version}\")\n",
    "print(f\"aoai_deployment_name: {aoai_deployment_name}\")\n",
    "print(f\"doc_intelligence_endpoint: {doc_intelligence_endpoint}\")\n",
    "# print(f\"doc_intelligence_key: {doc_intelligence_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read & Preprocess PDF file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the PDFs into individual pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Distributed training on Cloud, Language: English, Language Code: en\n"
     ]
    }
   ],
   "source": [
    "import shutil, random\n",
    "import openai\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from util.common_utils import get_language_code\n",
    "\n",
    "raw_data_dir = \"../raw_data\"\n",
    "splitted_raw_data_dir = \"splitted_raw_data\"\n",
    "file_path = f\"{raw_data_dir}/pdf/en-imagenet-training-wrote-by-daekeun.pdf\"\n",
    "\n",
    "DOMAIN = \"Distributed training on Cloud\"\n",
    "LANGUAGE = \"English\" # You can change your language here. e.g., \"Korean\", \"Japanese\", \"Chinese\"\n",
    "LANGUAGE_CODE = get_language_code(LANGUAGE) # ç”¨äºæ‰¾åˆ°ä¸åŒè¯­è¨€çš„prompt_template\n",
    "print(f\"Domain: {DOMAIN}, Language: {LANGUAGE}, Language Code: {LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Only use a poration of the PDF documents for testing. If there are a lot of pages or partial processing is required, cut and save only some pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import fitz #PyMuPDF\n",
    "\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(5, 25)] #ç¬¬5é¡µåˆ°æœ€å\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/part{idx}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinguish between pages composed mainly of text, pages composed primarily of images, and pages composed of mixed text/images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PDF Content Analysis Result:\n",
      "Text pages: [0, 1, 3, 5, 8]\n",
      "Mixed pages: [2, 4, 6, 7]\n",
      "splitted_raw_data\n"
     ]
    }
   ],
   "source": [
    "from util.common_utils import delete_folder_and_make_folder\n",
    "from util.preprocess import analyze_pdf_page_content, split_pdf\n",
    "\n",
    "analyzed_pdf_result = analyze_pdf_page_content(file_path)#åˆ¤æ–­Pdfæ¯é¡µçš„ç±»å‹:Text,Image,Mixedï¼ŒåŠå…¶å¯¹åº”é¡µç \n",
    "delete_folder_and_make_folder(splitted_raw_data_dir)    \n",
    "\n",
    "print(\"### PDF Content Analysis Result:\")\n",
    "for content_type, pages in analyzed_pdf_result.items():\n",
    "    print(f\"{content_type} pages: {pages}\")\n",
    "    split_pdf(file_path, f\"{splitted_raw_data_dir}/{content_type}.pdf\", pages)#æŒ‰Pdfæ¯é¡µçš„ç±»å‹ä¿å­˜ä¸­é—´ç»“æœä¾›åç»­ä½¿ç”¨\n",
    "print(splitted_raw_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(  # æ„é€ azure openai client\n",
    "        api_key=aoai_api_key,  \n",
    "        api_version=aoai_api_version,\n",
    "        base_url=f\"{aoai_api_endpoint}/openai/deployments/{aoai_deployment_name}\",\n",
    "        max_retries=1\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    document_intelligence_client = DocumentIntelligenceClient(  # æ„é€ azure doc intelligence client. å‚è€ƒï¼šhttps://github.com/microsoft/Form-Recognizer-Toolkit/blob/main/SampleCode/Python/sample_figure_understanding.ipynb\n",
    "        endpoint=doc_intelligence_endpoint, \n",
    "        credential=AzureKeyCredential(doc_intelligence_key),\n",
    "        headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Mixed page (Images and text mixed appropriately)\n",
    "\n",
    "After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Document\n",
    "\n",
    "Analyze the document with Azure AI Document Intelligence to extract the text and image information.\n",
    "\n",
    "-   `crop_image_from_file()`: Crop the image from the PDF file based on the bounding box.\n",
    "-   `is_bounding_box_larger_than()`: Check if the bounding box is larger than the threshold.\n",
    "-   `image_complexity()`: Check if the image is complex or simple based on image statistics.\n",
    "-   `understand_image_with_gpt():` Summarize the image with OpenAI GPT.\n",
    "\n",
    "![post-process](../imgs/post-process1.png)\n",
    "![post-process](../imgs/post-process2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures:\n",
      "5.958200000000001 3.0484\n",
      "\tDescription of figure 0: The image is a screenshot of a text box containing a sequence of shell commands to download the ImageNet dataset. Below it, there is a heading for an alternative method if the first method does not work.\n",
      "\n",
      "The shell commands in the text box are:\n",
      "```\n",
      "$ export IMAGENET_USERNAME=[YOUR_USERNAME]\n",
      "$ export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY]\n",
      "$ cd imagenet/data\n",
      "$ mv imagenet_2012_validation_synset_labels.txt synsets.txt\n",
      "$ nohup bash download_imagenet.sh synsets.txt >& download.log &\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    pdf_mixed_path = f\"{splitted_raw_data_dir}/Mixed.pdf\"\n",
    "\n",
    "    with open(pdf_mixed_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(# layoutåˆ†æçš„ç”¨æ³•å‚è€ƒ https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/prebuilt/layout?view=doc-intel-4.0.0&tabs=sample-code\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", \n",
    "            output_content_format=ContentFormat.MARKDOWN \n",
    "        )\n",
    "\n",
    "    result = poller.result()# azure dock intelligenceè¿”å›çš„å¯¹è±¡åŒ…å«å¾ˆå¤šæœ‰ç”¨å†…å®¹ï¼Œå‚è€ƒï¼šhttps://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept/analyze-document-response?view=doc-intel-3.1.0&source=recommendations\n",
    "    md_content = result.content\n",
    "\n",
    "    #### Updates the content of the figure description (empty content or caption) with the image summary text generated by gpt-4o.\n",
    "    from util.preprocess import (\n",
    "        image_complexity, is_bounding_box_larger_than, crop_image_from_file, \n",
    "        understand_image_with_gpt, update_figure_description\n",
    "    )\n",
    "    output_folder = \"pdf_mixed_tmp\"\n",
    "    delete_folder_and_make_folder(output_folder)\n",
    "    language = LANGUAGE\n",
    "    max_tokens = 1024\n",
    "    input_file_path = file_path\n",
    "\n",
    "    if result.figures:\n",
    "        print(\"Figures:\")\n",
    "        for idx, figure in enumerate(result.figures): # å¤„ç†azure doc intelligenceæ‰¾åˆ°çš„æ¯ä¸€å¼ å›¾ç‰‡\n",
    "            img_description = \"\"\n",
    "            #print(f\"Figure #{idx} has the following spans: {figure.spans}\")\n",
    "\n",
    "            # figure_content = \"\"\n",
    "            # for i, span in enumerate(figure.spans):# span: details the text spans related to the figure, specifying their offsets and lengths within the document's text. This connection helps in associating the figure with its relevant textual context\n",
    "            #     #print(f\"Span #{i}: {span}\")\n",
    "            #     figure_content += md_content[span.offset:span.offset + span.length] #å›¾ç‰‡åœ¨mdä¸­çš„ä½ç½®ï¼Œèµ·å§‹å­—ç¬¦ä¸‹è¡¨å’Œç»“æŸå­—ç¬¦ä¸‹æ ‡\n",
    "            # #print(f\"Original figure content in markdown: {figure_content}\")\n",
    "\n",
    "            # Note: figure bounding regions currently contain both the bounding region of figure caption and figure body\n",
    "            if figure.caption:# azure doc intelligenceåˆ†æå‡ºæ¥çš„figure captionæ˜¯pdfä¸Šå›¾ç‰‡é™„è¿‘çš„é‚£ä¸€å°ä¸²è¯´æ˜ï¼Œä¸ä¸€å®šæœ‰\n",
    "                caption_region = figure.caption.bounding_regions\n",
    "                #print(f\"\\tCaption: {figure.caption.content}\")\n",
    "                #print(f\"\\tCaption bounding region: {caption_region}\")\n",
    "                for region in figure.bounding_regions:# Each region specifies the page number (1-indexed) and bounding polygon. The bounding polygon is described as a sequence of points, clockwise from the left relative to the natural orientation of the element. For quadrilaterals, plot points are top-left, top-right, bottom-right, and bottom-left corners. Each point represents its x, y coordinate in the page unit specified by the unit property. In general, unit of measure for images is pixels while PDFs use inches.\n",
    "                    if region not in caption_region:# å¦‚æœå›¾ç‰‡ä¸æ˜¯åªæœ‰captionçš„è¯ï¼Œåˆ™éœ€è¦å¤„ç†å…¶å®ƒçœŸæ­£çš„å›¾ç‰‡\n",
    "                        #print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                        # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                        boundingbox = (\n",
    "                                region.polygon[0],  # x0 (left)\n",
    "                                region.polygon[1],  # y0 (top)\n",
    "                                region.polygon[4],  # x1 (right)\n",
    "                                region.polygon[5]   # y1 (bottom)\n",
    "                            )\n",
    "\n",
    "                        if is_bounding_box_larger_than(boundingbox):# å›¾åƒè¶³å¤Ÿå¤§(è¶…1x1è‹±å¯¸)æ‰ä¼šæˆªå–å‡ºæ¥å¤„ç†\n",
    "                            #print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "                            cropped_image = crop_image_from_file(pdf_mixed_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                            if image_complexity(cropped_image)[0] == \"Complex\":# å›¾ç‰‡çš„ç†µï¼Œæ¸…æ™°åº¦ï¼Œè¾¹ç¼˜æ•°éƒ½è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™å®šä¹‰ä¸ºå¤æ‚çš„ï¼Œä¿¡æ¯é‡å¤§çš„ï¼Œæœ‰æ„ä¹‰çš„å›¾ç‰‡ï¼Œåç»­éœ€è¦å¤„ç†\n",
    "                                # Get the base name of the file\n",
    "                                base_name = os.path.basename(input_file_path)\n",
    "                                # Remove the file extension\n",
    "                                file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                                output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                                cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "\n",
    "                                cropped_image.save(cropped_image_filename)\n",
    "                                print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                                try: # ç”¨gpt-4oç”Ÿæˆå›¾ç‰‡æè¿°. å¦‚æœazure doc intellegenceåœ¨å›¾ç‰‡é™„è¿‘æ‰¾åˆ°äº†captionï¼Œåˆ™ä½œä¸ºä¸Šä¸‹æ–‡ä¼ å…¥\n",
    "                                    image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, caption, max_tokens=max_tokens, language=language)\n",
    "                                except openai.BadRequestError as e:\n",
    "                                    print(f\"BadRequestError: {e}\")\n",
    "                                    image_summarization = \"\"\n",
    "                                img_description += image_summarization\n",
    "\n",
    "                                print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                            else:\n",
    "                                print(f'simple image at idx {idx}')\n",
    "\n",
    "            else:# å¦‚æœpdfä¸Šå›¾ç‰‡é™„è¿‘æ‰¾ä¸åˆ°ç›¸å…³captionè¯´æ˜\n",
    "                #print(\"\\tNo caption found for this figure.\")\n",
    "                for region in figure.bounding_regions:\n",
    "                    #print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    boundingbox = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "\n",
    "                    if is_bounding_box_larger_than(boundingbox):# å›¾åƒè¶³å¤Ÿå¤§(è¶…1x1è‹±å¯¸)æ‰ä¼šæˆªå–å‡ºæ¥å¤„ç†\n",
    "                        #print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "\n",
    "                        cropped_image = crop_image_from_file(input_file_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                        if image_complexity(cropped_image)[0] == \"Complex\":# å›¾ç‰‡çš„ç†µï¼Œæ¸…æ™°åº¦ï¼Œè¾¹ç¼˜æ•°éƒ½è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™å®šä¹‰ä¸ºå¤æ‚çš„ï¼Œä¿¡æ¯é‡å¤§çš„ï¼Œæœ‰æ„ä¹‰çš„å›¾ç‰‡ï¼Œåç»­éœ€è¦å¤„ç†\n",
    "                            # Get the base name of the file\n",
    "                            base_name = os.path.basename(input_file_path)\n",
    "                            # Remove the file extension\n",
    "                            file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                            output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                            cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "                            # cropped_image_filename = f\"data/cropped/image_{idx}.png\"\n",
    "                            cropped_image.save(cropped_image_filename)\n",
    "                            #print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                            try: # ç”¨gpt-4oç”Ÿæˆå›¾ç‰‡æè¿°\n",
    "                                image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                            except openai.BadRequestError as e:\n",
    "                                print(f\"BadRequestError: {e}\")\n",
    "                                image_summarization = \"\"\n",
    "                            img_description += image_summarization\n",
    "                            print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                        else:\n",
    "                            print(f'simple image at idx {idx}')\n",
    "\n",
    "            \n",
    "            md_content = update_figure_description(md_content, img_description, idx) # æŠŠazure gpt-4oç”Ÿæˆçš„å›¾ç‰‡æè¿°æ’å…¥åˆ°åŸæ–‡æ¡£ä¸­æ”¾ç½®å›¾ç‰‡(å›¾ç‰‡å ä½ç¬¦)çš„åœ°æ–¹ï¼Œæ­¤æ—¶è¿”å›çš„æ•´ä¸ªæ–‡æ¡£md_contentåœ¨åŸæ¥æœ‰å›¾ç‰‡çš„åœ°æ–¹å°±åŒ…å«äº†å›¾ç‰‡æè¿°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate chunks for mixed pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits (mixed case): 8\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import re\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(# æŒ‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§é€’å½’å¯¹æ–‡æ¡£è¿›è¡Œåˆ†éš”æˆchunkï¼Œä¹Ÿå¯è€ƒè™‘azure doc intelligenceçš„\n",
    "        separators=[\n",
    "            r'<!-- PageNumber=\"\\d+\" -->', # azure doc intelligence çš„é¡µåˆ†éš”ç¬¦\n",
    "            r\"\\n\\n\",\n",
    "            r\"\\n\",\n",
    "            \" \",\n",
    "            \".\",\n",
    "            \"\",\n",
    "        ],   \n",
    "        is_separator_regex = True,    \n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "\n",
    "    mixed_chunks = text_splitter.split_text(md_content) # æŒ‰ç…§ä¸Šè¿°é¡µï¼Œç« èŠ‚ï¼Œæ®µçš„åˆ†éš”ç¬¦æŠŠåŸæ–‡æ¡£åˆ†å‰²æˆå¤šä¸ªchunk\n",
    "else:\n",
    "    mixed_chunks = []\n",
    "print(\"Length of splits (mixed case): \" + str(len(mixed_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Text-heavy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "page_content='[Hands-on] Fast Training\n",
      "ImageNet on on-demand EC2\n",
      "GPU instances with Horovod\n",
      "ğŸ’»\n",
      "Author: Daekeun Kim (daekeun@amazon.com)\n",
      "Goal\n",
      "This document is for people who need distributed GPU training using Horovod for\n",
      "experimental purposes. Many steps are similar to what mentioned in Julien\n",
      "Simonâ€™s article(\n",
      ") and AWS\n",
      "Documentation(\n",
      "). So I recommend you to view these articles first. If there\n",
      "are some things that arenâ€™t going well (e.g., Downloading the dataset does not\n",
      "work, How to convert the raw data to the TFRecord feature set?, How to fix the\n",
      "error ModuleNotFoundError: No module named 'cv2'? ) please refer this\n",
      "document.\n",
      "https://medium.com/@julsimon/imagenet-part-1-going-on-an-\n",
      "adventure-c0a62976dc72\n",
      "https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-\n",
      "horovod-tensorflow.html\n",
      "Introduction\n",
      "For data preparation and data transformation, we do not need to use a GPU\n",
      "instance such as p2 and p3. Instead, we can start much cheaper instances like \n",
      "t2.large  instance with 1.0TB EBS volume.\n",
      "For distributed training, we need to use multiple GPU instances like p2, p3, g3 and\n",
      "g4.\n",
      "You can skip step 1 if you do not want to invent the wheel again because I have\n",
      "stored everything in my s3 bucket.' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 1\n",
      "page_content='g4.\n",
      "You can skip step 1 if you do not want to invent the wheel again because I have\n",
      "stored everything in my s3 bucket.\n",
      "24. 7. 22. ì˜¤ì „ 9:52\n",
      "[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\n",
      "https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\n",
      "1/9' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 2\n",
      "page_content='â€¢\n",
      "s3://dataset-image/imagenet/raw  (raw jpeg)\n",
      "â€¢\n",
      "s3://dataset-image/imagenet/tfrecord  (TFRecord before resizing)\n",
      "â€¢\n",
      "s3://dataset-image/imagenet/tfrecord-resized  (TFRecord after resizing to\n",
      "224x224)\n",
      "â€¢\n",
      "s3://dataset-image/imagenet/recordio  (RecordIO after resizing to\n",
      "256x256)\n",
      "â—¦The reason I did not resize 224x224 is that the below article shows\n",
      "different validation accuracy for resizing strategy.\n",
      "â—¦https://forums.fast.ai/t/impact-of-image-resizing-on-model-training-time-\n",
      "and-performance/1980\n",
      "Please let me know if you want to access the bucket because I did not grant any\n",
      "public access.\n",
      "Step 1. Downloading and Transformation\n",
      "Setting up an EC2 instance for Data Transformation\n",
      "â€¢ Create an EC2 instance for storing ImageNet dataset (Ubuntu 18.04 or 16.04.\n",
      "Linux is also available). t2.micro  is also available, but t2.large  is\n",
      "recommended due to memory size. Note that we do not need large storage\n",
      "size since we will make another EBS volume to attach the EC2 instance.\n",
      "â€¢ Create an EBS volume (1.0TB) for ImageNet dataset and then attach the\n",
      "volume it to your EC2 instance. ImageNet consists of 138GB for training set\n",
      "and 6.3GB for validation set, but we need an additional space since we need to' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 1, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Length of splits (text-heay case): 9\n"
     ]
    }
   ],
   "source": [
    "if \"Text\" in analyzed_pdf_result:\n",
    "    from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "    from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "    pdf_text_path = f\"{splitted_raw_data_dir}/Text.pdf\" # åŠ è½½åœ¨â€˜Split the PDFs into individual pagesâ€™ä¸­åˆ†å‰²å‡ºæ¥çš„æ–‡å­—ä¸ºä¸»çš„pdfé¡µ\n",
    "    loader = PyMuPDFLoader(pdf_text_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200, \n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    for idx, chunk in enumerate(text_chunks):\n",
    "        print(f\"Chunk {idx}\\n{chunk}\")\n",
    "        print(\"=\"*80)\n",
    "        if idx == 2:\n",
    "            break\n",
    "\n",
    "    text_chunks = [d.page_content for d in text_chunks]\n",
    "else:\n",
    "    text_chunks = []\n",
    "print(\"Length of splits (text-heay case): \" + str(len(text_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Image-heavy\n",
    "\n",
    "Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "\n",
    "### Preprocess Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "if \"Image\" in analyzed_pdf_result:\n",
    "    import fitz\n",
    "    from glob import glob\n",
    "\n",
    "    image_dir = \"./pdf_image_tmp\"\n",
    "    delete_folder_and_make_folder(image_dir) \n",
    "\n",
    "    pdf_image_path = f\"{splitted_raw_data_dir}/Image.pdf\" # åŠ è½½åœ¨â€˜Split the PDFs into individual pagesâ€™ä¸­åˆ†å‰²å‡ºæ¥çš„å›¾ç‰‡ä¸ºä¸»çš„pdfé¡µ\n",
    "    doc = fitz.open(pdf_image_path)\n",
    "    #clip_x, clip_y = 10, 45\n",
    "    clip_x, clip_y = 10, 10 # åŸºäºfitzåšçš„ä¸€äº›çŸ«æ­£\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        x, y, w, h = page.rect\n",
    "        clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "        page.set_cropbox(clip)\n",
    "        pix = page.get_pixmap()\n",
    "        pix.save(f\"{image_dir}/page_{i:03d}.jpg\")\n",
    "\n",
    "    images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "# å¯¹â€˜ä»å›¾ç‰‡ä¸ºä¸»çš„pdfé¡µâ€™æŠ å‡ºæ¥çš„å›¾ç‰‡ï¼Œç”¨azure gpt-4oè¿›è¡Œå›¾åƒæè¿°\n",
    "max_tokens = 1024\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name\n",
    ")\n",
    "\n",
    "human_prompt_main = f\"Given image, give a concise summary in {LANGUAGE}. Don't insert any XML tag such as <text> and </text> when answering.\"\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing image.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": human_prompt_main\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image_summaries (image-heavy case): 0\n",
      "CPU times: user 242 Î¼s, sys: 43 Î¼s, total: 285 Î¼s\n",
      "Wall time: 253 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if \"Image\" in analyzed_pdf_result:\n",
    "    from util.preprocess import encode_image_base64\n",
    "    #images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "    base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "    image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8})\n",
    "    image_summaries = remove_short_sentences(image_summaries)\n",
    "else:\n",
    "    image_summaries = []\n",
    "print(\"Length of image_summaries (image-heavy case): \" + str(len(image_summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct QnA Pairs\n",
    "\n",
    "---\n",
    "\n",
    "### Option 1.\n",
    "\n",
    "Leverage the `azure-ai-generative` package. The `QADataGenerator` class in this package makes it easy to generate QnA synthetic questions. However, using this class as is has the disadvantage of not being able to use custom prompts, so we inherited from it and created the `CustomQADataGenerator` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from util.qa import CustomQADataGenerator # ç»§æ‰¿è‡ªazure-ai-generativeçš„QADataGeneratorï¼Œå°è£…äº†å¦‚ä½•è·å–è‡ªå·±çš„prompt templateçš„é€»è¾‘\n",
    "model_config = {\n",
    "    \"deployment\": aoai_deployment_name,\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = CustomQADataGenerator(model_config=model_config, templates_dir=f\"./prompt_template/{LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "import os\n",
    "from azure.ai.generative.synthetic.qa import QAType\n",
    "concurrency = 6  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "#qa_type = QAType.CONVERSATION\n",
    "qa_type = QAType.LONG_ANSWER\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(# æ ¹æ® QAType.LONG_ANSWER å’Œ LANGUAGE_CODE ä¼šè·å–åˆ°æç¤ºè¯æ¨¡æ¿ 1_synthetic-qa-generation/seed/prompt_template/en/prompt_qa_long_answer.txt\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated QAs\n"
     ]
    }
   ],
   "source": [
    "input_batch = mixed_chunks + text_chunks + image_summaries # æœ‰å›¾ä¹Ÿæœ‰æ–‡å­—çš„pdfé¡µï¼Œæ–‡å­—ä¸ºä¸»çš„pdfé¡µï¼Œå›¾ç‰‡ä¸ºä¸»çš„pdfé¡µï¼Œè¿™äº›é¡µé¢çš„å†…å®¹éƒ½è¾“å…¥è®©azure gpt-4oç”Ÿæˆç›¸å…³çš„QAæ•°æ®\n",
    "results = await asyncio.gather(*[generate_async(text) for text in input_batch], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2.\n",
    "\n",
    "You write the entire sequence of code to create a QnA dataset without using azure-ai-generative but azure openai only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "# è¿™é‡Œæ¼”ç¤ºä¸€ä¸‹jsonæ ¼å¼çš„ç”Ÿæˆèƒ½åŠ›\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1024,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "prompt = get_qna_prompt_template(LANGUAGE) # å¦å¤–ä¸€ç§prompt templateï¼ŒåŒ…å«äº†contextï¼Œdomainå’Œnum_questionså˜é‡ï¼Œå¹¶ä¸”è¿”å›json\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "for doc in mixed_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in text_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in image_summaries:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'QUESTION': 'What is the recommended Python version to use for TensorFlow models repository according to the context?', 'ANSWER': 'The recommended Python version to use for TensorFlow models repository is Python2 because many codes of TensorFlow models repository do not work on Python3.'}, {'QUESTION': 'Why is it important to install OpenCV when working with ImageNet raw data?', 'ANSWER': 'It is important to install OpenCV because im2rec.py utilizes some OpenCV functions to convert ImageNet raw data to RecordIO files.'}, {'QUESTION': 'Where can you find instructions on how to mount an EBS volume on an EC2 instance?', 'ANSWER': 'Instructions on how to mount an EBS volume on an EC2 instance can be found at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html.'}], [{'QUESTION': 'What is the first step to download ImageNet according to the provided method?', 'ANSWER': 'The first step to download ImageNet according to the provided method is to go to http://www.image-net.org/, sign up, and get your own username and access key.'}, {'QUESTION': 'How long does it typically take to download ImageNet due to server instability?', 'ANSWER': 'It typically takes 4 to 5 days to download ImageNet due to server instability.'}, {'QUESTION': 'What should be the number of directories after extracting the training set?', 'ANSWER': 'After extracting the training set, the number of directories should be 1,000.'}], [{'QUESTION': \"What is the size of the 'ILSVRC2012_bbox_train_v2.tar.gz' file and when was it last modified?\", 'ANSWER': \"The 'ILSVRC2012_bbox_train_v2.tar.gz' file is 19,537,608 bytes in size and was last modified on October 1 at 00:01.\"}, {'QUESTION': 'Which file has the largest size in the provided context and what is its size?', 'ANSWER': \"The file 'ILSVRC2012_img_train.tar' has the largest size in the provided context, with a size of 147,897,477,120 bytes.\"}, {'QUESTION': 'What is the recommended image size for ImageNet baseline according to the context, and what size did Simon use?', 'ANSWER': 'The recommended image size for ImageNet baseline is 224x224, but Simon used 480x480.'}], {'QUESTION': \"What are the recommended EC2 instance types for distributed GPU training using Uber's Horovod or Tensorflow's DistributedStrategy?\", 'ANSWER': \"The recommended EC2 instance types for distributed GPU training using Uber's Horovod or Tensorflow's DistributedStrategy are p3.16xlarge or p3dn.24xlarge.\"}, [{'QUESTION': 'What instance type is used for distributed GPU training in the provided context?', 'ANSWER': 'The instance type used for distributed GPU training in the provided context is p3dn.24xlarge.'}, {'QUESTION': 'How many instances are shown in the example for distributed GPU training?', 'ANSWER': 'The example for distributed GPU training shows 8 instances.'}, {'QUESTION': 'Where can you find the remaining steps for setting up distributed training with Horovod and TensorFlow?', 'ANSWER': 'You can find the remaining steps for setting up distributed training with Horovod and TensorFlow at https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod-tensorflow.html.'}], [{'QUESTION': 'What should you do after training to verify the results?', 'ANSWER': 'After training, you should check the training log and evaluation log by checking the imagenet_resnet folder.'}, {'QUESTION': 'What is the purpose of the shell commands provided in the text box?', 'ANSWER': 'The purpose of the shell commands provided in the text box is to download the ImageNet dataset.'}, {'QUESTION': 'What should you do if the first method to download the ImageNet dataset does not work?', 'ANSWER': 'If the first method to download the ImageNet dataset does not work, you should refer to the alternative method mentioned below the shell commands.'}], {'QUESTION': \"What is the size of the file 'model.ckpt-20000.data-00001-of-00002'?\", 'ANSWER': \"The size of the file 'model.ckpt-20000.data-00001-of-00002' is 204668736 bytes.\"}, {'QUESTION': 'What is the learning rate at step 14000 in the vd_train_log?', 'ANSWER': 'The learning rate at step 14000 in the vd_train_log is 0.00012.'}, [{'QUESTION': 'What type of instances are recommended for data preparation and data transformation?', 'ANSWER': 'For data preparation and data transformation, it is recommended to use cheaper instances like t2.large with 1.0TB EBS volume.'}, {'QUESTION': 'Which instances are suggested for distributed GPU training?', 'ANSWER': 'For distributed GPU training, it is suggested to use multiple GPU instances like p2, p3, g3, and g4.'}, {'QUESTION': 'Where can you find additional information if you encounter issues such as downloading the dataset or converting raw data to the TFRecord feature set?', 'ANSWER': 'If you encounter issues such as downloading the dataset or converting raw data to the TFRecord feature set, you can refer to Julien Simonâ€™s article and AWS Documentation.'}], {'QUESTION': 'What is the purpose of the provided link in the context?', 'ANSWER': 'The provided link in the context is for a hands-on guide on fast training of ImageNet on on-demand EC2 GPU instances using Horovod.'}, [{'QUESTION': 'What is the recommended EC2 instance type for storing the ImageNet dataset and why?', 'ANSWER': 'The recommended EC2 instance type for storing the ImageNet dataset is t2.large due to its memory size.'}, {'QUESTION': 'Why is an EBS volume of 1.0TB recommended for the ImageNet dataset?', 'ANSWER': 'An EBS volume of 1.0TB is recommended for the ImageNet dataset because the training set consists of 138GB and the validation set consists of 6.3GB, and additional space is needed for data processing.'}, {'QUESTION': 'Why was the resizing strategy of 256x256 chosen for the RecordIO format instead of 224x224?', 'ANSWER': 'The resizing strategy of 256x256 was chosen for the RecordIO format instead of 224x224 because an article shows different validation accuracy for different resizing strategies.'}], {'QUESTION': 'What is the size of the training set and validation set in ImageNet?', 'ANSWER': 'The training set in ImageNet consists of 138GB and the validation set consists of 6.3GB.'}, [{'QUESTION': \"What is the first step to use TensorFlow's download script for ImageNet dataset?\", 'ANSWER': \"The first step to use TensorFlow's download script for the ImageNet dataset is to export your username and access key using the commands: $ export IMAGENET_USERNAME=[YOUR_USERNAME] and $ export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY].\"}, {'QUESTION': 'What should you do if Method 1 for downloading the ImageNet dataset does not work?', 'ANSWER': 'If Method 1 for downloading the ImageNet dataset does not work, you can use Method 2, which involves manually downloading the dataset using wget commands for different parts of the dataset, such as ILSVRC2012_img_train.tar and ILSVRC2012_bbox_train_v2.tar.'}, {'QUESTION': 'How do you extract the validation set after downloading it manually?', 'ANSWER': \"To extract the validation set after downloading it manually, you need to create a directory named 'validation', move the ILSVRC2012_img_val.tar file into this directory, navigate to the directory, and then extract the files using the command $ tar xf ILSVRC2012_img_val.tar.\"}], {'QUESTION': 'What is the first step to prepare the validation set for training ImageNet on EC2 GPU instances?', 'ANSWER': \"The first step is to create a directory named 'validation' and move the ILSVRC2012_img_val.tar file into it.\"}, {'QUESTION': \"What is the purpose of the script 'preprocess_imagenet.py' and how is it executed?\", 'ANSWER': \"The purpose of the script 'preprocess_imagenet.py' is to preprocess the ImageNet dataset. It is executed using the command: python preprocess_imagenet.py --local_scratch_dir=[YOUR DIRECTORY] --imagenet_username=[imagenet account] --imagenet_access_key=[imagenet access key].\"}, [{'QUESTION': 'What should be done after data transformation before training the ResNet-50 model?', 'ANSWER': 'After data transformation, a new bucket should be created and feature sets should be synced or copied to the bucket, and a snapshot of the EBS volume should be created.'}, {'QUESTION': 'Where can you find resources for training the ResNet-50 model on a single machine?', 'ANSWER': 'Resources for training the ResNet-50 model on a single machine can be found at the following links: https://medium.com/@julsimon/imagenet-part-2-the-road-goes-ever-on-and-on-578f09a749f9 and https://github.com/tensorflow/models/tree/master/official/r1/resnet.'}, {'QUESTION': 'What is the recommended resource for hands-on training of ImageNet on on-demand EC2 GPU instances with Horovod?', 'ANSWER': 'The recommended resource for hands-on training of ImageNet on on-demand EC2 GPU instances with Horovod is available at https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca.'}], [{'QUESTION': 'What is the learning rate (LR) at step 7000 in the hvd_train_log?', 'ANSWER': 'The learning rate (LR) at step 7000 in the hvd_train_log is 0.00023.'}, {'QUESTION': 'How many GPUs were used for training in the provided context?', 'ANSWER': '64 GPUs were used for training in the provided context.'}, {'QUESTION': 'What is the speed at step 50 in the hvd_train_log?', 'ANSWER': 'The speed at step 50 in the hvd_train_log is 43926.5.'}]]\n",
      "CPU times: user 366 ms, sys: 17.9 ms, total: 384 ms\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_pair = chain.batch(input_batch, {\"max_concurrency\": 5})\n",
    "print(qa_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to jsonl\n",
    "\n",
    "---\n",
    "\n",
    "If you want to augment dataset, you can try Evovle-Instruct or other data augmentation techniques.<br>\n",
    "Please refer to `../evolve-instruct` and `../glan-instruct` for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./dataset/imagenet-training-summary-oai.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = f\"\"\"You are the SME (Subject Matter Expert) in {DOMAIN}. Please answer the questions accurately. If the question is in {LANGUAGE}, write your answer in {LANGUAGE}.\"\"\"\n",
    "\n",
    "save_filename = \"imagenet-training-summary\"\n",
    "oai_qa_pair = convert_to_oai_format(question_answer_list, system_prompt_msg=system_prompt_msg)\n",
    "\n",
    "# save_result_jsonl = f\"{output_dir}/{save_filename}.jsonl\" # option 2\n",
    "save_result_jsonl = f\"{output_dir}/{save_filename}-oai.jsonl\" # option 1\n",
    "#save_jsonl(qa_pair, save_result_jsonl) # option 2\n",
    "save_jsonl(oai_qa_pair, save_result_jsonl) # option 1\n",
    "print(f\"Saved to {save_result_jsonl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf {splitted_raw_data_dir} pdf_image_tmp pdf_mixed_tmp outputs_tmp images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Synthetic Data\n",
    "\n",
    "---\n",
    "\n",
    "## Evolv-Instruct to generate more\n",
    "\n",
    "To generate more data from above seed data, we use [Evolve-Instruct](../evolve-instruct/README.md).\n",
    "![Evolv-Instruct](1_synthetic-qa-generation/evolve-instruct/evlvinstrc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EvolvMCTS4RL for RL\n",
    "\n",
    "To improve reasoning capability using RL (Reinforcement Learning), we use [EvolveMCTS4RL](../reasoningplaning/README.md) to generate data.\n",
    "\n",
    "\n",
    "![EvolvMCTS4RL](../reasoningplaning/evolveMCTS4RL.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
