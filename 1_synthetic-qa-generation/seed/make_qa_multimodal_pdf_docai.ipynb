{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QnA synthetic dataset from a Complex PDF using Azure AI Document Intelligence\n",
    "\n",
    "### Overview\n",
    "\n",
    "We process the PDF by dividing it into three parts.\n",
    "\n",
    "-   **Text-heavy** - Text-heavy PDF can be processed with open source without the need to use toolkits like Azure AI Document Intelligence.\n",
    "-   **Image-heavy** - Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "-   **Mixed** - After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n",
    "\n",
    "![summary](../imgs/summary-creating-qna-pdf.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Kernel: python31014jvsc74a57bd01f90a0206bde5cf3732dab79adbbcc7570d5fab64b89fc69d46a8fe33664a709\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "# auto-reload all the modules. Great way to make writing and testing your modules much easier. Refer to: https://nocomplexity.com/documents/jupyterlab/tip-autoloadmodule.html\n",
    "import os, sys\n",
    "lab_prep_dir = os.getcwd().split(\"SLMWorkshopCN\")[0] + \"SLMWorkshopCN/0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(lab_prep_dir))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aoai_api_endpoint: https://cog-pgwgybluulpec.openai.azure.com/\n",
      "aoai_api_version: 2024-07-01-preview\n",
      "aoai_deployment_name: gpt-4o\n",
      "doc_intelligence_endpoint: https://cog-di-pgwgybluulpec.cognitiveservices.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()#ç›¸å…³æœºå¯†é…ç½®å†™åœ¨.envä¸­ï¼Œå¹¶.gitignoreæ‰.env\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_GPT4V_DEPLOYMENT_NAME\")\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\")\n",
    "\n",
    "if not aoai_api_version:\n",
    "    aoai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "if not aoai_deployment_name:\n",
    "    aoai_deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "    \n",
    "print(f\"aoai_api_endpoint: {aoai_api_endpoint}\")\n",
    "# print(f\"aoai_api_key: {aoai_api_key}\")\n",
    "print(f\"aoai_api_version: {aoai_api_version}\")\n",
    "print(f\"aoai_deployment_name: {aoai_deployment_name}\")\n",
    "print(f\"doc_intelligence_endpoint: {doc_intelligence_endpoint}\")\n",
    "# print(f\"doc_intelligence_key: {doc_intelligence_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read & Preprocess PDF file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the PDFs into individual pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Distributed training on Cloud, Language: English, Language Code: en\n"
     ]
    }
   ],
   "source": [
    "import shutil, random\n",
    "import openai\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from util.common_utils import get_language_code\n",
    "\n",
    "raw_data_dir = \"../raw_data\"\n",
    "splitted_raw_data_dir = \"splitted_raw_data\"\n",
    "file_path = f\"{raw_data_dir}/pdf/en-imagenet-training-wrote-by-daekeun.pdf\"\n",
    "\n",
    "DOMAIN = \"Distributed training on Cloud\"\n",
    "LANGUAGE = \"English\" # You can change your language here. e.g., \"Korean\", \"Japanese\", \"Chinese\"\n",
    "LANGUAGE_CODE = get_language_code(LANGUAGE) # ç”¨äºæ‰¾åˆ°ä¸åŒè¯­è¨€çš„prompt_template(åœ¨1_synthetic-qa-generation/seed/prompt_template/ä¸­)\n",
    "print(f\"Domain: {DOMAIN}, Language: {LANGUAGE}, Language Code: {LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Only use a poration of the PDF documents for testing. If there are a lot of pages or partial processing is required, cut and save only some pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import fitz #PyMuPDF\n",
    "\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(5, 25)] #ç¬¬5é¡µåˆ°25é¡µ\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/part{idx}.pdf\")#æŠŠéœ€è¦å¤„ç†çš„é¡µå¦å¤–ä¿å­˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinguish between pages composed mainly of text, pages composed primarily of images, and pages composed of mixed text/images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PDF Content Analysis Result:\n",
      "Text pages: [0, 1, 3, 5, 8]\n",
      "Mixed pages: [2, 4, 6, 7]\n",
      "splitted_raw_data\n"
     ]
    }
   ],
   "source": [
    "from util.common_utils import delete_folder_and_make_folder\n",
    "from util.preprocess import analyze_pdf_page_content, split_pdf\n",
    "\n",
    "analyzed_pdf_result = analyze_pdf_page_content(file_path)#åˆ¤æ–­Pdfæ¯é¡µçš„ç±»å‹:Text,Image,Mixedï¼ŒåŠå…¶å¯¹åº”é¡µç \n",
    "delete_folder_and_make_folder(splitted_raw_data_dir)    \n",
    "\n",
    "print(\"### PDF Content Analysis Result:\")\n",
    "for content_type, pages in analyzed_pdf_result.items():\n",
    "    print(f\"{content_type} pages: {pages}\")\n",
    "    split_pdf(file_path, f\"{splitted_raw_data_dir}/{content_type}.pdf\", pages)#æŒ‰Pdfæ¯é¡µçš„ç±»å‹ä¿å­˜ä¸­é—´ç»“æœä¾›åç»­ä½¿ç”¨\n",
    "print(splitted_raw_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(  # æ„é€ azure openai client\n",
    "        api_key=aoai_api_key,  \n",
    "        api_version=aoai_api_version,\n",
    "        base_url=f\"{aoai_api_endpoint}/openai/deployments/{aoai_deployment_name}\",\n",
    "        max_retries=1\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    document_intelligence_client = DocumentIntelligenceClient(  # æ„é€ azure doc intelligence client. å‚è€ƒï¼šhttps://github.com/microsoft/Form-Recognizer-Toolkit/blob/main/SampleCode/Python/sample_figure_understanding.ipynb\n",
    "        endpoint=doc_intelligence_endpoint, \n",
    "        credential=AzureKeyCredential(doc_intelligence_key),\n",
    "        headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Mixed page (Images and text mixed appropriately)\n",
    "\n",
    "After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Document\n",
    "\n",
    "Analyze the document with Azure AI Document Intelligence to extract the text and image information.\n",
    "\n",
    "-   `crop_image_from_file()`: Crop the image from the PDF file based on the bounding box.\n",
    "-   `is_bounding_box_larger_than()`: Check if the bounding box is larger than the threshold.\n",
    "-   `image_complexity()`: Check if the image is complex or simple based on image statistics.\n",
    "-   `understand_image_with_gpt():` Summarize the image with OpenAI GPT.\n",
    "\n",
    "![post-process](../imgs/post-process1.png)\n",
    "![post-process](../imgs/post-process2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------- MD START\n",
      "<!-- PageHeader=\"24. 7. 22. ì˜¤ì „ 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      ". Format the EBS volume, mount it on /data , and then change the owner to\n",
      "ec2-user: ec2-user . You may refer to\n",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-\n",
      "volumes.html if you do not know how to mount it.\n",
      "\n",
      ". Download MXNet repository and TensorFlow models repository.\n",
      "\n",
      "\\$ cd /data \\$ git clone https://github.com/tensorflow/models.git\n",
      "$ git clone https://github.com/apache/incubator-mxnet.git # or,\n",
      "you can just type `pip install mxnet'\n",
      "\n",
      "Â· [Optional] For your convenience, use symbolic link such that:\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th colspan=\"2\">[ec2-user@ip-172-31-34-246 data]$ ls -1</th>\n",
      "<th></th>\n",
      "<th colspan=\"3\" rowspan=\"2\"></th>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>í•©ê³„ 13864</th>\n",
      "<th></th>\n",
      "<th></th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x 4 ec2-user ec2-user</td>\n",
      "<td>37</td>\n",
      "<td>9ì›”</td>\n",
      "<td colspan=\"3\">16 02:20 im2rec</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Lrwxrwxrwx 1 ec2-user ec2-user</td>\n",
      "<td>41</td>\n",
      "<td>9ì›”</td>\n",
      "<td>10</td>\n",
      "<td colspan=\"2\">02:38 imagenet -&gt; /data/models/research/inception/inception</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x 7 ec2-user ec2-user</td>\n",
      "<td>249</td>\n",
      "<td>9%</td>\n",
      "<td colspan=\"3\">10 02:22 models</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Lrwxrwxrwx 1 ec2-user ec2-user</td>\n",
      "<td>44</td>\n",
      "<td>9ì›”</td>\n",
      "<td>16</td>\n",
      "<td colspan=\"2\">00:29 mxnet -&gt; /usr/local/lib/python2.7/site-packages/mxnet</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw ------- 1 ec2-user ec2-user</td>\n",
      "<td>14193041</td>\n",
      "<td>9ì›”</td>\n",
      "<td>17</td>\n",
      "<td colspan=\"2\">23:46 nohup . out</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x 4 ec2-user ec2-user</td>\n",
      "<td>98</td>\n",
      "<td>9ì›”</td>\n",
      "<td>16</td>\n",
      "<td>00:52</td>\n",
      "<td>opencv</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">[ec2-user@ip-172-31-34-246 data]$</td>\n",
      "<td></td>\n",
      "<td colspan=\"3\"></td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      ". [Important Step] You need to install OpenCV also. (Both 3.x and 4.x work well).\n",
      "If you do not install OpenCV, then you cannot convert ImageNet raw data to\n",
      "RecordIO files since im2rec. py utilizes some OpenCV functions. You may\n",
      "refer to https://www.pyimagesearch.com/2018/08/15/how-to-install-opencv-\n",
      "4-on-ubuntu/.\n",
      "\n",
      ". [Caution] I strongly recommend to use Python2 instead of Python3 because\n",
      "many codes of Tensorflow models repository does not work on Python3.\n",
      "Please refer to https://stackoverflow.com/questions/38546672/inception-\n",
      "build-imagenet-data-py-typeerror-rgb-has-type-class-str-but-ex.\n",
      "\n",
      "\n",
      "# Downloading ImageNet\n",
      "\n",
      "Please note that ImageNet server is sometimes unstable so download speed is not\n",
      "fast, taking 4 to 5 days.\n",
      "\n",
      "\n",
      "## Method 1\n",
      "\n",
      "Â· Go to http://www.image-net.org/, sign up, and get your own username and\n",
      "access key.\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"3/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. ì˜¤ì „ 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      ". Extract the training set\n",
      "\n",
      "\\$ mkdir train \\$ mv ILSVRC2012_img_train. tar train \\$ cd train \\$\n",
      "tar xf ILSVRC2012_img_train. tar $ find . - name \" *. tar\" | while\n",
      "read NAME ; do mkdir -p \"\\${NAME%. tar}\"; tar -xvf \"\\$ {NAME}\" -C\n",
      "\"$ {NAME%. tar}\"; rm -f \"$ {NAME}\"; done\n",
      "\n",
      "Â· After extracting the training set, check if the number of directories is 1,000\n",
      "(class 1 is n01728572 and class 1000 is n15075141).\n",
      "\n",
      "Â· Extract bounding boxes\n",
      "\n",
      "\\$ mkdir bounding_boxes \\$ mv ILSVRC2012_bbox_train_v2.tar.gz\n",
      "bounding_boxes \\$ mv ILSVRC2012_bbox_val_v3.tgz bounding_boxes \\$\n",
      "cd bounding_boxes \\$ tar xzf ILSVRC2012_bbox_val_v3.tgz \\$ mkdir\n",
      "train \\$ mv ILSVRC2012_bbox_train_v2. tar.gz train \\$ cd train $\n",
      "tar xzf ILSVRC2012_bbox_train_v2.tar.gz\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>-rw-rw-r --</th>\n",
      "<th>1</th>\n",
      "<th>ec2-user</th>\n",
      "<th>ec2-user</th>\n",
      "<th>19537608</th>\n",
      "<th>10 ì›”</th>\n",
      "<th>1</th>\n",
      "<th>00:01</th>\n",
      "<th>ILSVRC2012_bbox_train_v2.tar.gz</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>2221290</td>\n",
      "<td>9%</td>\n",
      "<td>18</td>\n",
      "<td>2012</td>\n",
      "<td>ILSVRC2012_bbox_val_v3. tgz</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>147897477120</td>\n",
      "<td>6%</td>\n",
      "<td>14</td>\n",
      "<td>2012</td>\n",
      "<td>ILSVRC2012_img_train. tar</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>6744924160</td>\n",
      "<td>6%</td>\n",
      "<td>14</td>\n",
      "<td>2012</td>\n",
      "<td>ILSVRC2012_img_val. tar</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x</td>\n",
      "<td>1002</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>32768</td>\n",
      "<td>9%</td>\n",
      "<td>17</td>\n",
      "<td>01:36</td>\n",
      "<td>bounding_boxes</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-r -- r --</td>\n",
      "<td>1</td>\n",
      "<td>root</td>\n",
      "<td>root</td>\n",
      "<td>29709928</td>\n",
      "<td>9%</td>\n",
      "<td>17</td>\n",
      "<td>06:02</td>\n",
      "<td>imagenet_2012_bounding_boxes. csv</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x</td>\n",
      "<td>1002</td>\n",
      "<td>ec2-user</td>\n",
      "<td colspan=\"2\">ec2-user 32768</td>\n",
      "<td>9ì›”</td>\n",
      "<td>17</td>\n",
      "<td>02:29</td>\n",
      "<td>train</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x</td>\n",
      "<td>1002</td>\n",
      "<td>ec2-user</td>\n",
      "<td colspan=\"2\">ec2-user 2691072</td>\n",
      "<td>9%</td>\n",
      "<td>17</td>\n",
      "<td>05:54</td>\n",
      "<td>validation</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"5\">(base) [ec2-user@ip-172-31-35-5 data]$</td>\n",
      "<td></td>\n",
      "<td colspan=\"3\"></td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "# Data Transformation\n",
      "\n",
      "\n",
      "## RecordIO format\n",
      "\n",
      ". Use im2rec. py the same way Simon did.\n",
      "(https://medium.com/@julsimon/imagenet-part-1-going-on-an-adventure-\n",
      "c0a62976dc72). It takes 1.5 days on the t2. large instance. I think he did\n",
      "some typos (ImageNet baseline usually uses 224x224 size image, but he uses\n",
      "480Ã—480).\n",
      "\n",
      "\n",
      "## TFRecord format\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"5/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. ì˜¤ì „ 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      "Â· Create an EC2 instance for Training (Deep Learning AMI (Ubuntu 16.04) or\n",
      "Deep Learning AMI (Amazon Linux)). p3.16xlarge or p3dn. 24xlarge is\n",
      "recommended if you need to do distributed GPU training using Uber's\n",
      "Horovod or Tensorflow's DistributedStrategy). Please also note that the\n",
      "default root volume size is 75GB, but I recommend you to increase 100GB\n",
      "since training logs and model checkpoints are stored in the root volume if you\n",
      "do not modify training configuration. If you not want to increase the volume\n",
      "size, then you can delete some conda environments such as Theano, Chainer,\n",
      "Caffe, and Caffe2 after logging in to the EC2 instance.\n",
      "\n",
      "Â· https://aws.amazon.com/ko/getting-started/tutorials/get-started-dlami/\n",
      "\n",
      ". If you want to train on distributed GPUs, then you need to create multiple GPU\n",
      "instances with the same setting. For example, the below figure shows 8\n",
      "p3dn. 24xlarge instances.\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th colspan=\"6\">Q â˜’ search : POC_HU24 Add filter</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>â˜</th>\n",
      "<th>Name</th>\n",
      "<th>Instance ID</th>\n",
      "<th>Instance Type</th>\n",
      "<th>Availability Zone</th>\n",
      "<th>Instance State</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_81</td>\n",
      "<td>i-089bb90e91fef7b09</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_82</td>\n",
      "<td>i-09be131f79506dcc1</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_83</td>\n",
      "<td>i-0c44553f8570af264</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_84</td>\n",
      "<td>i-0d8f1a29d7864e892</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_85</td>\n",
      "<td>i-0de84adf899462171</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_86</td>\n",
      "<td>i-0e56678cc29ad0de8</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_87</td>\n",
      "<td>i-Of4912fb1d7760a1b</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>â˜</td>\n",
      "<td>POC_HU24_88</td>\n",
      "<td>i-Of4ddd1c5bbfd4dd7</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>â˜’ running</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      ". Please refer to the website for the remaining steps;\n",
      "https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod-\n",
      "tensorflow.html. Note that all code and all feature sets(TFRecord and\n",
      "RecordIO) must be on the same path on each server.\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"7/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. ì˜¤ì „ 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      ". After training, please check the training log and evaluation log by checking\n",
      "imagenet_resnet folder:\n",
      "\n",
      "\n",
      "<figure>\n",
      "</figure>\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<td colspan=\"2\">...</td>\n",
      "<td colspan=\"3\">ubuntu@ip-172-31-3-51: ~ /examples/horovod/tensorflow/imagenet_resnet (ssh)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"5\">ubuntu@ip-172-31-3-51 :~ /examples/horovod/tensorflow/imagenet_resnet$ 1s -1 total 646280</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">8 -rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td colspan=\"3\">89 Sep 23 02:54 checkpoint</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">-rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td>18880</td>\n",
      "<td colspan=\"2\">Oct 1 01:22 eval_hvd_train. log</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">-rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td>21227745</td>\n",
      "<td colspan=\"2\">Sep 23 03:01 events. out. tfevents . 1569199858. ip-172-31-3-51</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">-rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td>9287777</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>graph. pbtxt</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>18880</td>\n",
      "<td>Sep 23 03:03</td>\n",
      "<td>hvd_train. log</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>8</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model.ckpt-0.data-00000-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>204668736</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model. ckpt-0.data-00001-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>17114</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model. ckpt-0. index</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>5709416</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model. ckpt-0.meta</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>8</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model.ckpt-10000.data-00000-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>204668736</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model. ckpt-10000.data-00001-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>17114</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model. ckpt-10000. index</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>5709416</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model. ckpt-10000.meta</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>8</td>\n",
      "<td>Sep 23 02:54</td>\n",
      "<td>model.ckpt-20000.data-00000-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>204668736</td>\n",
      "<td>Sep 23 02:54</td>\n",
      "<td>model. ckpt-20000.data-00001-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>17114</td>\n",
      "<td colspan=\"2\">Sep 23 02:54 model. ckpt-20000. index</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>5709416</td>\n",
      "<td colspan=\"2\">Sep 23 02:54 model. ckpt-20000.meta</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"5\">ubuntu@ip-172-31-3-51 :~ /examples/horovod/tensorflow/imagenet_resnet$</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "Â· vd_train_log (32 GPUS; 4 p3dn.24xlarge instances)\n",
      "\n",
      "\\- Step Epoch Speed Loss FinLoss LR - 0 0.0 952.2 6.923 8.262\n",
      "0.00100 - 1 0.0 2686.6 6.928 8.267 0.00305 - 50 0.3 22243.7\n",
      "\n",
      "6.586 7.919 0.10353 - .. - 14000 89.5 21021.1 0.750 1.152\n",
      "\n",
      "0.00012 - 14050 89.8 21818.7 0.583 0.985 0.00002 - Finished in\n",
      "5289.161954164505\n",
      "\n",
      "Â· eval_hvd_train.log (32 GPUS; 4 p3dn.24xlarge instances)\n",
      "\n",
      "ubuntu@ip-172-31-3-51 :~ /examples/horovod/tensorflow$ cat eval_hvd_train_gpu32. log\n",
      "PY3.6.5 |Anaconda, Inc. I (default, Apr 29 2018, 16:14:56)\n",
      "\n",
      "[GCC 7.2.0]TF1.13.1\n",
      "\n",
      "Horovod size: 8\n",
      "\n",
      "Using data from: /home/ubuntu/data1/tf-imagenet/\n",
      "Evaluating\n",
      "\n",
      "Validation dataset size: 50000\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>step</th>\n",
      "<th>epoch</th>\n",
      "<th>top1</th>\n",
      "<th>top5</th>\n",
      "<th>loss</th>\n",
      "<th>checkpoint_time(UTC)</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>14075</td>\n",
      "<td>90.2</td>\n",
      "<td>75.821</td>\n",
      "<td>92.90</td>\n",
      "<td>0.92</td>\n",
      "<td>2019-09-20 07:50:57</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"6\">Finished evaluation</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"8/9\" -->\n",
      "\n",
      "-------------------------------------------------------------- MD END\n",
      "Figures:\n",
      "5.9852 3.2923\n",
      "\tDescription of figure 0: The image shows a portion of a text which includes a shell script snippet for downloading the ImageNet dataset using a specific script. The instructions mentioned involve exporting a username and access key, navigating to a directory, renaming a file, and running a download script in the background while logging the output. The specific commands in the shell script are:\n",
      "\n",
      "```sh\n",
      "$ export IMAGENET_USERNAME=[YOUR_USERNAME] \n",
      "$ export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY] \n",
      "$ cd imagenet/data \n",
      "$ mv imagenet_2012_validation_synset_labels.txt synsets.txt\n",
      "$ nohup bash download_imagenet.sh . synsets.txt >& download.log &\n",
      "```\n",
      "\n",
      "Below this snippet, there is a heading for an alternative method labeled \"Method 2 (Alternative method if Method 1 does not work),\" which suggests downloading the ImageNet dataset manually.\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    pdf_mixed_path = f\"{splitted_raw_data_dir}/Mixed.pdf\"\n",
    "\n",
    "    with open(pdf_mixed_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(# layoutåˆ†æçš„ç”¨æ³•å‚è€ƒ https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/prebuilt/layout\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", \n",
    "            output_content_format=ContentFormat.MARKDOWN #æŠŠpdfè½¬æˆMarkDownæ–‡æ¡£\n",
    "        )\n",
    "\n",
    "    result = poller.result()# azure dock intelligenceè¿”å›çš„å¯¹è±¡åŒ…å«å¾ˆå¤šæœ‰ç”¨å†…å®¹ï¼Œå‚è€ƒï¼šhttps://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept/analyze-document-response\n",
    "    md_content = result.content\n",
    "\n",
    "    print(\"-------------------------------------------------------------- MD START\")\n",
    "    print(md_content)\n",
    "    print(\"-------------------------------------------------------------- MD END\")\n",
    "\n",
    "    #### Updates the content of the figure description (empty content or caption) with the image summary text generated by gpt-4o.\n",
    "    from util.preprocess import (\n",
    "        image_complexity, is_bounding_box_larger_than, crop_image_from_file, \n",
    "        understand_image_with_gpt, update_figure_description\n",
    "    )\n",
    "    output_folder = \"pdf_mixed_tmp\"\n",
    "    delete_folder_and_make_folder(output_folder)\n",
    "    language = LANGUAGE\n",
    "    max_tokens = 1024\n",
    "    input_file_path = file_path\n",
    "\n",
    "    if result.figures:\n",
    "        print(\"Figures:\")\n",
    "        for idx, figure in enumerate(result.figures): # å¤„ç†azure doc intelligenceæ‰¾åˆ°çš„æ¯ä¸€å¼ å›¾ç‰‡\n",
    "            img_description = \"\"\n",
    "            #print(f\"Figure #{idx} has the following spans: {figure.spans}\")\n",
    "\n",
    "            # figure_content = \"\"\n",
    "            # for i, span in enumerate(figure.spans):# span: details the text spans related to the figure, specifying their offsets and lengths within the document's text. This connection helps in associating the figure with its relevant textual context\n",
    "            #     #print(f\"Span #{i}: {span}\")\n",
    "            #     figure_content += md_content[span.offset:span.offset + span.length] #å›¾ç‰‡åœ¨mdä¸­çš„ä½ç½®ï¼Œèµ·å§‹å­—ç¬¦ä¸‹è¡¨å’Œç»“æŸå­—ç¬¦ä¸‹æ ‡\n",
    "            # #print(f\"Original figure content in markdown: {figure_content}\")\n",
    "\n",
    "            # Note: figure bounding regions currently contain both the bounding region of figure caption and figure body\n",
    "            if figure.caption:# azure doc intelligenceåˆ†æå‡ºæ¥çš„figure captionæ˜¯pdfä¸Šå›¾ç‰‡é™„è¿‘çš„é‚£ä¸€å°ä¸²è¯´æ˜ï¼Œä¸ä¸€å®šæœ‰\n",
    "                caption_region = figure.caption.bounding_regions\n",
    "                #print(f\"\\tCaption: {figure.caption.content}\")\n",
    "                #print(f\"\\tCaption bounding region: {caption_region}\")\n",
    "                for region in figure.bounding_regions:# Each region specifies the page number (1-indexed) and bounding polygon. The bounding polygon is described as a sequence of points, clockwise from the left relative to the natural orientation of the element. For quadrilaterals, plot points are top-left, top-right, bottom-right, and bottom-left corners. Each point represents its x, y coordinate in the page unit specified by the unit property. In general, unit of measure for images is pixels while PDFs use inches.\n",
    "                    if region not in caption_region:# å¦‚æœå›¾ç‰‡ä¸æ˜¯åªæœ‰captionçš„è¯ï¼Œåˆ™éœ€è¦å¤„ç†å…¶å®ƒçœŸæ­£çš„å›¾ç‰‡\n",
    "                        #print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                        # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                        boundingbox = (\n",
    "                                region.polygon[0],  # x0 (left)\n",
    "                                region.polygon[1],  # y0 (top)\n",
    "                                region.polygon[4],  # x1 (right)\n",
    "                                region.polygon[5]   # y1 (bottom)\n",
    "                            )\n",
    "\n",
    "                        if is_bounding_box_larger_than(boundingbox):# å›¾åƒè¶³å¤Ÿå¤§(è¶…1x1è‹±å¯¸)æ‰ä¼šæˆªå–å‡ºæ¥å¤„ç†\n",
    "                            #print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "                            cropped_image = crop_image_from_file(pdf_mixed_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                            if image_complexity(cropped_image)[0] == \"Complex\":# å›¾ç‰‡çš„ç†µï¼Œæ¸…æ™°åº¦ï¼Œè¾¹ç¼˜æ•°éƒ½è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™å®šä¹‰ä¸ºå¤æ‚çš„ï¼Œä¿¡æ¯é‡å¤§çš„ï¼Œæœ‰æ„ä¹‰çš„å›¾ç‰‡ï¼Œåç»­éœ€è¦å¤„ç†\n",
    "                                # Get the base name of the file\n",
    "                                base_name = os.path.basename(input_file_path)\n",
    "                                # Remove the file extension\n",
    "                                file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                                output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                                cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "\n",
    "                                cropped_image.save(cropped_image_filename)\n",
    "                                print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                                try: # ç”¨gpt-4oç”Ÿæˆå›¾ç‰‡æè¿°. å¦‚æœazure doc intellegenceåœ¨å›¾ç‰‡é™„è¿‘æ‰¾åˆ°äº†captionï¼Œåˆ™ä½œä¸ºä¸Šä¸‹æ–‡ä¼ å…¥\n",
    "                                    image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, figure.caption.content, max_tokens=max_tokens, language=language)\n",
    "                                except openai.BadRequestError as e:\n",
    "                                    print(f\"BadRequestError: {e}\")\n",
    "                                    image_summarization = \"\"\n",
    "                                img_description += image_summarization\n",
    "\n",
    "                                print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                            else:\n",
    "                                print(f'simple image at idx {idx}')\n",
    "\n",
    "            else:# å¦‚æœpdfä¸Šå›¾ç‰‡é™„è¿‘æ‰¾ä¸åˆ°ç›¸å…³captionè¯´æ˜\n",
    "                #print(\"\\tNo caption found for this figure.\")\n",
    "                for region in figure.bounding_regions:\n",
    "                    #print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    boundingbox = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "\n",
    "                    if is_bounding_box_larger_than(boundingbox):# å›¾åƒè¶³å¤Ÿå¤§(è¶…1x1è‹±å¯¸)æ‰ä¼šæˆªå–å‡ºæ¥å¤„ç†\n",
    "                        #print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "\n",
    "                        cropped_image = crop_image_from_file(input_file_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                        if image_complexity(cropped_image)[0] == \"Complex\":# å›¾ç‰‡çš„ç†µï¼Œæ¸…æ™°åº¦ï¼Œè¾¹ç¼˜æ•°éƒ½è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™å®šä¹‰ä¸ºå¤æ‚çš„ï¼Œä¿¡æ¯é‡å¤§çš„ï¼Œæœ‰æ„ä¹‰çš„å›¾ç‰‡ï¼Œåç»­éœ€è¦å¤„ç†\n",
    "                            # Get the base name of the file\n",
    "                            base_name = os.path.basename(input_file_path)\n",
    "                            # Remove the file extension\n",
    "                            file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                            output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                            cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "                            # cropped_image_filename = f\"data/cropped/image_{idx}.png\"\n",
    "                            cropped_image.save(cropped_image_filename)\n",
    "                            #print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                            try: # ç”¨gpt-4oç”Ÿæˆå›¾ç‰‡æè¿°\n",
    "                                image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                            except openai.BadRequestError as e:\n",
    "                                print(f\"BadRequestError: {e}\")\n",
    "                                image_summarization = \"\"\n",
    "                            img_description += image_summarization\n",
    "                            print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                        else:\n",
    "                            print(f'simple image at idx {idx}')\n",
    "\n",
    "            \n",
    "            md_content = update_figure_description(md_content, img_description, idx) # æŠŠazure gpt-4oç”Ÿæˆçš„å›¾ç‰‡æè¿°æ’å…¥åˆ°åŸæ–‡æ¡£ä¸­æ”¾ç½®å›¾ç‰‡(å›¾ç‰‡å ä½ç¬¦)çš„åœ°æ–¹ï¼Œæ­¤æ—¶è¿”å›çš„æ•´ä¸ªæ–‡æ¡£md_contentåœ¨åŸæ¥æœ‰å›¾ç‰‡çš„åœ°æ–¹å°±åŒ…å«äº†å›¾ç‰‡æè¿°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate chunks for mixed pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits (mixed case): 9\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import re\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(# æŒ‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§é€’å½’å¯¹æ–‡æ¡£è¿›è¡Œåˆ†éš”æˆchunkï¼Œä¹Ÿå¯è€ƒè™‘azure doc intelligenceçš„chunking(https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept/retrieval-augmented-generation?#semantic-chunking)\n",
    "        separators=[\n",
    "            r'<!-- PageNumber=\"\\d+\" -->', # azure doc intelligence è½¬æˆ markdown åçš„é¡µåˆ†éš”ç¬¦\n",
    "            r\"\\n\\n\",\n",
    "            r\"\\n\",\n",
    "            \" \",\n",
    "            \".\",\n",
    "            \"\",\n",
    "        ],   \n",
    "        is_separator_regex = True,    \n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "\n",
    "    mixed_chunks = text_splitter.split_text(md_content) # æŒ‰ç…§ä¸Šè¿°é¡µï¼Œç« èŠ‚ï¼Œæ®µçš„åˆ†éš”ç¬¦æŠŠåŸæ–‡æ¡£åˆ†å‰²æˆå¤šä¸ªchunk\n",
    "else:\n",
    "    mixed_chunks = []\n",
    "print(\"Length of splits (mixed case): \" + str(len(mixed_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Text-heavy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "page_content='[Hands-on] Fast Training\n",
      "ImageNet on on-demand EC2\n",
      "GPU instances with Horovod\n",
      "ğŸ’»\n",
      "Author: Daekeun Kim (daekeun@amazon.com)\n",
      "Goal\n",
      "This document is for people who need distributed GPU training using Horovod for\n",
      "experimental purposes. Many steps are similar to what mentioned in Julien\n",
      "Simonâ€™s article(\n",
      ") and AWS\n",
      "Documentation(\n",
      "). So I recommend you to view these articles first. If there\n",
      "are some things that arenâ€™t going well (e.g., Downloading the dataset does not\n",
      "work, How to convert the raw data to the TFRecord feature set?, How to fix the\n",
      "error ModuleNotFoundError: No module named 'cv2'? ) please refer this\n",
      "document.\n",
      "https://medium.com/@julsimon/imagenet-part-1-going-on-an-\n",
      "adventure-c0a62976dc72\n",
      "https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-\n",
      "horovod-tensorflow.html\n",
      "Introduction\n",
      "For data preparation and data transformation, we do not need to use a GPU\n",
      "instance such as p2 and p3. Instead, we can start much cheaper instances like \n",
      "t2.large  instance with 1.0TB EBS volume.\n",
      "For distributed training, we need to use multiple GPU instances like p2, p3, g3 and\n",
      "g4.\n",
      "You can skip step 1 if you do not want to invent the wheel again because I have\n",
      "stored everything in my s3 bucket.' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 1\n",
      "page_content='g4.\n",
      "You can skip step 1 if you do not want to invent the wheel again because I have\n",
      "stored everything in my s3 bucket.\n",
      "24. 7. 22. ì˜¤ì „ 9:52\n",
      "[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\n",
      "https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\n",
      "1/9' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 2\n",
      "page_content='â€¢\n",
      "s3://dataset-image/imagenet/raw  (raw jpeg)\n",
      "â€¢\n",
      "s3://dataset-image/imagenet/tfrecord  (TFRecord before resizing)\n",
      "â€¢\n",
      "s3://dataset-image/imagenet/tfrecord-resized  (TFRecord after resizing to\n",
      "224x224)\n",
      "â€¢\n",
      "s3://dataset-image/imagenet/recordio  (RecordIO after resizing to\n",
      "256x256)\n",
      "â—¦The reason I did not resize 224x224 is that the below article shows\n",
      "different validation accuracy for resizing strategy.\n",
      "â—¦https://forums.fast.ai/t/impact-of-image-resizing-on-model-training-time-\n",
      "and-performance/1980\n",
      "Please let me know if you want to access the bucket because I did not grant any\n",
      "public access.\n",
      "Step 1. Downloading and Transformation\n",
      "Setting up an EC2 instance for Data Transformation\n",
      "â€¢ Create an EC2 instance for storing ImageNet dataset (Ubuntu 18.04 or 16.04.\n",
      "Linux is also available). t2.micro  is also available, but t2.large  is\n",
      "recommended due to memory size. Note that we do not need large storage\n",
      "size since we will make another EBS volume to attach the EC2 instance.\n",
      "â€¢ Create an EBS volume (1.0TB) for ImageNet dataset and then attach the\n",
      "volume it to your EC2 instance. ImageNet consists of 138GB for training set\n",
      "and 6.3GB for validation set, but we need an additional space since we need to' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 1, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Length of splits (text-heay case): 9\n"
     ]
    }
   ],
   "source": [
    "if \"Text\" in analyzed_pdf_result:\n",
    "    from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "    from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "    pdf_text_path = f\"{splitted_raw_data_dir}/Text.pdf\" # åŠ è½½åœ¨â€˜Split the PDFs into individual pagesâ€™é‚£ä¸€èŠ‚ä¸­åˆ†å‰²å‡ºæ¥çš„æ–‡å­—ä¸ºä¸»çš„pdfé¡µ\n",
    "    loader = PyMuPDFLoader(pdf_text_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200, \n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitter.split_documents(documents)#åˆ†chunk\n",
    "\n",
    "    for idx, chunk in enumerate(text_chunks):\n",
    "        print(f\"Chunk {idx}\\n{chunk}\")\n",
    "        print(\"=\"*80)\n",
    "        if idx == 2:\n",
    "            break\n",
    "\n",
    "    text_chunks = [d.page_content for d in text_chunks]\n",
    "else:\n",
    "    text_chunks = []\n",
    "print(\"Length of splits (text-heay case): \" + str(len(text_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Image-heavy\n",
    "\n",
    "Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "\n",
    "### Preprocess Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "if \"Image\" in analyzed_pdf_result:\n",
    "    import fitz\n",
    "    from glob import glob\n",
    "\n",
    "    image_dir = \"./pdf_image_tmp\"\n",
    "    delete_folder_and_make_folder(image_dir) \n",
    "\n",
    "    pdf_image_path = f\"{splitted_raw_data_dir}/Image.pdf\" # åŠ è½½åœ¨â€˜Split the PDFs into individual pagesâ€™é‚£ä¸€èŠ‚ä¸­åˆ†å‰²å‡ºæ¥çš„å›¾ç‰‡ä¸ºä¸»çš„pdfé¡µ\n",
    "    doc = fitz.open(pdf_image_path)\n",
    "    #clip_x, clip_y = 10, 45\n",
    "    clip_x, clip_y = 10, 10 # åŸºäºfitzåšçš„ä¸€äº›çŸ«æ­£\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        x, y, w, h = page.rect\n",
    "        clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "        page.set_cropbox(clip)\n",
    "        pix = page.get_pixmap()\n",
    "        pix.save(f\"{image_dir}/page_{i:03d}.jpg\")\n",
    "\n",
    "    images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "# å¯¹ä»ä¸Šä¸€ä¸ªcellæŠ å‡ºæ¥çš„å›¾ç‰‡ï¼Œç”¨azure gpt-4oè¿›è¡Œå›¾åƒæè¿°\n",
    "max_tokens = 1024\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name\n",
    ")\n",
    "\n",
    "human_prompt_main = f\"Given image, give a concise summary in {LANGUAGE}. Don't insert any XML tag such as <text> and </text> when answering.\"\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing image.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": human_prompt_main\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser() #langchainçš„è¯­æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image_summaries (image-heavy case): 0\n",
      "CPU times: user 174 Î¼s, sys: 67 Î¼s, total: 241 Î¼s\n",
      "Wall time: 200 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if \"Image\" in analyzed_pdf_result:\n",
    "    from util.preprocess import encode_image_base64\n",
    "    #images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "    base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "    image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8}) #æŠŠä¸Šä¸Šä¸€cellæŠ å‡ºæ¥çš„å›¾ç‰‡å‘ç»™GPT4Oç”Ÿæˆæ–‡å­—æè¿°\n",
    "    image_summaries = remove_short_sentences(image_summaries)\n",
    "else:\n",
    "    image_summaries = []\n",
    "print(\"Length of image_summaries (image-heavy case): \" + str(len(image_summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct QnA Pairs\n",
    "\n",
    "---\n",
    "\n",
    "### Option 1.\n",
    "\n",
    "Leverage the `azure-ai-generative` package. The `QADataGenerator` class in this package makes it easy to generate QnA synthetic questions. However, using this class as is has the disadvantage of not being able to use custom prompts, so we inherited from it and created the `CustomQADataGenerator` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from util.qa import CustomQADataGenerator # ç»§æ‰¿è‡ªazure-ai-generativeçš„QADataGeneratorï¼Œç»§æ‰¿çš„å­ç±»å°è£…äº†å¦‚ä½•è·å–è‡ªå·±çš„prompt templateçš„é€»è¾‘\n",
    "# azure-ai-generativeçš„QADataGeneratorå‚è€ƒhttps://learn.microsoft.com/en-us/python/api/azure-ai-generative/azure.ai.generative.synthetic.qa.qadatagenerator\n",
    "model_config = {\n",
    "    \"deployment\": aoai_deployment_name,\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "qa_generator = CustomQADataGenerator(model_config=model_config, templates_dir=f\"./prompt_template/{LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "import os\n",
    "from azure.ai.generative.synthetic.qa import QAType\n",
    "concurrency = 6  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "#qa_type = QAType.CONVERSATION\n",
    "qa_type = QAType.LONG_ANSWER\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(# æ ¹æ® QAType.LONG_ANSWER å’Œ LANGUAGE_CODE ä¼šè·å–åˆ°æç¤ºè¯æ¨¡æ¿ 1_synthetic-qa-generation/seed/prompt_template/en/prompt_qa_long_answer.txt\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated QAs\n"
     ]
    }
   ],
   "source": [
    "input_batch = mixed_chunks + text_chunks + image_summaries # æœ‰å›¾ä¹Ÿæœ‰æ–‡å­—çš„pdfé¡µï¼Œæ–‡å­—ä¸ºä¸»çš„pdfé¡µï¼Œå›¾ç‰‡ä¸ºä¸»çš„pdfé¡µï¼Œè¿™äº›é¡µé¢å†…å®¹åˆ‡æˆçš„chunkséƒ½è¾“å…¥è®©azure gpt-4oç”Ÿæˆç›¸å…³çš„QAæ•°æ®\n",
    "results = await asyncio.gather(*[generate_async(text) for text in input_batch], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2.\n",
    "\n",
    "You write the entire sequence of code to create a QnA dataset without using azure-ai-generative but azure openai only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "# è¿™é‡Œæ¼”ç¤ºä¸€ä¸‹jsonæ ¼å¼çš„ç”Ÿæˆèƒ½åŠ›\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1024,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "prompt = get_qna_prompt_template(LANGUAGE) # å¦å¤–ä¸€ç§prompt templateï¼ŒåŒ…å«äº†contextï¼Œdomainå’Œnum_questionså˜é‡ï¼Œå¹¶ä¸”è¿”å›json\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "for doc in mixed_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in text_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in image_summaries:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'QUESTION': 'How can you format and mount an EBS volume on /data and change its owner to ec2-user?', 'ANSWER': 'To format and mount an EBS volume on /data and change its owner to ec2-user, you can refer to the AWS documentation at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html.'}, [{'QUESTION': 'Why is it necessary to install OpenCV when converting ImageNet raw data to RecordIO files?', 'ANSWER': 'It is necessary to install OpenCV because the im2rec.py script utilizes some OpenCV functions to convert ImageNet raw data to RecordIO files.'}, {'QUESTION': 'What is the recommended version of Python to use for TensorFlow models, and why?', 'ANSWER': 'The recommended version of Python to use for TensorFlow models is Python2 because many codes in the TensorFlow models repository do not work on Python3.'}, {'QUESTION': 'What should you do after extracting the training set to ensure the extraction was successful?', 'ANSWER': 'After extracting the training set, you should check if the number of directories is 1,000, with class 1 being n01728572 and class 1000 being n15075141.'}], {'QUESTION': \"What is the size of the file 'ILSVRC2012_img_train.tar'?\", 'ANSWER': \"The size of the file 'ILSVRC2012_img_train.tar' is 147897477120 bytes.\"}, [{'QUESTION': \"What type of EC2 instances are recommended for distributed GPU training using Horovod or TensorFlow's DistributedStrategy?\", 'ANSWER': \"The recommended EC2 instances for distributed GPU training using Horovod or TensorFlow's DistributedStrategy are p3.16xlarge or p3dn.24xlarge.\"}, {'QUESTION': 'What is the default root volume size for the recommended EC2 instances, and what is the suggested increase?', 'ANSWER': 'The default root volume size for the recommended EC2 instances is 75GB, and it is suggested to increase it to 100GB.'}, {'QUESTION': 'What can be done if you do not want to increase the root volume size of the EC2 instance?', 'ANSWER': 'If you do not want to increase the root volume size, you can delete some conda environments such as Theano, Chainer, Caffe, and Caffe2 after logging in to the EC2 instance.'}], [{'QUESTION': 'What instance type is used for distributed GPU training in the provided context?', 'ANSWER': 'The instance type used for distributed GPU training in the provided context is p3dn.24xlarge.'}, {'QUESTION': 'How many instances are shown in the table for distributed GPU training?', 'ANSWER': 'There are 8 instances shown in the table for distributed GPU training.'}, {'QUESTION': 'What is the availability zone for all the instances listed in the table?', 'ANSWER': 'The availability zone for all the instances listed in the table is us-west-2c.'}], [{'QUESTION': 'What is the purpose of exporting IMAGENET_USERNAME and IMAGENET_ACCESS_KEY in the shell script?', 'ANSWER': 'The purpose of exporting IMAGENET_USERNAME and IMAGENET_ACCESS_KEY in the shell script is to set the environment variables required for authenticating and downloading the ImageNet dataset.'}, {'QUESTION': \"What should be done after navigating to the 'imagenet/data' directory according to the shell script snippet?\", 'ANSWER': \"After navigating to the 'imagenet/data' directory, the file 'imagenet_2012_validation_synset_labels.txt' should be renamed to 'synsets.txt'.\"}, {'QUESTION': \"What is the purpose of running the 'download_imagenet.sh' script with 'nohup' and logging the output to 'download.log'?\", 'ANSWER': \"The purpose of running the 'download_imagenet.sh' script with 'nohup' and logging the output to 'download.log' is to execute the download process in the background and capture the log output for later review.\"}], {'QUESTION': \"What is the size of the file 'model.ckpt-0.data-00001-of-00002'?\", 'ANSWER': \"The size of the file 'model.ckpt-0.data-00001-of-00002' is 204668736 bytes.\"}, [{'QUESTION': \"What is the size of the file 'model.ckpt-20000.data-00001-of-00002'?\", 'ANSWER': \"The size of the file 'model.ckpt-20000.data-00001-of-00002' is 204668736 bytes.\"}, {'QUESTION': 'Who is the owner of the files listed in the context?', 'ANSWER': \"The owner of the files listed in the context is 'ubuntu'.\"}, {'QUESTION': \"What is the timestamp for the file 'model.ckpt-10000.meta'?\", 'ANSWER': \"The timestamp for the file 'model.ckpt-10000.meta' is Sep 23 01:53.\"}], [{'QUESTION': 'What was the final loss value at step 14000 during the training process?', 'ANSWER': 'The final loss value at step 14000 during the training process was 0.750.'}, {'QUESTION': 'What was the top-1 accuracy at epoch 90.2 during the evaluation?', 'ANSWER': 'The top-1 accuracy at epoch 90.2 during the evaluation was 75.821.'}, {'QUESTION': 'How many GPUs were used for the training and evaluation processes?', 'ANSWER': 'A total of 32 GPUs were used for both the training and evaluation processes.'}], [{'QUESTION': 'What type of instances are recommended for data preparation and transformation?', 'ANSWER': 'For data preparation and data transformation, it is recommended to use cheaper instances like t2.large with 1.0TB EBS volume.'}, {'QUESTION': 'Which instances are suggested for distributed GPU training using Horovod?', 'ANSWER': 'For distributed GPU training using Horovod, it is suggested to use multiple GPU instances like p2, p3, g3, and g4.'}, {'QUESTION': 'Where can you find additional information if you encounter issues such as downloading the dataset or converting raw data to the TFRecord feature set?', 'ANSWER': 'If you encounter issues such as downloading the dataset or converting raw data to the TFRecord feature set, you can refer to Julien Simonâ€™s article and AWS Documentation.'}], [{'QUESTION': 'What is the title of the hands-on guide mentioned in the context?', 'ANSWER': \"The title of the hands-on guide mentioned in the context is 'Hands-on Fast Training ImageNet on on-demand EC2 GPU instances with Horovod'.\"}, {'QUESTION': 'Where can the hands-on guide be accessed?', 'ANSWER': 'The hands-on guide can be accessed at the URL: https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca.'}, {'QUESTION': 'What is the date and time mentioned in the context?', 'ANSWER': 'The date and time mentioned in the context are 24. 7. 22. ì˜¤ì „ 9:52.'}], {'QUESTION': 'What is the recommended EC2 instance type for storing the ImageNet dataset and why?', 'ANSWER': 'The recommended EC2 instance type for storing the ImageNet dataset is t2.large due to its memory size.'}, [{'QUESTION': 'What is the total size of the ImageNet training and validation sets?', 'ANSWER': 'The total size of the ImageNet training set is 138GB and the validation set is 6.3GB.'}, {'QUESTION': 'Why do we need additional space beyond the size of the ImageNet datasets?', 'ANSWER': 'We need additional space to extract tar files and to transform the datasets into feature sets like TFRecord and RecordIO.'}, {'QUESTION': 'What is the AWS CLI command to create a 1000GB volume in a specific region and availability zone?', 'ANSWER': \"The AWS CLI command to create a 1000GB volume is: $ aws ec2 create-volume --size 1000 --region [YOUR_AWS_REGION] --availability-zone [YOUR_AZ] --volume-type sc1 --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=ImageNet}]'.\"}], [{'QUESTION': \"What is the first step to use TensorFlow's download script for ImageNet?\", 'ANSWER': \"The first step to use TensorFlow's download script for ImageNet is to export your username and access key using the commands: $ export IMAGENET_USERNAME=[YOUR_USERNAME] and $ export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY].\"}, {'QUESTION': 'What should you do if Method 1 for downloading the ImageNet dataset does not work?', 'ANSWER': 'If Method 1 for downloading the ImageNet dataset does not work, you should download the ImageNet dataset manually using the provided wget commands for the training and validation sets.'}, {'QUESTION': 'How do you extract the validation set after downloading it manually?', 'ANSWER': \"To extract the validation set after downloading it manually, you need to create a directory named 'validation', move the ILSVRC2012_img_val.tar file into this directory, navigate to the directory, and then extract the contents using the command: $ tar xf ILSVRC2012_img_val.tar.\"}], {'QUESTION': 'What is the first step to prepare the validation set for training ImageNet on EC2 GPU instances?', 'ANSWER': \"The first step to prepare the validation set is to create a directory named 'validation' and move the ILSVRC2012_img_val.tar file into it.\"}, [{'QUESTION': \"What is the purpose of the script 'preprocess_imagenet.py'?\", 'ANSWER': \"The purpose of the script 'preprocess_imagenet.py' is to preprocess the ImageNet dataset, and it can be executed using the command 'python preprocess_imagenet.py \\\\ --local_scratch_dir=[YOUR DIRECTORY] \\\\ --imagenet_username=[imagenet account] \\\\ --imagenet_access_key=[imagenet access key]'.\"}, {'QUESTION': 'How can you resize the ImageNet dataset using TensorFlow?', 'ANSWER': \"You can resize the ImageNet dataset using TensorFlow by running the command 'python tensorflow_image_resizer.py \\\\ -d imagenet \\\\ -i [PATH TO TFRECORD TRAINING DATASET] \\\\ -o [PATH TO RESIZED TFRECORD TRAINING DATASET] \\\\ --subset_name train \\\\ --num_preprocess_threads 60 \\\\ --num_intra_threads 2 \\\\ --num_inter_threads 2'.\"}, {'QUESTION': 'What should you do after transforming the data?', 'ANSWER': 'After transforming the data, you should create a new bucket and sync or copy feature sets to the bucket, and then create a snapshot of the EBS volume.'}], [{'QUESTION': 'What should be done after data transformation according to the context?', 'ANSWER': 'After data transformation, a new bucket should be created and feature sets should be synced or copied to the bucket.'}, {'QUESTION': 'What is the first step mentioned for training the ResNet-50 model with Horovod?', 'ANSWER': 'The first step mentioned for training the ResNet-50 model with Horovod is to create a snapshot of the EBS volume.'}, {'QUESTION': 'Where can you find a hands-on guide for fast training ImageNet on on-demand EC2 GPU instances with Horovod?', 'ANSWER': 'A hands-on guide for fast training ImageNet on on-demand EC2 GPU instances with Horovod can be found at https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca.'}], {'QUESTION': 'What is the learning rate at step 7000 in the hvd_train_log?', 'ANSWER': 'The learning rate at step 7000 in the hvd_train_log is 0.00023.'}]\n",
      "CPU times: user 219 ms, sys: 14 ms, total: 233 ms\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_pair = chain.batch(input_batch, {\"max_concurrency\": 5})\n",
    "print(qa_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to jsonl\n",
    "\n",
    "---\n",
    "\n",
    "If you want to augment dataset, you can try Evovle-Instruct or other data augmentation techniques.<br>\n",
    "Please refer to `../evolve-instruct` and `../glan-instruct` for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./dataset/imagenet-training-summary-oai.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = f\"\"\"You are the SME (Subject Matter Expert) in {DOMAIN}. Please answer the questions accurately. If the question is in {LANGUAGE}, write your answer in {LANGUAGE}.\"\"\"\n",
    "\n",
    "save_filename = \"imagenet-training-summary\"\n",
    "oai_qa_pair = convert_to_oai_format(question_answer_list, system_prompt_msg=system_prompt_msg)\n",
    "\n",
    "# save_result_jsonl = f\"{output_dir}/{save_filename}.jsonl\" # option 2\n",
    "save_result_jsonl = f\"{output_dir}/{save_filename}-oai.jsonl\" # option 1\n",
    "#save_jsonl(qa_pair, save_result_jsonl) # option 2\n",
    "save_jsonl(oai_qa_pair, save_result_jsonl) # option 1\n",
    "print(f\"Saved to {save_result_jsonl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf {splitted_raw_data_dir} pdf_image_tmp pdf_mixed_tmp outputs_tmp images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Synthetic Data\n",
    "\n",
    "---\n",
    "\n",
    "## Evolv-Instruct to generate more\n",
    "\n",
    "To generate more data from above seed data, we use [Evolve-Instruct](../evolve-instruct/README.md).\n",
    "![Evolv-Instruct](../evolve-instruct/evlvinstrc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EvolvMCTS4RL for RL\n",
    "\n",
    "To improve reasoning capability using RL (Reinforcement Learning), we use [EvolveMCTS4RL](../reasoningplaning/README.md) to generate data.\n",
    "\n",
    "\n",
    "![EvolvMCTS4RL](../reasoningplaning/evolveMCTS4RL.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
