{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuning Open Source LLM using the Azure ML Python SDK (Custom Script)\n",
    "\n",
    "### Overview\n",
    "\n",
    "There are several cases where you might want to use custom scripts without MLflow in Azure ML.\n",
    "\n",
    "#### 1. Compatibility with existing workflows\n",
    "\n",
    "There are times when you don't want to use MLflow to maintain compatibility with existing workflows or toolchains. For example:\n",
    "\n",
    "-   Customized logging solution: You already have a separate solution in place for logging and tracing.\n",
    "-   Requiring a specific format of data logging: If you need data logging in a specific format that is not MLflow's format.\n",
    "\n",
    "#### 2. Need more granular control\n",
    "\n",
    "You need more granular control over the training and inference process. MLflow provides a lot of convenience, but sometimes it makes it difficult to have granular control.\n",
    "\n",
    "#### 3. Simple use cases\n",
    "\n",
    "If your use case is simple enough that you don't need all of MLflow's features, you might be able to get by with basic AzureML functionality. If you're working on a toy project or a simple model training task and want to get by without complex tools, start with simple code.\n",
    "\n",
    "#### 4. Security and compliance\n",
    "\n",
    "You cannot use external tools because of specific security and compliance requirements.\n",
    "\n",
    "-   Data security: You can't use external logging services or data stores due to specific data security requirements.\n",
    "-   Regulatory compliance: When data must be stored in a specific format or location due to specific regulatory compliance requirements.\n",
    "\n",
    "This notebook shows a basic example of training a model with a custom script.\n",
    "\n",
    "[Note] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: python31014jvsc74a57bd01f90a0206bde5cf3732dab79adbbcc7570d5fab64b89fc69d46a8fe33664a709\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "lab_prep_dir = os.getcwd().split(\"SLMWorkshopCN\")[0] + \"SLMWorkshopCN/0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(lab_prep_dir))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 11:03:56,762 - logger - INFO - ===== 0. Azure ML Training Info =====\n",
      "2025-03-12 11:03:56,766 - logger - INFO - AZURE_SUBSCRIPTION_ID=49aee8bf-3f02-464f-a0ba-e3467e7d85e2\n",
      "2025-03-12 11:03:56,769 - logger - INFO - AZURE_RESOURCE_GROUP=rg-slmwrkshp_9\n",
      "2025-03-12 11:03:56,771 - logger - INFO - AZURE_WORKSPACE=mlw-pgwgybluulpec\n",
      "2025-03-12 11:03:56,773 - logger - INFO - AZURE_DATA_NAME=lgds-sftdemo241201\n",
      "2025-03-12 11:03:56,775 - logger - INFO - DATA_DIR=./dataset\n",
      "2025-03-12 11:03:56,776 - logger - INFO - CLOUD_DIR=./cloud\n",
      "2025-03-12 11:03:56,778 - logger - INFO - HF_MODEL_NAME_OR_PATH=microsoft/Phi-3.5-mini-instruct\n",
      "2025-03-12 11:03:56,779 - logger - INFO - IS_DEBUG=True\n",
      "2025-03-12 11:03:56,781 - logger - INFO - USE_LOWPRIORITY_VM=False\n",
      "2025-03-12 11:03:56,788 - logger - INFO - azure_env_name=llm-finetuning-2024-11-05\n",
      "2025-03-12 11:03:56,793 - logger - INFO - azure_compute_cluster_name=gpu-h100\n",
      "2025-03-12 11:03:56,795 - logger - INFO - azure_compute_cluster_size=Standard_NC40ads_H100_v5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()#相关机密配置写在.env中，并.gitignore掉.env\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(\"config_prd.yml\") as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "AZURE_SUBSCRIPTION_ID = d[\"config\"][\"AZURE_SUBSCRIPTION_ID\"]\n",
    "AZURE_RESOURCE_GROUP = d[\"config\"][\"AZURE_RESOURCE_GROUP\"]\n",
    "AZURE_WORKSPACE = d[\"config\"][\"AZURE_WORKSPACE\"]\n",
    "AZURE_DATA_NAME = d[\"config\"][\"AZURE_DATA_NAME\"]\n",
    "DATA_DIR = d[\"config\"][\"DATA_DIR\"]\n",
    "CLOUD_DIR = d[\"config\"][\"CLOUD_DIR\"]\n",
    "HF_MODEL_NAME_OR_PATH = d[\"config\"][\"HF_MODEL_NAME_OR_PATH\"]\n",
    "IS_DEBUG = d[\"config\"][\"IS_DEBUG\"]\n",
    "USE_LOWPRIORITY_VM = d[\"config\"][\"USE_LOWPRIORITY_VM\"]\n",
    "\n",
    "azure_env_name = d[\"train\"][\"azure_env_name\"]\n",
    "azure_compute_cluster_name = d[\"train\"][\"azure_compute_cluster_name\"]\n",
    "azure_compute_cluster_size = d[\"train\"][\"azure_compute_cluster_size\"]\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CLOUD_DIR, exist_ok=True)\n",
    "\n",
    "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
    "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
    "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
    "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
    "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
    "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
    "\n",
    "logger.info(f\"azure_env_name={azure_env_name}\")\n",
    "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
    "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Dataset preparation\n",
    "\n",
    "---\n",
    "\n",
    "Preparing dataset is the first step in training a model. You can use the `datasets` library to load the dataset if you want to use Hugging Face datasets.<br>\n",
    "Otherwise, you can use your own dataset from previous hands-on sessions.\n",
    "\n",
    "We have prepared a dataset, [`lab1_augmented_samples.json`](lab1_augmented_samples.json), for this hands-on session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luogang/anaconda3/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "USE_HF_DATASETS = False  # Determine if we use Hugging Face Datasets or not\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "from logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 11:04:10,177 - logger - INFO - ===== 1. Custom Dataset preparation from Lab 1.  =====\n",
      "2025-03-12 11:04:10,180 - logger - INFO - Preparing dataset.\n",
      "2025-03-12 11:04:10,226 - logger - INFO - Save dataset to ./dataset\n"
     ]
    }
   ],
   "source": [
    "if not USE_HF_DATASETS:\n",
    "\n",
    "    # Function to load data from the provided file and convert to JSONL format for single-turn conversations\n",
    "    def load_and_convert_to_jsonl(\n",
    "        file_path, system_prompt_msg=\"You're an AI assistant.\"\n",
    "    ):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for item in data:\n",
    "            jsonl_entry = {\n",
    "                \"prompt\": system_prompt_msg,\n",
    "                \"messages\": [\n",
    "                    {\"content\": item[\"input\"], \"role\": \"user\"},\n",
    "                    {\"content\": item[\"output\"], \"role\": \"assistant\"},\n",
    "                ],\n",
    "            }\n",
    "            result.append(json.dumps(jsonl_entry))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def save_jsonl_data(jsonl_data, file_path):\n",
    "        with open(file_path, \"w\") as file:\n",
    "            for entry in jsonl_data:\n",
    "                file.write(entry + \"\\n\")\n",
    "\n",
    "    # Function to split data into training and testing sets\n",
    "    def split_train_test(jsonl_data, train_size=0.8):\n",
    "        # Shuffle the data\n",
    "        random.shuffle(jsonl_data)\n",
    "\n",
    "        # Calculate split index\n",
    "        split_index = int(len(jsonl_data) * train_size)\n",
    "\n",
    "        # Split the data\n",
    "        train_data = jsonl_data[:split_index]\n",
    "        test_data = jsonl_data[split_index:]\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "    logger.info(f\"===== 1. Custom Dataset preparation from Lab 1.  =====\")\n",
    "    logger.info(f\"Preparing dataset.\")\n",
    "    file_path = \"lab1_augmented_samples.json\"\n",
    "    system_prompt_msg = \"You are the SME (Subject Matter Expert) in Distributed training on Cloud. Please answer the questions accurately.\"\n",
    "    jsonl_dataset = load_and_convert_to_jsonl(file_path, system_prompt_msg) # 转成训练数据的格式\n",
    "    train_dataset, test_dataset = split_train_test(jsonl_dataset, train_size=0.8)\n",
    "    logger.info(f\"Save dataset to {DATA_DIR}\")\n",
    "    save_jsonl_data(train_dataset, f\"{DATA_DIR}/train.jsonl\")\n",
    "    save_jsonl_data(test_dataset, f\"{DATA_DIR}/eval.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HF_DATASETS:\n",
    "    logger.info(f\"===== 1. Hugging Face Dataset preparation =====\")\n",
    "    logger.info(f\"Loading dataset. It may take several minutes to load the dataset.\")\n",
    "    # Load dataset from the hub\n",
    "    dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:2%]\")\n",
    "\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    if IS_DEBUG:\n",
    "        logger.info(\n",
    "            f\"Activated Debug mode. The number of sample was resampled to 1000.\"\n",
    "        )\n",
    "        dataset = dataset.select(range(1000))\n",
    "\n",
    "    logger.info(f\"Save dataset to {DATA_DIR}\")\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    train_dataset.to_json(f\"{DATA_DIR}/train.jsonl\")\n",
    "    test_dataset = dataset[\"test\"]\n",
    "    test_dataset.to_json(f\"{DATA_DIR}/eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 11:04:25,484 - logger - INFO - ===== 2. Training preparation =====\n",
      "2025-03-12 11:04:25,486 - logger - INFO - Calling DefaultAzureCredential.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f490e594250>,\n",
      "         subscription_id=49aee8bf-3f02-464f-a0ba-e3467e7d85e2,\n",
      "         resource_group_name=rg-slmwrkshp_9,\n",
      "         workspace_name=mlw-pgwgybluulpec)\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "logger.info(f\"===== 2. Training preparation =====\")\n",
    "logger.info(f\"Calling DefaultAzureCredential.\")\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE\n",
    ") # 创建AML workspace client, 其实一个AIF ai prj就对应了一个AML wrkspac\n",
    "\n",
    "# The code below may conflict with AI Foundry as of February 2025.\n",
    "# ml_client = None\n",
    "# try:\n",
    "#     ml_client = MLClient.from_config(credential)\n",
    "# except Exception as ex:\n",
    "#     print(ex)\n",
    "#     ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Create AzureML environment and data\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda).\n",
    "This hands-on uses conda yaml.\n",
    "\n",
    "Training data can be used as a dataset stored in the local development environment, but can also be registered as AzureML data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./cloud/train/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_DIR}/train/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - bitsandbytes==0.43.3\n",
    "    - transformers==4.44.2\n",
    "    - peft~=0.12\n",
    "    - accelerate~=0.33\n",
    "    - trl==0.10.1\n",
    "    - einops==0.8.0\n",
    "    - datasets==2.21.0\n",
    "    - mlflow==2.16.0\n",
    "    - azureml-mlflow==1.57.0\n",
    "    - azureml-sdk==1.57.0\n",
    "    - torchvision==0.19.0\n",
    "    - torch==2.4.0\n",
    "    - tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docker environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./cloud/train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_DIR}/train/Dockerfile\n",
    "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
    "\n",
    "USER root\n",
    "\n",
    "# support Deepspeed launcher requirement of passwordless ssh login\n",
    "RUN apt-get update && apt-get -y upgrade\n",
    "RUN pip install --upgrade pip\n",
    "RUN apt-get install -y openssh-server openssh-client\n",
    "\n",
    "# Install pip dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt --no-cache-dir\n",
    "\n",
    "RUN MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./cloud/train/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_DIR}/train/requirements.txt\n",
    "azureml-acft-accelerator==0.0.63\n",
    "azureml_acft_common_components==0.0.63\n",
    "azureml-acft-contrib-hf-nlp==0.0.63\n",
    "azureml-evaluate-mlflow==0.0.63\n",
    "azureml-metrics[text]==0.0.63\n",
    "mltable==1.6.1\n",
    "mpi4py==4.0.1\n",
    "sentencepiece==0.2.0\n",
    "transformers==4.46.1\n",
    "datasets==3.1.0\n",
    "accelerate==1.1.0\n",
    "diffusers==0.31.0\n",
    "onnxruntime==1.20.0\n",
    "rouge-score==0.1.2\n",
    "sacrebleu==2.4.3\n",
    "bitsandbytes==0.44.1\n",
    "einops==0.8.0\n",
    "aiohttp==3.10.11\n",
    "peft==0.13.2\n",
    "deepspeed==0.15.3\n",
    "trl==0.12.0\n",
    "tiktoken==0.8.0\n",
    "packaging==24.1\n",
    "timm==1.0.11\n",
    "azure-identity\n",
    "tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_environment_asset(\n",
    "    ml_client, env_name, conda_yml=\"cloud/conda.yml\", update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
    "            conda_file=conda_yml,\n",
    "            name=env_name,\n",
    "            description=\"Environment created for llm fine-tuning.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created/Updated Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "def get_or_create_docker_environment_asset(\n",
    "    ml_client, env_name, docker_dir, update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image) #AIF暂时没有,但AIF/Code有用到一个内置的env。这行代码创建env后，可在AML对应的wrkspac/Environments中找到\n",
    "        logger.info(f\"Created Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "def get_or_create_data_asset(ml_client, data_name, data_local_dir, update=False):\n",
    "\n",
    "    try:\n",
    "        latest_data_version = max(\n",
    "            [int(d.version) for d in ml_client.data.list(name=data_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Data asset, but will update the Data.\")\n",
    "        else:\n",
    "            data_asset = ml_client.data.get(name=data_name, version=latest_data_version)\n",
    "            logger.info(f\"Found Data asset: {data_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        data = Data(\n",
    "            path=data_local_dir,\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=f\"{data_name} for fine tuning\",\n",
    "            tags={\"FineTuningType\": \"Instruction\", \"Language\": \"En\"},\n",
    "            name=data_name,\n",
    "        )\n",
    "        data_asset = ml_client.data.create_or_update(data)  # AML对应的wrkspac/Data 中，或 AIF/AiPrj/Data+idx中\n",
    "        logger.info(f\"Created/Updated Data asset: {data_name}\")\n",
    "\n",
    "    return data_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 11:06:19,166 - logger - INFO - Found Environment asset: llm-finetuning-2024-11-05. Will not create again\n",
      "2025-03-12 11:06:21,330 - logger - INFO - Found Data asset: lgds-sftdemo241201. Will not create again\n"
     ]
    }
   ],
   "source": [
    "# env = get_or_create_environment_asset(ml_client, azure_env_name, conda_yml=f\"{CLOUD_DIR}/conda.yml\", update=False)\n",
    "env = get_or_create_docker_environment_asset(    #AIF暂时没有,但AIF/Code有用到一个内置的env。这行代码创建env后，可在AML对应的wrkspac/Environments中找到\n",
    "    ml_client, azure_env_name, docker_dir=f\"{CLOUD_DIR}/train\", update=False\n",
    ")\n",
    "data = get_or_create_data_asset(  # AML对应的wrkspac/Data 中，或 AIF/AiPrj/Data+idx中\n",
    "    ml_client, AZURE_DATA_NAME, data_local_dir=DATA_DIR, update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Training script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mDISABLE_MLFLOW_INTEGRATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]=\u001b[33m\"\u001b[39;49;00m\u001b[33mTRUE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mWANDB_DISABLED\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m load_dataset\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mpeft\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m LoraConfig\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtrl\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m SFTTrainer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m load_dataset\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mdatetime\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m datetime\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logging.basicConfig(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        datefmt=\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "logger.setLevel(logging.INFO)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mload_model\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_name_or_path = args.model_name_or_path    \u001b[37m\u001b[39;49;00m\n",
      "    model_kwargs = \u001b[36mdict\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        use_cache=\u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        trust_remote_code=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        torch_dtype=torch.bfloat16,\u001b[37m\u001b[39;49;00m\n",
      "        device_map=\u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.model_max_length = args.max_seq_length\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.pad_token = tokenizer.unk_token \u001b[34mif\u001b[39;49;00m tokenizer.unk_token \u001b[34melse\u001b[39;49;00m tokenizer.eos_token  \u001b[37m# Use unk rather than eos token to prevent endless generation. But eos_tkn is more general than unk_tkn\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.padding_side = \u001b[33m\"\u001b[39;49;00m\u001b[33mright\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model, tokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mapply_chat_template\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "    example,\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "):\u001b[37m\u001b[39;49;00m\n",
      "    messages = example[\u001b[33m\"\u001b[39;49;00m\u001b[33mmessages\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Add an empty system message if there is none\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m messages[\u001b[34m0\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mrole\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] != \u001b[33m\"\u001b[39;49;00m\u001b[33msystem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        messages.insert(\u001b[34m0\u001b[39;49;00m, {\u001b[33m\"\u001b[39;49;00m\u001b[33mrole\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33msystem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcontent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[37m\u001b[39;49;00m\n",
      "    example[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tokenizer.apply_chat_template(\u001b[37m# https://hf-mirror.com/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        messages, tokenize=\u001b[34mFalse\u001b[39;49;00m, add_generation_prompt=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m example\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mmain\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m###################\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Hyper-parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m###################\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# # Only overwrite environ if wandb param passed\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# if len(args.wandb_project) > 0:\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     os.environ['WANDB_API_KEY'] = args.wandb_api_key    \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     os.environ[\"WANDB_PROJECT\"] = args.wandb_project\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# if len(args.wandb_watch) > 0:\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     os.environ[\"WANDB_WATCH\"] = args.wandb_watch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# if len(args.wandb_log_model) > 0:\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#     os.environ[\"WANDB_LOG_MODEL\"] = args.wandb_log_model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# use_wandb = len(args.wandb_project) > 0 or (\"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0) \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    os.makedirs(args.model_dir, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>>model_dir>>:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mos.path.abspath(args.model_dir)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>>output_dir>>:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mos.path.abspath(args.output_dir)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    output_dir = os.path.join(os.path.abspath(args.model_dir), args.output_dir)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>>output_dir_NEW>>:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mos.path.abspath(output_dir)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    os.makedirs(output_dir, exist_ok=\u001b[34mTrue\u001b[39;49;00m)    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    training_config = {           \u001b[37m# HF param\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbf16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mdo_eval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mFalse\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.learning_rate,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlog_level\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33minfo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlogging_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.logging_steps,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlogging_strategy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlr_scheduler_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.lr_scheduler_type,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_train_epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmax_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: -\u001b[34m1\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33moutput_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: output_dir,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33moverwrite_output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mper_device_train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mper_device_eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.eval_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mremove_unused_columns\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msave_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.save_steps,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msave_total_limit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mseed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.seed,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mgradient_checkpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mgradient_checkpointing_kwargs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\u001b[33m\"\u001b[39;49;00m\u001b[33muse_reentrant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mFalse\u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mgradient_accumulation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.grad_accum_steps,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mwarmup_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.warmup_ratio,\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    peft_config = {           \u001b[37m# HF param\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.lora_r,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlora_alpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.lora_alpha,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlora_dropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.lora_dropout,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbias\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtask_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mCAUSAL_LM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#\"target_modules\": \"all-linear\",\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtarget_modules\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: [\u001b[33m\"\u001b[39;49;00m\u001b[33mq_proj\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mk_proj\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mv_proj\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mo_proj\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mgate_proj\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mup_proj\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mdown_proj\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmodules_to_save\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    train_conf = TrainingArguments(           \u001b[37m# HF param\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        **training_config,\u001b[37m\u001b[39;49;00m\n",
      "        report_to= [\u001b[33m\"\u001b[39;49;00m\u001b[33mtensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[37m#\"wandb\" if use_wandb else \"none\",\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        run_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mcustom_run\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,    \u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m>>logging_dir>>:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_conf.logging_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    os.makedirs(train_conf.logging_dir, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    peft_conf = LoraConfig(**peft_config)\u001b[37m\u001b[39;49;00m\n",
      "    model, tokenizer = load_model(args)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m###############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Setup logging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m###############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    log_level = train_conf.get_process_log_level()\u001b[37m\u001b[39;49;00m\n",
      "    logger.setLevel(log_level)\u001b[37m\u001b[39;49;00m\n",
      "    datasets.utils.logging.set_verbosity(log_level)\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.set_verbosity(log_level)\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.enable_default_handler()\u001b[37m\u001b[39;49;00m\n",
      "    transformers.utils.logging.enable_explicit_format()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Log on each process a small summary\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.warning(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mProcess rank: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_conf.local_rank\u001b[33m}\u001b[39;49;00m\u001b[33m, device: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_conf.device\u001b[33m}\u001b[39;49;00m\u001b[33m, n_gpu: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_conf.n_gpu\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        + \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m distributed training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mbool\u001b[39;49;00m(train_conf.local_rank\u001b[37m \u001b[39;49;00m!=\u001b[37m \u001b[39;49;00m-\u001b[34m1\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m, 16-bits training: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_conf.fp16\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining/evaluation parameters \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_conf\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPEFT parameters \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpeft_conf\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m##################\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data Processing\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m##################\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# 数据流（可选）：在1_training_mlflow_phi3.ipynb/Data Prepare中把原始数据转成训练所需格式保存到本地的DATA_DIR；把DATA_DIR中的数据作为data asset上传到azure；ipynb/Start training job中把data asset的名字和版本传给该程序的args.train_dir；因此这里的train_dir就是指向从data asset拉下来的本地地址了\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTRAIN_DIR:>>\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.train_dir\u001b[33m}\u001b[39;49;00m\u001b[33m<<\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = load_dataset(\u001b[33m'\u001b[39;49;00m\u001b[33mjson\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, data_files=os.path.join(args.train_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.jsonl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    eval_dataset = load_dataset(\u001b[33m'\u001b[39;49;00m\u001b[33mjson\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, data_files=os.path.join(args.train_dir, \u001b[33m'\u001b[39;49;00m\u001b[33meval.jsonl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), split=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mrm col:>>\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_dataset.features\u001b[33m}\u001b[39;49;00m\u001b[33m<<\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    column_names = \u001b[36mlist\u001b[39;49;00m(train_dataset.features)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    processed_train_dataset = train_dataset.map(\u001b[37m\u001b[39;49;00m\n",
      "        apply_chat_template,\u001b[37m\u001b[39;49;00m\n",
      "        fn_kwargs={\u001b[33m\"\u001b[39;49;00m\u001b[33mtokenizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tokenizer},\u001b[37m\u001b[39;49;00m\n",
      "        num_proc=\u001b[34m10\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        remove_columns=column_names,\u001b[37m\u001b[39;49;00m\n",
      "        desc=\u001b[33m\"\u001b[39;49;00m\u001b[33mApplying chat template to train_sft\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    processed_eval_dataset = eval_dataset.map(\u001b[37m\u001b[39;49;00m\n",
      "        apply_chat_template,\u001b[37m\u001b[39;49;00m\n",
      "        fn_kwargs={\u001b[33m\"\u001b[39;49;00m\u001b[33mtokenizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tokenizer},\u001b[37m\u001b[39;49;00m\n",
      "        num_proc=\u001b[34m10\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        remove_columns=column_names,\u001b[37m\u001b[39;49;00m\n",
      "        desc=\u001b[33m\"\u001b[39;49;00m\u001b[33mApplying chat template to test_sft\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m###########\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m###########\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    trainer = SFTTrainer(\u001b[37m\u001b[39;49;00m\n",
      "        model=model,\u001b[37m\u001b[39;49;00m\n",
      "        args=train_conf,\u001b[37m\u001b[39;49;00m\n",
      "        peft_config=peft_conf,\u001b[37m\u001b[39;49;00m\n",
      "        train_dataset=processed_train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        eval_dataset=processed_eval_dataset,\u001b[37m\u001b[39;49;00m\n",
      "        max_seq_length=args.max_seq_length,\u001b[37m\u001b[39;49;00m\n",
      "        dataset_text_field=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "        packing=\u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Show current memory stats\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    gpu_stats = torch.cuda.get_device_properties(\u001b[34m0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    start_gpu_memory = \u001b[36mround\u001b[39;49;00m(torch.cuda.max_memory_reserved() / \u001b[34m1024\u001b[39;49;00m / \u001b[34m1024\u001b[39;49;00m / \u001b[34m1024\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    max_memory = \u001b[36mround\u001b[39;49;00m(gpu_stats.total_memory / \u001b[34m1024\u001b[39;49;00m / \u001b[34m1024\u001b[39;49;00m / \u001b[34m1024\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mGPU = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mgpu_stats.name\u001b[33m}\u001b[39;49;00m\u001b[33m. Max memory = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmax_memory\u001b[33m}\u001b[39;49;00m\u001b[33m GB.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mstart_gpu_memory\u001b[33m}\u001b[39;49;00m\u001b[33m GB of memory reserved.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    trainer_stats = trainer.train() \u001b[37m## train\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Logging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    metrics = trainer_stats.metrics\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Show final memory and time stats \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    used_memory = \u001b[36mround\u001b[39;49;00m(torch.cuda.max_memory_reserved() / \u001b[34m1024\u001b[39;49;00m / \u001b[34m1024\u001b[39;49;00m / \u001b[34m1024\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    used_memory_for_lora = \u001b[36mround\u001b[39;49;00m(used_memory - start_gpu_memory, \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    used_percentage = \u001b[36mround\u001b[39;49;00m(used_memory         /max_memory*\u001b[34m100\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    lora_percentage = \u001b[36mround\u001b[39;49;00m(used_memory_for_lora/max_memory*\u001b[34m100\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmetrics[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m seconds used for training.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mround\u001b[39;49;00m(metrics[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]/\u001b[34m60\u001b[39;49;00m,\u001b[37m \u001b[39;49;00m\u001b[34m2\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m minutes used for training.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPeak reserved memory = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mused_memory\u001b[33m}\u001b[39;49;00m\u001b[33m GB.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPeak reserved memory for training = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mused_memory_for_lora\u001b[33m}\u001b[39;49;00m\u001b[33m GB.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPeak reserved memory % of max memory = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mused_percentage\u001b[33m}\u001b[39;49;00m\u001b[33m %.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPeak reserved memory for training % of max memory = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlora_percentage\u001b[33m}\u001b[39;49;00m\u001b[33m %.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    trainer.log_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, metrics)\u001b[37m\u001b[39;49;00m\n",
      "    trainer.save_metrics(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, metrics)\u001b[37m\u001b[39;49;00m\n",
      "    trainer.save_state() \u001b[37m# 保存trainer的状态\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Evaluation\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# tokenizer.padding_side = \"left\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# metrics = trainer.evaluate()\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# metrics[\"eval_samples\"] = len(processed_eval_dataset)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# trainer.log_metrics(\"eval\", metrics)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# trainer.save_metrics(\"eval\", metrics)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# ############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# # Save model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# ############\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.save_merged_model:\u001b[37m\u001b[39;49;00m\n",
      "        model_tmp_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_tmp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        os.makedirs(model_tmp_dir, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(model_tmp_dir)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSave merged model: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mpeft\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m AutoPeftModelForCausalLM\u001b[37m\u001b[39;49;00m\n",
      "        model = AutoPeftModelForCausalLM.from_pretrained(model_tmp_dir, low_cpu_mem_usage=\u001b[34mTrue\u001b[39;49;00m, torch_dtype=torch.bfloat16)\u001b[37m\u001b[39;49;00m\n",
      "        merged_model = model.merge_and_unload()\u001b[37m\u001b[39;49;00m\n",
      "        merged_model.save_pretrained(args.model_dir, safe_serialization=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSave PEFT model: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(args.model_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.save_pretrained(args.model_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mparse_args\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# setup argparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# curr_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# hyperparameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name_or_path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mmicrosoft/Phi-3.5-mini-instruct\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mInput directory for training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m# Qwen/Qwen2.5-0.5B-Instruct, microsoft/Phi-3.5-mini-instruct\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mInput directory for training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m./model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moutput directory for model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m./ckp_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mdirectory to temporarily store when training a model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mtraining - mini batch size for each gpu/process\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation - mini batch size for each gpu/process\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m5e-06\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--logging_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlogging steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33msave steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--grad_accum_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgradient accumulation steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr_scheduler_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mlinear\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mseed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwarmup ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m2048\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmax seq length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save_merged_model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# lora hyperparameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_r\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlora r\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_alpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlora alpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_dropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m0.05\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlora dropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# # wandb params\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--wandb_api_key\", type=str, default=\"\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--wandb_project\", type=str, default=\"\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--wandb_run_name\", type=str, default=\"\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--wandb_watch\", type=str, default=\"gradients\") # options: false | gradients | all\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--wandb_log_model\", type=str, default=\"false\") # options: false | true\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parse args\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    args = parser.parse_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# return args\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m args\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#sys.argv = ['']\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    args = parse_args()\u001b[37m\u001b[39;49;00m\n",
      "    main(args)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src_train/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Training\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create the compute cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 11:13:01,296 - logger - INFO - ===== 3. Training ===== \n",
      " in gpu-h100 with size Standard_NC40ads_H100_v5\n",
      "2025-03-12 11:13:03,516 - logger - INFO - Looks like the compute cluster doesn't exist. Creating a new one with compute size Standard_NC40ads_H100_v5!\n",
      "2025-03-12 11:13:03,519 - logger - INFO - Attempt #1 - Trying to create a dedicated compute\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "logger.info(f\"===== 3. Training ===== \\n in {azure_compute_cluster_name} with size {azure_compute_cluster_size}\")\n",
    "### Create the compute cluster\n",
    "try:\n",
    "    compute = ml_client.compute.get(azure_compute_cluster_name)\n",
    "    logger.info(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    logger.info(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {azure_compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        logger.info(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        tier = \"LowPriority\" if USE_LOWPRIORITY_VM else \"Dedicated\"\n",
    "        compute = AmlCompute(\n",
    "            name=azure_compute_cluster_name,\n",
    "            size=azure_compute_cluster_size,\n",
    "            tier=tier,\n",
    "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        logger.info(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Start training job\n",
    "\n",
    "The `command` allows user to configure the following key aspects.\n",
    "\n",
    "-   `inputs` - This is the dictionary of inputs using name value pairs to the command.\n",
    "    -   `type` - The type of input. This can be a `uri_file` or `uri_folder`. The default is `uri_folder`.\n",
    "    -   `path` - The path to the file or folder. These can be local or remote files or folders. For remote files - http/https, wasb are supported.\n",
    "        -   Azure ML `data`/`dataset` or `datastore` are of type `uri_folder`. To use `data`/`dataset` as input, you can use registered dataset in the workspace using the format '<data_name>:<version>'. For e.g Input(type='uri_folder', path='my_dataset:1')\n",
    "    -   `mode` - Mode of how the data should be delivered to the compute target. Allowed values are `ro_mount`, `rw_mount` and `download`. Default is `ro_mount`\n",
    "-   `code` - This is the path where the code to run the command is located\n",
    "-   `compute` - The compute on which the command will run. You can run it on the local machine by using `local` for the compute.\n",
    "-   `command` - This is the command that needs to be run\n",
    "    in the `command` using the `${{inputs.<input_name>}}` expression. To use files or folders as inputs, we can use the `Input` class. The `Input` class supports three parameters:\n",
    "-   `environment` - This is the environment needed for the command to run. Curated (built-in) or custom environments from the workspace can be used.\n",
    "-   `instance_count` - Number of nodes. Default is 1.\n",
    "-   `distribution` - Distribution configuration for distributed training scenarios. Azure Machine Learning supports PyTorch, TensorFlow, and MPI-based distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 11:16:34,543 - logger - INFO - Env: llm-finetuning-2024-11-05@latest\n",
      "2025-03-12 11:16:34,546 - logger - INFO - Command: python train.py --train_dir ${{inputs.train_dir}}             --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}}             --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}\n",
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading src_train (0.03 MBs): 100%|██████████| 26273/26273 [00:01<00:00, 13933.12it/s]\n",
      "\u001b[39m\n",
      "\n",
      "2025-03-12 11:16:55,607 - logger - INFO - Started training job. Now a dedicated Compute Cluster for training is provisioned and the environment\n",
      "required for training is automatically set up from Environment.\n",
      "\n",
      "If you have set up a new custom Environment, it will take approximately 20 minutes or more to set up the Environment before provisioning the training cluster.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: frosty_nail_1jnj0bh3xp\n",
      "Web View: https://ml.azure.com/runs/frosty_nail_1jnj0bh3xp?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: frosty_nail_1jnj0bh3xp\n",
      "Web View: https://ml.azure.com/runs/frosty_nail_1jnj0bh3xp?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "\n",
    "USE_BUILTIN_ENV = False\n",
    "str_command = \"\"\n",
    "\n",
    "if USE_BUILTIN_ENV:\n",
    "    str_env = \"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/77\"  # Use built-in Environment asset\n",
    "    str_command += \"pip install -r requirements.txt && \"\n",
    "else:\n",
    "    str_env = f\"{azure_env_name}@latest\"  # Use Curated (built-in) Environment asset\n",
    "\n",
    "str_command += \"python train.py --train_dir ${{inputs.train_dir}} \\\n",
    "            --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} \\\n",
    "            --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}\"\n",
    "\n",
    "logger.info(f\"Env: {str_env}\")\n",
    "logger.info(f\"Command: {str_command}\")\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        # train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
    "        train_dir=Input(path=f\"{AZURE_DATA_NAME}@latest\"),  # Get data from Data asset\n",
    "        epoch=d[\"train\"][\"epoch\"],\n",
    "        train_batch_size=d[\"train\"][\"train_batch_size\"],\n",
    "        eval_batch_size=d[\"train\"][\"eval_batch_size\"],\n",
    "        model_dir=d[\"train\"][\"model_dir\"],\n",
    "    ),\n",
    "    code=\"./src_train\",  # local path where the code is stored\n",
    "    compute=azure_compute_cluster_name,\n",
    "    command=str_command,\n",
    "    environment=str_env,\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        \"process_count_per_instance\": 1,  # For multi-gpu training set this to an integer value more than 1\n",
    "    },\n",
    ")\n",
    "\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "logger.info(\n",
    "    \"\"\"Started training job. Now a dedicated Compute Cluster for training is provisioned and the environment\n",
    "required for training is automatically set up from Environment.\n",
    "\n",
    "If you have set up a new custom Environment, it will take approximately 20 minutes or more to set up the Environment before provisioning the training cluster.\n",
    "\"\"\"\n",
    ")\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>phi3</td><td>frosty_nail_1jnj0bh3xp</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/frosty_nail_1jnj0bh3xp?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec&amp;tid=16b3c013-d300-468d-ac64-7eda0820b6d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "Command({'parameters': {}, 'init': False, 'name': 'frosty_nail_1jnj0bh3xp', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/butterluo/SLMWorkshopCN.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '3c7fbaf7cccbaa7bf383f5aad52b6b0052909ac8', 'azureml.git.dirty': 'True', '_azureml.ComputeTargetType': 'amlctrain', '_azureml.ClusterName': 'gpu-h100', 'ContentSnapshotId': '63bb3740-dc51-44af-81ea-b315306e2810'}, 'print_as_yaml': False, 'id': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/jobs/frosty_nail_1jnj0bh3xp', 'Resource__source_path': '', 'base_path': '/mnt/d/BT/SRC/NLP/LLM/SFT/SLMWorkshopCN/2_slm-fine-tuning-mlstudio/phi3', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f49552eb700>, 'serialize': <msrest.serialization.Serializer object at 0x7f495512d7b0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <TraceLogger attr_dict (WARNING)>, 'display_name': 'frosty_nail_1jnj0bh3xp', 'experiment_name': 'phi3', 'compute': 'gpu-h100', 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/frosty_nail_1jnj0bh3xp?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'train_dir': {'type': 'uri_folder', 'path': 'lgds-sftdemo241201:1', 'mode': 'ro_mount'}, 'epoch': '1', 'train_batch_size': '8', 'eval_batch_size': '8', 'model_dir': './outputs'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.frosty_nail_1jnj0bh3xp', 'mode': 'rw_mount'}}, 'inputs': {'train_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f495512d6c0>, 'epoch': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f495512d750>, 'train_batch_size': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f495512d3c0>, 'eval_batch_size': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f495512d570>, 'model_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f495512d420>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f495512d960>}, 'component': CommandComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'frosty_nail_1jnj0bh3xp', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/d/BT/SRC/NLP/LLM/SFT/SLMWorkshopCN/2_slm-fine-tuning-mlstudio/phi3', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f49552eb700>, 'serialize': <msrest.serialization.Serializer object at 0x7f49552eb3d0>, 'command': 'python train.py --train_dir ${{inputs.train_dir}}             --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}}             --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}', 'code': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/codes/9ae068f7-97be-4073-b4bf-f760548da30b/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/environments/llm-finetuning-2024-11-05/versions/2', 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f49552eba90>, 'resources': None, 'queue_settings': None, 'version': None, 'schema': None, 'type': 'command', 'display_name': 'frosty_nail_1jnj0bh3xp', 'is_deterministic': True, 'inputs': {'train_dir': {'type': 'uri_folder', 'path': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/data/lgds-sftdemo241201/versions/1', 'mode': 'ro_mount'}, 'epoch': {'type': 'string', 'default': '1'}, 'train_batch_size': {'type': 'string', 'default': '8'}, 'eval_batch_size': {'type': 'string', 'default': '8'}, 'model_dir': {'type': 'string', 'default': './outputs'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.frosty_nail_1jnj0bh3xp', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': []}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/frosty_nail_1jnj0bh3xp?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f49552eb700>}, 'instance_id': 'a799a9a3-57f1-4780-96ef-a0964497fb60', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f49552eba90>, 'environment_variables': {}, 'environment': 'llm-finetuning-2024-11-05:2', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(returned_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the `trained_model` output is available\n",
    "job_name = returned_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. (Optional) Create model asset and get fine-tuned LLM to local folder\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name,\n",
    "    job_name,\n",
    "    model_dir=\"outputs\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_model_version = max(\n",
    "            [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Model asset, but will update the Model.\")\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(\n",
    "                name=model_name, version=latest_model_version\n",
    "            )\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        model_path = f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"\n",
    "        run_model = Model(\n",
    "            name=model_name,\n",
    "            path=model_path,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type,  # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        logger.info(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_model_name = d[\"serve\"][\"azure_model_name\"]\n",
    "model_dir = d[\"train\"][\"model_dir\"]\n",
    "model = get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    azure_model_name,\n",
    "    job_name,\n",
    "    model_dir,\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"===== 4. (Optional) Create model asset and get fine-tuned LLM to local folder =====\"\n",
    ")\n",
    "logger.info(f\"azure_model_name={azure_model_name}\")\n",
    "logger.info(f\"model_dir={model_dir}\")\n",
    "logger.info(f\"model={model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Get fine-tuned LLM to local folder\n",
    "\n",
    "You can copy it to your local directory to perform inference or serve the model in Azure environment. (e.g., real-time endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the model (this is optional)\n",
    "local_model_dir = \"./artifact_downloads\"\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "ml_client.models.download(\n",
    "    name=azure_model_name, download_path=local_model_dir, version=model.version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $DATA_DIR {local_model_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
