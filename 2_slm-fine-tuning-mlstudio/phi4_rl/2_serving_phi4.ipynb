{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796cf06d-e4d3-422e-8301-a961e9520f52",
   "metadata": {},
   "source": [
    "# Open Source LLM serving using the Azure ML Python SDK\n",
    "\n",
    "[Note] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "840136a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: python31014jvsc74a57bd01f90a0206bde5cf3732dab79adbbcc7570d5fab64b89fc69d46a8fe33664a709\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "lab_prep_dir = os.getcwd().split(\"SLMWorkshopCN\")[0] + \"SLMWorkshopCN/0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(lab_prep_dir))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538d1c11-a30d-4342-85d5-ba59e1890b76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy_gold_q5lypvqkw5\t\n"
     ]
    }
   ],
   "source": [
    "# %store -r job_name\n",
    "job_name = \"happy_gold_q5lypvqkw5\t\" # 这里是手动指定job_name。上一行是使用在1_training_custom_phi3.ipynb或1_training_mlflow_phi3.ipynb中保存的job_name\n",
    "try:\n",
    "    job_name\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the previous notebook (model training) again.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(job_name)#在训练Notebook中要把之前训练模型的job名字记录下来，这里有用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb139b1d-500c-4ef2-9af3-728f2a5ea05f",
   "metadata": {},
   "source": [
    "## 1. Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5234c47-b3e5-4218-8a98-3988c8991643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 22:28:25,704 - logger - INFO - ===== 0. Azure ML Deployment Info =====\n",
      "2025-02-26 22:28:25,706 - logger - INFO - AZURE_SUBSCRIPTION_ID=49aee8bf-3f02-464f-a0ba-e3467e7d85e2\n",
      "2025-02-26 22:28:25,707 - logger - INFO - AZURE_RESOURCE_GROUP=rg-slmwrkshp_9\n",
      "2025-02-26 22:28:25,709 - logger - INFO - AZURE_WORKSPACE=mlw-pgwgybluulpec\n",
      "2025-02-26 22:28:25,710 - logger - INFO - AZURE_DATA_NAME=lgds-gsm8k-main-demo\n",
      "2025-02-26 22:28:25,711 - logger - INFO - DATA_DIR=./dataset\n",
      "2025-02-26 22:28:25,712 - logger - INFO - CLOUD_DIR=./cloud\n",
      "2025-02-26 22:28:25,713 - logger - INFO - HF_MODEL_NAME_OR_PATH=microsoft/phi-4\n",
      "2025-02-26 22:28:25,714 - logger - INFO - azure_env_name=llm-srv-2024-11-05\n",
      "2025-02-26 22:28:25,714 - logger - INFO - azure_model_name=phi4-grpo-2024-11-05\n",
      "2025-02-26 22:28:25,715 - logger - INFO - azure_endpoint_name=phi4-endpoint-2024-11-05\n",
      "2025-02-26 22:28:25,716 - logger - INFO - azure_deployment_name=phi4-blue\n",
      "2025-02-26 22:28:25,717 - logger - INFO - azure_serving_cluster_size=Standard_NC40ads_H100_v5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open('config_prd.yml') as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
    "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
    "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
    "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
    "DATA_DIR = d['config']['DATA_DIR']\n",
    "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
    "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
    "\n",
    "azure_env_name = d['serve']['azure_env_name']\n",
    "azure_model_name = d['serve']['azure_model_name']\n",
    "azure_endpoint_name = d['serve']['azure_endpoint_name']\n",
    "azure_deployment_name = d['serve']['azure_deployment_name']\n",
    "azure_serving_cluster_size = d['serve']['azure_serving_cluster_size']\n",
    "\n",
    "\n",
    "logger.info(\"===== 0. Azure ML Deployment Info =====\")\n",
    "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
    "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
    "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
    "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "\n",
    "logger.info(f\"azure_env_name={azure_env_name}\")\n",
    "logger.info(f\"azure_model_name={azure_model_name}\")\n",
    "logger.info(f\"azure_endpoint_name={azure_endpoint_name}\")\n",
    "logger.info(f\"azure_deployment_name={azure_deployment_name}\")\n",
    "logger.info(f\"azure_serving_cluster_size={azure_serving_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843e0f-3cf1-4e86-abb7-a49919fac8d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Serving preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb4a273-ba31-4f47-a2fd-dc8cdea390f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 22:28:37,249 - logger - INFO - ===== 2. Serving preparation =====\n",
      "2025-02-26 22:28:37,250 - logger - INFO - Calling DefaultAzureCredential.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7fb14415b670>,\n",
      "         subscription_id=49aee8bf-3f02-464f-a0ba-e3467e7d85e2,\n",
      "         resource_group_name=rg-slmwrkshp_9,\n",
      "         workspace_name=mlw-pgwgybluulpec)\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "logger.info(f\"===== 2. Serving preparation =====\")\n",
    "logger.info(f\"Calling DefaultAzureCredential.\")\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE\n",
    ") # 创建AML workspace client, 其实一个AIF ai prj就对应了一个AML wrkspac\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a377b-d10c-413e-a67b-2c11a3cff7fd",
   "metadata": {},
   "source": [
    "### 2.2. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73532c39-3fdd-40a7-b2be-9f5a2f22443a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(ml_client, model_name, job_name, model_dir=\"outputs\", model_type=\"custom_model\", update=False):\n",
    "    \n",
    "    try:\n",
    "        latest_model_version = max([int(m.version) for m in ml_client.models.list(name=model_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Model asset, but will update the Model.')\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(name=model_name, version=latest_model_version)\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")        \n",
    "        model_path = f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"    # 从训练job的输出目录拿到模型权重\n",
    "        run_model = Model(\n",
    "            name=model_name,        \n",
    "            path=model_path,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)#AIF/ai prj/Models+Endpoints但只有deployment和endpoint而无权重; AML/wrkspc/Models有权重文件，该代码就是注册模型到这里。\n",
    "        logger.info(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aaa9671-fc98-4e5e-a70c-7771caa1c7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 22:29:18,372 - logger - INFO - Found Model asset: phi4-grpo-2024-11-05. Will not create again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation_context:\n",
      "  created_at: '2025-02-26T13:18:56.848645+00:00'\n",
      "  created_by: Gang Luo\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2025-02-26T13:18:56.848645+00:00'\n",
      "  last_modified_by: Gang Luo\n",
      "  last_modified_by_type: User\n",
      "description: Model created from run.\n",
      "id: azureml:/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/models/phi4-grpo-2024-11-05/versions/1\n",
      "job_name: happy_gold_q5lypvqkw5\n",
      "name: phi4-grpo-2024-11-05\n",
      "path: azureml://subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.happy_gold_q5lypvqkw5/outputs\n",
      "properties: {}\n",
      "stage: Development\n",
      "tags: {}\n",
      "type: custom_model\n",
      "version: '1'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_dir = d['train']['model_dir']\n",
    "model = get_or_create_model_asset(ml_client, azure_model_name, job_name, model_dir, model_type=\"custom_model\", update=False)\n",
    "print(model) #AIF/ai prj/Models+Endpoints但只有deployment和endpoint而无权重; AML/wrkspc/Models有权重文件，该代码就是注册模型到这里。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df561a-7846-4450-a2dd-4af6396b1719",
   "metadata": {},
   "source": [
    "### 2.3. Create AzureML environment\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda). This hands-on uses Docker container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac357a-3944-4702-a338-b9f4b67dadc9",
   "metadata": {},
   "source": [
    "#### Docker environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ace0e7-e49c-4d8a-ac2f-604b6e00b145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./cloud/serve/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_DIR}/serve/Dockerfile\n",
    "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
    "\n",
    "# Install pip dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt --no-cache-dir\n",
    "\n",
    "# Inference requirements\n",
    "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20230419.v1 /artifacts /var/\n",
    "\n",
    "RUN /var/requirements/install_system_requirements.sh && \\\n",
    "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
    "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
    "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
    "    rm -f /etc/nginx/sites-enabled/default\n",
    "ENV SVDIR=/var/runit\n",
    "ENV WORKER_TIMEOUT=400\n",
    "EXPOSE 5001 8883 8888\n",
    "\n",
    "# support Deepspeed launcher requirement of passwordless ssh login\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y openssh-server openssh-client\n",
    "\n",
    "RUN pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e4b20f8-d76c-46ca-bea8-b501c4210210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./cloud/serve/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_DIR}/serve/requirements.txt\n",
    "azureml-mlflow==1.58.0\n",
    "accelerate==1.4.0\n",
    "beautifulsoup4==4.13.3\n",
    "bitsandbytes==0.45.3\n",
    "datasets==3.3.2\n",
    "deepspeed==0.15.4\n",
    "huggingface_hub==0.29.1\n",
    "latex2sympy2_extended==1.0.6\n",
    "Markdown==3.7\n",
    "math_verify==0.5.2\n",
    "mlflow_skinny==2.15.0\n",
    "numpy~=1.23.5\n",
    "openai==1.64.0\n",
    "packaging==24.2\n",
    "pandas==2.2.3\n",
    "peft==0.14.0\n",
    "python-dotenv==1.0.1\n",
    "safetensors==0.5.2\n",
    "torch==2.5.1\n",
    "tqdm==4.66.4\n",
    "transformers==4.48.2\n",
    "trl==0.15.1\n",
    "unsloth==2025.2.15\n",
    "unsloth_zoo==2025.2.7\n",
    "wandb==0.19.7\n",
    "azureml-sdk==1.58.0\n",
    "azureml-core==1.58.0\n",
    "azureml-dataset-runtime==1.58.0\n",
    "azureml-defaults==1.58.0\n",
    "azureml-contrib-services==1.58.0\n",
    "azureml-inference-server-http~=1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7b19bc6-dc43-4948-8c71-97a033a5f5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 23:27:48,416 - logger - INFO - Exception: Found Environment asset, but will update the Environment.\n",
      "\u001b[32mUploading serve (0.0 MBs): 100%|██████████| 1470/1470 [00:01<00:00, 1430.64it/s]\n",
      "\u001b[39m\n",
      "\n",
      "2025-02-26 23:33:08,390 - logger - INFO - Created Environment asset: llm-srv-2024-11-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build:\n",
      "  dockerfile_path: Dockerfile\n",
      "  path: https://stpgwgybluulpec.blob.core.windows.net/azureml-blobstore-f7c27ee9-fb96-407c-9b8f-a5c76209316e/LocalUpload/30aa0c7a6287fe6fbc127148904fda5a/serve/\n",
      "creation_context:\n",
      "  created_at: '2025-02-26T15:33:02.406134+00:00'\n",
      "  created_by: Gang Luo\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2025-02-26T15:33:02.406134+00:00'\n",
      "  last_modified_by: Gang Luo\n",
      "  last_modified_by_type: User\n",
      "description: Environment created from a Docker context.\n",
      "id: azureml:/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/environments/llm-srv-2024-11-05/versions/2\n",
      "name: llm-srv-2024-11-05\n",
      "os_type: linux\n",
      "tags: {}\n",
      "version: '2'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "\n",
    "def get_or_create_docker_environment_asset(ml_client, env_name, docker_dir, update=False):\n",
    "    \n",
    "    try:\n",
    "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created Environment asset: {env_name}\")\n",
    "    \n",
    "    return env_asset\n",
    "\n",
    "env = get_or_create_docker_environment_asset(ml_client, azure_env_name, f\"{CLOUD_DIR}/serve\", update=True)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496e364-c8ed-43c1-a248-2530c1eb44a4",
   "metadata": {},
   "source": [
    "### 2.4. Serving script\n",
    "\n",
    "If you are not serving with MLflow but with a custom model, you are free to write your own code.The `score.py` example below shows how to write the code.\n",
    "\n",
    "-   `init()`: This function is the place to write logic for global initialization operations like loading the LLM model.\n",
    "-   `run()`: Inference logic is called for every invocation of the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d2a7975-8473-4f48-9d15-5180e3276b11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src_serve/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src_serve/score.py\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
    "    You can write the logic here to perform init operations like caching the model in memory\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global tokenizer\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # Please provide your model's folder name if there is one\n",
    "    model_path = os.path.join(\n",
    "        os.getenv(\"AZUREML_MODEL_DIR\"), \"{{score_model_dir}}\"\n",
    "    )\n",
    "    model_id = \"{{hf_model_name_or_path}}\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_id, # Load up `Phi-4 14B`, and set parameters\n",
    "        # max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True, # False for LoRA 16bit\n",
    "        fast_inference = True, # Enable vLLM fast inference\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) \n",
    "\n",
    "    model.load_adapter(model_path)\n",
    "    logging.info(\"Loaded model.\")\n",
    "    \n",
    "def run(json_data: str):\n",
    "    logging.info(\"Request received\")\n",
    "    data = json.loads(json_data)\n",
    "    input_data= data[\"input_data\"]\n",
    "    params = data['params']\n",
    "    \n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    output = pipe(input_data, **params)\n",
    "    generated_text = output[0]['generated_text']\n",
    "    logging.info(\"Output Response: \" + generated_text)\n",
    "    json_result = {\"result\": str(generated_text)}\n",
    "    \n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20399372-3db8-4ca9-86ba-5d3123e108cf",
   "metadata": {},
   "source": [
    "Plug in the appropriate variables in the model inference script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68daf5a2-4aaa-4649-bb58-eec6abc942bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m pipeline\u001b[37m\u001b[39;49;00m\n",
      "     6\t\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36munsloth\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m FastLanguageModel\u001b[37m\u001b[39;49;00m\n",
      "     7\t\u001b[37m\u001b[39;49;00m\n",
      "     8\t\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32minit\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "     9\t\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    10\t\u001b[33m    This function is called when the container is initialized/started, typically after create/update of the deployment.\u001b[39;49;00m\n",
      "    11\t\u001b[33m    You can write the logic here to perform init operations like caching the model in memory\u001b[39;49;00m\n",
      "    12\t\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    13\t    \u001b[34mglobal\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "    14\t    \u001b[34mglobal\u001b[39;49;00m tokenizer\u001b[37m\u001b[39;49;00m\n",
      "    15\t    \u001b[37m# AZUREML_MODEL_DIR is an environment variable created during deployment.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    16\t    \u001b[37m# It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    17\t    \u001b[37m# Please provide your model's folder name if there is one\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    18\t    model_path = os.path.join(\u001b[37m\u001b[39;49;00m\n",
      "    19\t        os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mAZUREML_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33m./outputs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    20\t    )\u001b[37m\u001b[39;49;00m\n",
      "    21\t    model_id = \u001b[33m\"\u001b[39;49;00m\u001b[33mmicrosoft/phi-4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    22\t    model, tokenizer = FastLanguageModel.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "    23\t        model_name = model_id, \u001b[37m# Load up `Phi-4 14B`, and set parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    24\t        \u001b[37m# max_seq_length = max_seq_length,\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    25\t        load_in_4bit = \u001b[34mTrue\u001b[39;49;00m, \u001b[37m# False for LoRA 16bit\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    26\t        fast_inference = \u001b[34mTrue\u001b[39;49;00m, \u001b[37m# Enable vLLM fast inference\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    27\t    )\u001b[37m\u001b[39;49;00m\n",
      "    28\t    FastLanguageModel.for_inference(model) \u001b[37m\u001b[39;49;00m\n",
      "    29\t\u001b[37m\u001b[39;49;00m\n",
      "    30\t    model.load_adapter(model_path)\u001b[37m\u001b[39;49;00m\n",
      "    31\t    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    32\t    \u001b[37m\u001b[39;49;00m\n",
      "    33\t\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mrun\u001b[39;49;00m(json_data: \u001b[36mstr\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "    34\t    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mRequest received\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    35\t    data = json.loads(json_data)\u001b[37m\u001b[39;49;00m\n",
      "    36\t    input_data= data[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    37\t    params = data[\u001b[33m'\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    38\t    \u001b[37m\u001b[39;49;00m\n",
      "    39\t    pipe = pipeline(\u001b[33m\"\u001b[39;49;00m\u001b[33mtext-generation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, model=model, tokenizer=tokenizer)\u001b[37m\u001b[39;49;00m\n",
      "    40\t    output = pipe(input_data, **params)\u001b[37m\u001b[39;49;00m\n",
      "    41\t    generated_text = output[\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mgenerated_text\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    42\t    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mOutput Response: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + generated_text)\u001b[37m\u001b[39;49;00m\n",
      "    43\t    json_result = {\u001b[33m\"\u001b[39;49;00m\u001b[33mresult\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mstr\u001b[39;49;00m(generated_text)}\u001b[37m\u001b[39;49;00m\n",
      "    44\t    \u001b[37m\u001b[39;49;00m\n",
      "    45\t    \u001b[34mreturn\u001b[39;49;00m json_result\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "import jinja2\n",
    "from pathlib import Path\n",
    "TRAINED_MLFLOW = False\n",
    "\n",
    "jinja_env = jinja2.Environment()  \n",
    "\n",
    "template = jinja_env.from_string(Path(\"src_serve/score.py\").open().read())\n",
    "score_model_dir = model_dir #os.path.join(model_dir, \"peft\") if TRAINED_MLFLOW else model_dir    \n",
    "\n",
    "Path(\"src_serve/score.py\").open(\"w\").write(\n",
    "    template.render(score_model_dir=score_model_dir, hf_model_name_or_path=HF_MODEL_NAME_OR_PATH)\n",
    ")\n",
    "\n",
    "!pygmentize src_serve/score.py | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb187e-370f-4481-9d82-a38ae982c1e3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serving\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create endpoint\n",
    "\n",
    "Create an endpoint. This process does not provision a GPU cluster yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77c22433-4ab8-4db9-956b-7f437b86dfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 23:33:09,115 - logger - INFO - ===== 3. Serving =====\n",
      "2025-02-26 23:33:11,439 - logger - INFO - ---Endpoint already exists---\n",
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "2025-02-26 23:34:52,032 - logger - INFO - \n",
      "---Endpoint created successfully---\n",
      "\n",
      "2025-02-26 23:34:52,034 - logger - INFO - Creating Endpoint took 1 minute and 42.92 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_mode: key\n",
      "description: Test endpoint for phi4-grpo-2024-11-05\n",
      "id: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/onlineEndpoints/phi4-endpoint-2024-11-05\n",
      "identity:\n",
      "  principal_id: b2583326-5bcf-41c9-a0a9-161f06256881\n",
      "  tenant_id: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "  type: system_assigned\n",
      "kind: Managed\n",
      "location: eastus\n",
      "mirror_traffic: {}\n",
      "name: phi4-endpoint-2024-11-05\n",
      "openapi_uri: https://phi4-endpoint-2024-11-05.eastus.inference.ml.azure.com/swagger.json\n",
      "properties:\n",
      "  AzureAsyncOperationUri: https://management.azure.com/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oeidp:f7c27ee9-fb96-407c-9b8f-a5c76209316e:4a7848b9-ce91-4b3a-aebf-831f4ea38e45?api-version=2022-02-01-preview\n",
      "  azureml.onlineendpointid: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/providers/microsoft.machinelearningservices/workspaces/mlw-pgwgybluulpec/onlineendpoints/phi4-endpoint-2024-11-05\n",
      "  createdAt: 2025-02-26T14:33:10.626571+0000\n",
      "  createdBy: Gang Luo\n",
      "  lastModifiedAt: 2025-02-26T15:26:56.050875+0000\n",
      "provisioning_state: Succeeded\n",
      "public_network_access: enabled\n",
      "scoring_uri: https://phi4-endpoint-2024-11-05.eastus.inference.ml.azure.com/score\n",
      "tags: {}\n",
      "traffic: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    IdentityConfiguration,\n",
    "    ManagedIdentityConfiguration,\n",
    ")\n",
    "\n",
    "logger.info(f\"===== 3. Serving =====\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Check if the endpoint already exists in the workspace\n",
    "try:\n",
    "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
    "    logger.info(\"---Endpoint already exists---\")\n",
    "except:\n",
    "    # Create an online endpoint if it doesn't exist\n",
    "\n",
    "    # Define the endpoint\n",
    "    endpoint = ManagedOnlineEndpoint(\n",
    "        name=azure_endpoint_name,\n",
    "        description=f\"Test endpoint for {model.name}\",\n",
    "        # identity=IdentityConfiguration(\n",
    "        #     type=\"user_assigned\",\n",
    "        #     user_assigned_identities=[ManagedIdentityConfiguration(resource_id=uai_id)],\n",
    "        # )\n",
    "        # if uai_id != \"\"\n",
    "        # else None,\n",
    "    )\n",
    "\n",
    "# Trigger the endpoint creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(endpoint).wait() #AIF/ai prj/Models+Endpoints但只有deployment和endpoint而无权重; AML/wrkspc/Endpoints\n",
    "    logger.info(\"\\n---Endpoint created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Endpoint creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err\n",
    "    \n",
    "t1 = time.time()\n",
    "\n",
    "from humanfriendly import format_timespan\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating Endpoint took {timespan}\")    \n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a05af-0aca-4d36-9c0f-bfa4dcc6203b",
   "metadata": {},
   "source": [
    "### 3.2. Create Deployment\n",
    "\n",
    "Create a Deployment. This takes a lot of time as GPU clusters must be provisioned and the serving environment must be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3afaa8b-5af1-49d1-990f-414da0effe8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint phi4-endpoint-2024-11-05 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 08:41:48,955 - logger - INFO - \n",
      "---Deployment created successfully---\n",
      "\n",
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "2025-02-27 08:41:56,996 - logger - INFO - Creating deployment took 8 minutes and 40.31 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--endpoint--: \n",
      "auth_mode: key\n",
      "description: Test endpoint for phi4-grpo-2024-11-05\n",
      "id: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/onlineEndpoints/phi4-endpoint-2024-11-05\n",
      "identity:\n",
      "  principal_id: b2583326-5bcf-41c9-a0a9-161f06256881\n",
      "  tenant_id: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "  type: system_assigned\n",
      "kind: Managed\n",
      "location: eastus\n",
      "mirror_traffic: {}\n",
      "name: phi4-endpoint-2024-11-05\n",
      "openapi_uri: https://phi4-endpoint-2024-11-05.eastus.inference.ml.azure.com/swagger.json\n",
      "properties:\n",
      "  AzureAsyncOperationUri: https://management.azure.com/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oeidp:f7c27ee9-fb96-407c-9b8f-a5c76209316e:4a7848b9-ce91-4b3a-aebf-831f4ea38e45?api-version=2022-02-01-preview\n",
      "  azureml.onlineendpointid: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/providers/microsoft.machinelearningservices/workspaces/mlw-pgwgybluulpec/onlineendpoints/phi4-endpoint-2024-11-05\n",
      "  createdAt: 2025-02-26T14:33:10.626571+0000\n",
      "  createdBy: Gang Luo\n",
      "  lastModifiedAt: 2025-02-26T15:26:56.050875+0000\n",
      "provisioning_state: Succeeded\n",
      "public_network_access: enabled\n",
      "scoring_uri: https://phi4-endpoint-2024-11-05.eastus.inference.ml.azure.com/score\n",
      "tags: {}\n",
      "traffic:\n",
      "  phi4-blue: 100\n",
      "\n",
      "--deployment--: \n",
      "app_insights_enabled: false\n",
      "code_configuration:\n",
      "  code: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/codes/0937fb39-c350-4068-a3a3-3cfbad72b779/versions/1\n",
      "  scoring_script: score.py\n",
      "endpoint_name: phi4-endpoint-2024-11-05\n",
      "environment: azureml:/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/environments/llm-srv-2024-11-05/versions/2\n",
      "environment_variables: {}\n",
      "instance_count: 1\n",
      "instance_type: Standard_NC40ads_H100_v5\n",
      "liveness_probe:\n",
      "  failure_threshold: 5\n",
      "  initial_delay: 500\n",
      "  period: 90\n",
      "  success_threshold: 1\n",
      "  timeout: 10\n",
      "model: azureml:/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/models/phi4-grpo-2024-11-05/versions/1\n",
      "name: phi4-blue\n",
      "properties: {}\n",
      "readiness_probe:\n",
      "  failure_threshold: 3\n",
      "  initial_delay: 30\n",
      "  period: 30\n",
      "  success_threshold: 1\n",
      "  timeout: 10\n",
      "request_settings:\n",
      "  max_concurrent_requests_per_instance: 3\n",
      "  max_queue_wait_ms: 60000\n",
      "  request_timeout_ms: 90000\n",
      "tags: {}\n",
      "type: managed\n",
      "\n",
      "--endpoint_poller--: \n",
      "<azure.core.polling._poller.LROPoller object at 0x7fb0efc3ad10>\n",
      "CPU times: user 546 ms, sys: 61.8 ms, total: 608 ms\n",
      "Wall time: 8min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from azure.ai.ml.entities import (    \n",
    "    OnlineRequestSettings,\n",
    "    CodeConfiguration,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    Environment\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=azure_deployment_name,\n",
    "    endpoint_name=azure_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=azure_serving_cluster_size, # 这里只用了cluster size而没用cluster name，所以它占用的是servless instance\n",
    "    instance_count=1,\n",
    "    #code_configuration=code_configuration,\n",
    "    environment=env,\n",
    "    scoring_script=\"score.py\",\n",
    "    code_path=\"./src_serve\",\n",
    "    #environment_variables=deployment_env_vars,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=3,\n",
    "        request_timeout_ms=90000, \n",
    "        max_queue_wait_ms=60000\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=5,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=90,\n",
    "        initial_delay=500,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=3,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=30,\n",
    "        initial_delay=30,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Trigger the deployment creation\n",
    "try:\n",
    "    ml_client.begin_create_or_update(deployment).wait()# 在AIF/ai prj/models+endpoints中Model Depolyment标签页; AML/wrkspce/Enpoints选择一个endpoint的Detail标签页右侧就有该ep所关联的所有deployment的信息，在页面创建deployment也是这里\n",
    "    logger.info(\"\\n---Deployment created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err\n",
    "    \n",
    "endpoint.traffic = {azure_deployment_name: 100}\n",
    "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
    "\n",
    "t1 = time.time()\n",
    "timespan = format_timespan(t1 - t0)\n",
    "logger.info(f\"Creating deployment took {timespan}\")\n",
    "\n",
    "print(f\"--endpoint--: \\n{endpoint}\")\n",
    "print(f\"--deployment--: \\n{deployment}\")\n",
    "print(f\"--endpoint_poller--: \\n{endpoint_poller}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bc7b788-01f1-47d8-8142-5fdfb0014063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_mode: key\n",
      "description: Test endpoint for phi4-grpo-2024-11-05\n",
      "id: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/onlineEndpoints/phi4-endpoint-2024-11-05\n",
      "identity:\n",
      "  principal_id: b2583326-5bcf-41c9-a0a9-161f06256881\n",
      "  tenant_id: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "  type: system_assigned\n",
      "kind: Managed\n",
      "location: eastus\n",
      "mirror_traffic: {}\n",
      "name: phi4-endpoint-2024-11-05\n",
      "openapi_uri: https://phi4-endpoint-2024-11-05.eastus.inference.ml.azure.com/swagger.json\n",
      "properties:\n",
      "  AzureAsyncOperationUri: https://management.azure.com/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oeidp:f7c27ee9-fb96-407c-9b8f-a5c76209316e:86cc5685-6117-4adf-ab9a-64bf0786f47d?api-version=2022-02-01-preview\n",
      "  azureml.onlineendpointid: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/providers/microsoft.machinelearningservices/workspaces/mlw-pgwgybluulpec/onlineendpoints/phi4-endpoint-2024-11-05\n",
      "  createdAt: 2025-02-26T14:33:10.626571+0000\n",
      "  createdBy: Gang Luo\n",
      "  lastModifiedAt: 2025-02-27T00:41:55.437581+0000\n",
      "provisioning_state: Succeeded\n",
      "public_network_access: enabled\n",
      "scoring_uri: https://phi4-endpoint-2024-11-05.eastus.inference.ml.azure.com/score\n",
      "tags: {}\n",
      "traffic:\n",
      "  phi4-blue: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_poller.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f437f6-153a-42d5-ab22-0011d0fe2481",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Test\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Invocation\n",
    "\n",
    "Try calling the endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adf37c63-4d1e-44a3-8c94-268ffc716da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 08:43:58,804 - logger - INFO - Test script directory: ./phi3-inference-test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "sample = {\n",
    "    \"input_data\": \n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Tell me Microsoft's brief history.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell a BASIC interpreter for the Altair 8800.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What about Azure's history?\"}\n",
    "        ],\n",
    "    \"params\": {\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": True,\n",
    "        \"return_full_text\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "test_src_dir = \"./phi3-inference-test\"\n",
    "os.makedirs(test_src_dir, exist_ok=True)\n",
    "logger.info(f\"Test script directory: {test_src_dir}\")\n",
    "sample_data_path = os.path.join(test_src_dir, \"sample-request.json\")\n",
    "\n",
    "with open(sample_data_path, \"w\") as f:\n",
    "    json.dump(sample, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53b3079d-a412-497d-aa37-795a1683ac78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Azure, Microsoft's cloud computing platform, has a rich history that reflects the evolution of cloud services. Here's a brief overview:\n",
      "\n",
      "1. **Early Development (2008-2009):** \n",
      "   - Microsoft began developing Azure in 2008, initially under the code name \"Project Red Dog.\" The goal was to create a cloud platform that could compete with Amazon Web Services (AWS), which had launched in 2006.\n",
      "   - In 2009, Microsoft announced the Windows Azure platform, which was designed to provide a range of cloud services, including infrastructure as a service (IaaS) and platform as a service (\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "{'result': 'Microsoft Azure, Microsoft\\'s cloud computing platform, has a rich history that reflects the evolution of cloud services. Here\\'s a brief overview:\\n\\n1. **Early Development (2008-2009):** \\n   - Microsoft began developing Azure in 2008, initially under the code name \"Project Red Dog.\" The goal was to create a cloud platform that could compete with Amazon Web Services (AWS), which had launched in 2006.\\n   - In 2009, Microsoft announced the Windows Azure platform, which was designed to provide a range of cloud services, including infrastructure as a service (IaaS) and platform as a service ('}\n"
     ]
    }
   ],
   "source": [
    "result = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=azure_endpoint_name,\n",
    "    deployment_name=azure_deployment_name,\n",
    "    request_file=sample_data_path,\n",
    ")\n",
    "\n",
    "result_json = json.loads(result)\n",
    "print(result_json['result'])\n",
    "print(\"~~~~~~~~~~~~~\" * 20)\n",
    "print(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d96bd-75da-4c10-923c-edad899fc4d3",
   "metadata": {},
   "source": [
    "### 4.2. LLM latency/throughput benchmarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20bf4fff-915b-45a8-9f46-bff0e80373df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_latency(endpoint_name, deployment_name, sample_data_path, num_warmups=1, num_infers=5):\n",
    "    print(f\"Measuring latency for Endpoint '{endpoint_name}' and Deployment '{deployment_name}', num_infers={num_infers}\")\n",
    "\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(num_warmups):\n",
    "        result = ml_client.online_endpoints.invoke(\n",
    "            endpoint_name=endpoint_name,\n",
    "            deployment_name=deployment_name,\n",
    "            request_file=sample_data_path,\n",
    "        ) \n",
    "        \n",
    "    begin = time.time()        \n",
    "    # Timed run\n",
    "    for _ in range(num_infers):\n",
    "        start_time = perf_counter()\n",
    "        result = ml_client.online_endpoints.invoke(\n",
    "            endpoint_name=endpoint_name,\n",
    "            deployment_name=deployment_name,\n",
    "            request_file=sample_data_path,\n",
    "        )\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    end = time.time() \n",
    "        \n",
    "    # Compute run statistics\n",
    "    duration = end - begin    \n",
    "    time_avg_sec = np.mean(latencies)\n",
    "    time_std_sec = np.std(latencies)\n",
    "    time_p95_sec = np.percentile(latencies, 95)\n",
    "    time_p99_sec = np.percentile(latencies, 99)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'duration': duration,\n",
    "        'avg_sec': time_avg_sec,\n",
    "        'std_sec': time_std_sec,        \n",
    "        'p95_sec': time_p95_sec,\n",
    "        'p99_sec': time_p99_sec    \n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def benchmark_latency_multicore(endpoint_name, deployment_name, sample_data_path, num_warmups=1, num_infers=5, num_threads=2):\n",
    "    import time\n",
    "    import concurrent.futures\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(num_warmups):\n",
    "        result = ml_client.online_endpoints.invoke(\n",
    "            endpoint_name=endpoint_name,\n",
    "            deployment_name=deployment_name,\n",
    "            request_file=sample_data_path,\n",
    "        )        \n",
    "                \n",
    "    latencies = []\n",
    "\n",
    "    # Thread task: Each of these thread tasks executes in a serial loop for a single model.\n",
    "    #              Multiple of these threads are launched to achieve parallelism.\n",
    "    def task(model):\n",
    "        for _ in range(num_infers):\n",
    "            start = time.time()\n",
    "            result = ml_client.online_endpoints.invoke(\n",
    "                endpoint_name=endpoint_name,\n",
    "                deployment_name=deployment_name,\n",
    "                request_file=sample_data_path,\n",
    "            )   \n",
    "            finish = time.time()\n",
    "            latencies.append(finish - start)\n",
    "            \n",
    "    # Submit tasks\n",
    "    begin = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as pool:\n",
    "        for i in range(num_threads):\n",
    "            pool.submit(task, model)\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute metrics\n",
    "    duration = end - begin\n",
    "    inferences = len(latencies)\n",
    "    throughput = inferences / duration\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    \n",
    "    # Compute run statistics\n",
    "    time_avg_sec = np.mean(latencies)\n",
    "    time_std_sec = np.std(latencies)\n",
    "    time_p95_sec = np.percentile(latencies, 95)\n",
    "    time_p99_sec = np.percentile(latencies, 99)\n",
    "    \n",
    "    time_std_sec = np.std(latencies)\n",
    "    time_p95_sec = np.percentile(latencies, 95)\n",
    "    time_p99_sec = np.percentile(latencies, 99)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'threads': num_threads,\n",
    "        'duration': duration,\n",
    "        'throughput': throughput,\n",
    "        'avg_sec': avg_latency,\n",
    "        'std_sec': time_std_sec,        \n",
    "        'p95_sec': time_p95_sec,\n",
    "        'p99_sec': time_p99_sec    \n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8827fb45-c7ab-4aa8-b9f4-1c4e610d695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring latency for Endpoint 'phi4-endpoint-2024-11-05' and Deployment 'phi4-blue', num_infers=5\n"
     ]
    }
   ],
   "source": [
    "benchmark_result = benchmark_latency(azure_endpoint_name, azure_deployment_name, sample_data_path, num_warmups=1, num_infers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c62e9289-fe65-4cbf-abc8-f19a4f486bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration': 36.699119329452515, 'avg_sec': 7.560433123994153, 'std_sec': 0.3585406471364, 'p95_sec': 7.946338906389428, 'p99_sec': 7.946960451669293}\n"
     ]
    }
   ],
   "source": [
    "print(benchmark_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834a237-e751-446a-ac21-7272c29b0c2c",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59508aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {test_src_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72663d-d773-435b-871f-cc51b1e51763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ml_client.online_endpoints.begin_delete(azure_endpoint_name) #不del，后续promptflow要用到"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
