{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Florence2 for VQA (Visual Question Answering) using the Azure ML Python SDK and MLflow\n",
    "\n",
    "### Overview\n",
    "\n",
    "Azure ML Workspace is compatible with MLflow and can be used as an MLflow Tracking Server, as described in the following official guide from Microsoft. MLflow provides features such as experiment tracking, model management, and model deployment, allowing you to manage data science and machine learning workflows more efficiently and systematically. Below are the main advantages of using Azure ML and MLflow together.\n",
    "\n",
    "#### 1. Experiment tracking and management\n",
    "\n",
    "You can systematically manage the parameters, metrics, and artifacts of all your experiments. Integrating with Azur eML allows you to easily track and manage this information within your Azure ML workspace.\n",
    "\n",
    "#### 2. Model management\n",
    "\n",
    "MLflow provides a model registry for model versioning. Integrate with AzureML to systematically manage and deploy all versions of your models. When combined with AzureML's deployment capabilities, models can be easily deployed to a variety of environments (e.g. Azure Kubernetes Service, Azure Container Instances).\n",
    "\n",
    "#### 3. Reproducibility and collaboration\n",
    "\n",
    "MLflow records the parameters and environment of every experiment, so you can accurately reproduce the experiment. This is very useful when you need to redo the same experiment across collaborating team members, or when you need to rerun an experiment at a later date.\n",
    "\n",
    "#### 4. CI/CD integration\n",
    "\n",
    "MLflow makes it easy to implement continuous integration (CI) and continuous deployment (CD) of machine learning models. Integrate with Azure DevOps or GitHub Actions to automatically run training, validation, and deployment processes as model changes occur.\n",
    "\n",
    "When training a model with Hugging Face's Trainer API, if you specify `report_to=\"azure_ml\"`, basic indicators will be automatically logged without any additional code. Of course, you can freely log custom indicators using Bring Your Own Script like the conventional method, but Azure ML's basic logging function is also excellent, so try using it as a baseline.\n",
    "\n",
    "[Note] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: python31014jvsc74a57bd01f90a0206bde5cf3732dab79adbbcc7570d5fab64b89fc69d46a8fe33664a709\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "lab_prep_dir = os.getcwd().split(\"SLMWorkshopCN\")[0] + \"SLMWorkshopCN/0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(lab_prep_dir))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 12:00:43,084 - logger - INFO - ===== 0. Azure ML Training Info =====\n",
      "2025-03-12 12:00:43,086 - logger - INFO - AZURE_SUBSCRIPTION_ID=49aee8bf-3f02-464f-a0ba-e3467e7d85e2\n",
      "2025-03-12 12:00:43,087 - logger - INFO - AZURE_RESOURCE_GROUP=rg-slmwrkshp_9\n",
      "2025-03-12 12:00:43,087 - logger - INFO - AZURE_WORKSPACE=mlw-pgwgybluulpec\n",
      "2025-03-12 12:00:43,088 - logger - INFO - AZURE_DATA_NAME=lgds-sftflorence241201\n",
      "2025-03-12 12:00:43,088 - logger - INFO - DATA_DIR=./dataset\n",
      "2025-03-12 12:00:43,089 - logger - INFO - CLOUD_DIR=./cloud\n",
      "2025-03-12 12:00:43,090 - logger - INFO - HF_MODEL_NAME_OR_PATH=microsoft/Florence-2-base-ft\n",
      "2025-03-12 12:00:43,090 - logger - INFO - IS_DEBUG=True\n",
      "2025-03-12 12:00:43,091 - logger - INFO - USE_LOWPRIORITY_VM=False\n",
      "2025-03-12 12:00:43,091 - logger - INFO - azure_env_name=llm-finetuning-florence-2024-11-05\n",
      "2025-03-12 12:00:43,092 - logger - INFO - azure_compute_cluster_name=gpu-h100\n",
      "2025-03-12 12:00:43,092 - logger - INFO - azure_compute_cluster_size=Standard_NC40ads_H100_v5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()#相关机密配置写在.env中，并.gitignore掉.env\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(\"config_prd.yml\") as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "AZURE_SUBSCRIPTION_ID = d[\"config\"][\"AZURE_SUBSCRIPTION_ID\"]\n",
    "AZURE_RESOURCE_GROUP = d[\"config\"][\"AZURE_RESOURCE_GROUP\"]\n",
    "AZURE_WORKSPACE = d[\"config\"][\"AZURE_WORKSPACE\"]\n",
    "AZURE_DATA_NAME = d[\"config\"][\"AZURE_DATA_NAME\"]\n",
    "DATA_DIR = d[\"config\"][\"DATA_DIR\"]\n",
    "CLOUD_DIR = d[\"config\"][\"CLOUD_DIR\"]\n",
    "HF_MODEL_NAME_OR_PATH = d[\"config\"][\"HF_MODEL_NAME_OR_PATH\"]\n",
    "IS_DEBUG = d[\"config\"][\"IS_DEBUG\"]\n",
    "USE_LOWPRIORITY_VM = d[\"config\"][\"USE_LOWPRIORITY_VM\"]\n",
    "\n",
    "azure_env_name = d[\"train\"][\"azure_env_name\"]\n",
    "azure_compute_cluster_name = d[\"train\"][\"azure_compute_cluster_name\"]\n",
    "azure_compute_cluster_size = d[\"train\"][\"azure_compute_cluster_size\"]\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CLOUD_DIR, exist_ok=True)\n",
    "\n",
    "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
    "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
    "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
    "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
    "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
    "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
    "\n",
    "logger.info(f\"azure_env_name={azure_env_name}\")\n",
    "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
    "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Dataset preparation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store datasets from the HuggingFace hub on shared storage because storing them in the root can run out of space.\n",
    "\n",
    "已经把数据保持在./dataset中，从github取下来即可，可以不跑这一节。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from pathlib import Path\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# curr_dir = Path.cwd()\n",
    "# model_cache_dir = os.path.join(curr_dir.parent.parent, \"model\")\n",
    "# dataset_cache_dir = os.path.join(curr_dir.parent.parent, \"dataset\")\n",
    "\n",
    "# dataset = load_dataset(\"HuggingFaceM4/DocumentVQA\", cache_dir=dataset_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if IS_DEBUG:\n",
    "    # dataset[\"train\"] = dataset[\"train\"].select(range(1000))\n",
    "    # dataset[\"validation\"] = dataset[\"validation\"].select(range(200))\n",
    "    # dataset[\"test\"] = dataset[\"test\"].select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luogang/anaconda3/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 16/16 [00:00<00:00, 579.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8/8 [00:00<00:00, 279.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8/8 [00:00<00:00, 309.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# from local disk\n",
    "#\n",
    "# import torch\n",
    "# from pathlib import Path\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# curr_dir = Path.cwd()\n",
    "# dataset_dir = os.path.join(curr_dir, \"dataset_tmp\")\n",
    "# dataset = load_dataset(dataset_dir)\n",
    "\n",
    "# dataset[\"train\"] = dataset[\"train\"].select(range(16))\n",
    "# dataset[\"validation\"] = dataset[\"validation\"].select(range(8))\n",
    "# dataset[\"test\"] = dataset[\"test\"].select(range(8))\n",
    "# dataset.save_to_disk(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f580ace89d0>,\n",
      "         subscription_id=49aee8bf-3f02-464f-a0ba-e3467e7d85e2,\n",
      "         resource_group_name=rg-slmwrkshp_9,\n",
      "         workspace_name=mlw-pgwgybluulpec)\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(\n",
    "    credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE\n",
    ")\n",
    "\n",
    "# The code below may conflict with AI Foundry as of February 2025.\n",
    "# ml_client = None\n",
    "# try:\n",
    "#     ml_client = MLClient.from_config(credential)\n",
    "# except Exception as ex:\n",
    "#     print(ex)\n",
    "#     ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Create AzureML environment and data\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda).\n",
    "This hands-on uses conda yaml.\n",
    "\n",
    "Training data can be used as a dataset stored in the local development environment, but can also be registered as AzureML data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "\n",
    "\n",
    "def get_or_create_environment_asset(\n",
    "    ml_client, env_name, conda_yml=\"cloud/conda.yml\", update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
    "            conda_file=conda_yml,\n",
    "            name=env_name,\n",
    "            description=\"Environment created for llm fine-tuning.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        print(f\"Created Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "def get_or_create_docker_environment_asset(\n",
    "    ml_client, env_name, docker_dir, update=False\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_env_version = max(\n",
    "            [int(e.version) for e in ml_client.environments.list(name=env_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\n",
    "                \"Found Environment asset, but will update the Environment.\"\n",
    "            )\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(\n",
    "                name=env_name, version=latest_env_version\n",
    "            )\n",
    "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        print(f\"Created Environment asset: {env_name}\")\n",
    "\n",
    "    return env_asset\n",
    "\n",
    "\n",
    "def get_or_create_data_asset(ml_client, data_name, data_local_dir, update=False):\n",
    "\n",
    "    try:\n",
    "        latest_data_version = max(\n",
    "            [int(d.version) for d in ml_client.data.list(name=data_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Data asset, but will update the Data.\")\n",
    "        else:\n",
    "            data_asset = ml_client.data.get(name=data_name, version=latest_data_version)\n",
    "            print(f\"Found Data asset: {data_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        data = Data(\n",
    "            path=data_local_dir,\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=f\"{data_name} for fine tuning\",\n",
    "            tags={\"FineTuningType\": \"Instruction\", \"Language\": \"En\"},\n",
    "            name=data_name,\n",
    "        )\n",
    "        data_asset = ml_client.data.create_or_update(data)\n",
    "        print(f\"Created Data asset: {data_name}\")\n",
    "\n",
    "    return data_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Environment asset: llm-finetuning-florence-2024-11-05. Will not create again\n",
      "Found Data asset: lgds-sftflorence241201. Will not create again\n"
     ]
    }
   ],
   "source": [
    "env = get_or_create_docker_environment_asset(\n",
    "    ml_client, azure_env_name, docker_dir=f\"{CLOUD_DIR}/train\", update=False\n",
    ")\n",
    "data = get_or_create_data_asset(\n",
    "    ml_client, AZURE_DATA_NAME, data_local_dir=DATA_DIR, update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Training script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mshutil\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mmlflow\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mrandom\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtqdm\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m tqdm\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mdatasets\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m load_dataset, load_from_disk\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mdatetime\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m datetime\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mfunctools\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m partial\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m Dataset, DataLoader\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m AutoProcessor, get_scheduler\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m AutoModelForCausalLM, AutoProcessor\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mPIL\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m Image, ImageDraw, ImageFont\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[36mtextwrap\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Define the custom DocVQADataset class\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[04m\u001b[32mDocVQADataset\u001b[39;49;00m(Dataset):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.data = data\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.data)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\u001b[37m\u001b[39;49;00m\n",
      "        example = \u001b[36mself\u001b[39;49;00m.data[idx]\u001b[37m\u001b[39;49;00m\n",
      "        question = \u001b[33m\"\u001b[39;49;00m\u001b[33m<DocVQA>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + example[\u001b[33m'\u001b[39;49;00m\u001b[33mquestion\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        first_answer = example[\u001b[33m'\u001b[39;49;00m\u001b[33manswers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        image = example[\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m image.mode != \u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            image = image.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m question, first_answer, image\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Overray image with text\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mcreate_image_with_text\u001b[39;49;00m(image, text, font_size=\u001b[34m20\u001b[39;49;00m, font_path=\u001b[34mNone\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "    width, height = image.size\u001b[37m\u001b[39;49;00m\n",
      "    width = (\u001b[36mint\u001b[39;49;00m)(width * \u001b[34m0.5\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    height = (\u001b[36mint\u001b[39;49;00m)(height * \u001b[34m0.5\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    image = image.resize((width, height))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set the font size and path\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m font_path:\u001b[37m\u001b[39;49;00m\n",
      "        font = ImageFont.truetype(font_path, font_size)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        font = ImageFont.load_default()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Wrap the text\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    wrapper = textwrap.TextWrapper(width=\u001b[34m40\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    text_lines = wrapper.wrap(text)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Create a new blank image with extra space for text\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    total_height = image.height + font_size * \u001b[36mlen\u001b[39;49;00m(text_lines) + \u001b[34m20\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    new_image = Image.new(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, (image.width, total_height), \u001b[33m\"\u001b[39;49;00m\u001b[33mwhite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Paste the original image on the new image\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    new_image.paste(image, (\u001b[34m0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Create a drawing context\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    draw = ImageDraw.Draw(new_image)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Draw the text below the image\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    text_y_position = image.height + \u001b[34m10\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m text_lines:\u001b[37m\u001b[39;49;00m\n",
      "        draw.text((\u001b[34m10\u001b[39;49;00m, text_y_position), line, font=font, fill=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        text_y_position += font_size + \u001b[34m2\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m new_image\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Function to run the model on an example\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mrun_example\u001b[39;49;00m(task_prompt, text_input, image):\u001b[37m\u001b[39;49;00m\n",
      "    prompt = task_prompt + text_input\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Ensure the image is in RGB mode\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m image.mode != \u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        image = image.convert(\u001b[33m\"\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    inputs = processor(text=prompt, images=image, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).to(device)\u001b[37m\u001b[39;49;00m\n",
      "    generated_ids = model.generate(\u001b[37m\u001b[39;49;00m\n",
      "        input_ids=inputs[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "        pixel_values=inputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "        max_new_tokens=\u001b[34m1024\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        num_beams=\u001b[34m3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=\u001b[34mFalse\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m parsed_answer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mload_model\u001b[39;49;00m(model_name_or_path=\u001b[33m\"\u001b[39;49;00m\u001b[33mmicrosoft/Florence-2-base-ft\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, freeze_vision_encoder=\u001b[34mTrue\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mglobal\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mglobal\u001b[39;49;00m processor\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    model_kwargs = \u001b[36mdict\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        trust_remote_code=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        revision=\u001b[33m\"\u001b[39;49;00m\u001b[33mrefs/pr/6\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,        \u001b[37m\u001b[39;49;00m\n",
      "        device_map=device\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    processor_kwargs = \u001b[36mdict\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        trust_remote_code=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        revision=\u001b[33m\"\u001b[39;49;00m\u001b[33mrefs/pr/6\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\u001b[37m\u001b[39;49;00m\n",
      "    processor = AutoProcessor.from_pretrained(model_name_or_path, **processor_kwargs)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m freeze_vision_encoder:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.vision_tower.parameters():\u001b[37m\u001b[39;49;00m\n",
      "            param.is_trainable = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mcollate_fn\u001b[39;49;00m(batch, processor):\u001b[37m\u001b[39;49;00m\n",
      "    questions, answers, images = \u001b[36mzip\u001b[39;49;00m(*batch)\u001b[37m\u001b[39;49;00m\n",
      "    inputs = processor(text=\u001b[36mlist\u001b[39;49;00m(questions), images=\u001b[36mlist\u001b[39;49;00m(images), return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, padding=\u001b[34mTrue\u001b[39;49;00m).to(device)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m inputs, answers\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mparse_args\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# setup argparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# curr_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# hyperparameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name_or_path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mmicrosoft/Florence-2-base-ft\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mModel name or path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m../dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mInput directory for training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m./model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moutput directory for model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m./output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mdirectory to temporarily store when training a model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mtraining - mini batch size for each gpu/process\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation - mini batch size for each gpu/process\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m1e-06\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--logging_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m5\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlogging steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m20\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33msave steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--grad_accum_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgradient accumulation steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr_scheduler_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mlinear\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mseed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwarmup ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parse args\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    args = parser.parse_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# return args\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m args\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mtrain_model\u001b[39;49;00m(args, train_dataset, val_dataset):\u001b[37m\u001b[39;49;00m\n",
      "    epochs = args.epochs\u001b[37m\u001b[39;49;00m\n",
      "    save_steps = args.save_steps\u001b[37m\u001b[39;49;00m\n",
      "    grad_accum_steps = args.grad_accum_steps\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    train_batch_size = args.train_batch_size\u001b[37m\u001b[39;49;00m\n",
      "    eval_batch_size = args.eval_batch_size\u001b[37m\u001b[39;49;00m\n",
      "    num_workers = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Create dataloader\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, collate_fn=partial(collate_fn, processor=processor), num_workers=num_workers, shuffle=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    val_loader = DataLoader(val_dataset, batch_size=eval_batch_size, collate_fn=partial(collate_fn, processor=processor), num_workers=num_workers)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[34m1e-6\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    num_training_steps = epochs * \u001b[36mlen\u001b[39;49;00m(train_loader)\u001b[37m\u001b[39;49;00m\n",
      "    num_warmup_steps = \u001b[36mint\u001b[39;49;00m(num_training_steps * args.warmup_ratio)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    lr_scheduler = get_scheduler(\u001b[37m\u001b[39;49;00m\n",
      "        name=args.lr_scheduler_type,\u001b[37m\u001b[39;49;00m\n",
      "        optimizer=optimizer,\u001b[37m\u001b[39;49;00m\n",
      "        num_warmup_steps=num_warmup_steps,\u001b[37m\u001b[39;49;00m\n",
      "        num_training_steps=num_training_steps,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    saved_models = []\u001b[37m\u001b[39;49;00m\n",
      "    model.train() \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m mlflow.start_run() \u001b[34mas\u001b[39;49;00m run: \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "        mlflow.log_params({\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: epochs,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33meval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.eval_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mseed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.seed,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mlr_scheduler_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.lr_scheduler_type,        \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgrad_accum_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: grad_accum_steps, \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_training_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: num_training_steps,\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: num_warmup_steps,\u001b[37m\u001b[39;49;00m\n",
      "        })\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(epochs):     \u001b[37m\u001b[39;49;00m\n",
      "            train_loss = \u001b[34m0.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            optimizer.zero_grad()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m step, (inputs, answers) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(tqdm(train_loader, desc=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining Epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[37m \u001b[39;49;00m+\u001b[37m \u001b[39;49;00m\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepochs\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                input_ids = inputs[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "                pixel_values = inputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[37m\u001b[39;49;00m\n",
      "                labels = processor.tokenizer(text=answers, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, padding=\u001b[34mTrue\u001b[39;49;00m, return_token_type_ids=\u001b[34mFalse\u001b[39;49;00m).input_ids.to(device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\u001b[37m\u001b[39;49;00m\n",
      "                loss = outputs.loss\u001b[37m\u001b[39;49;00m\n",
      "                loss.backward()\u001b[37m\u001b[39;49;00m\n",
      "                train_loss += loss.item()           \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[34mif\u001b[39;49;00m (step + \u001b[34m1\u001b[39;49;00m) % grad_accum_steps == \u001b[34m0\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                    train_loss /= grad_accum_steps \u001b[37m# compute gradient average  \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                    learning_rate = lr_scheduler.get_last_lr()[\u001b[34m0\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "                    progress = (step+\u001b[34m1\u001b[39;49;00m)/\u001b[36mlen\u001b[39;49;00m(train_loader)\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepochs\u001b[33m}\u001b[39;49;00m\u001b[33m], Step [\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mstep+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_loader)\u001b[33m}\u001b[39;49;00m\u001b[33m], Learning Rate: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlearning_rate\u001b[33m}\u001b[39;49;00m\u001b[33m, Loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_loss\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                    mlflow.log_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train_loss)\u001b[37m\u001b[39;49;00m\n",
      "                    mlflow.log_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, learning_rate)\u001b[37m\u001b[39;49;00m\n",
      "                    mlflow.log_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mprogress\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, progress)\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[37m\u001b[39;49;00m\n",
      "                    optimizer.step()\u001b[37m\u001b[39;49;00m\n",
      "                    optimizer.zero_grad()\u001b[37m\u001b[39;49;00m\n",
      "                    lr_scheduler.step()\u001b[37m\u001b[39;49;00m\n",
      "                    train_loss = \u001b[34m0.0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[34mif\u001b[39;49;00m (step + \u001b[34m1\u001b[39;49;00m) % save_steps == \u001b[34m0\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                    output_dir = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m./\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/steps_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mstep+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                    os.makedirs(output_dir, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                    model.save_pretrained(output_dir)\u001b[37m\u001b[39;49;00m\n",
      "                    processor.save_pretrained(output_dir)                \u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mModel saved at step \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mstep+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m of epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch+\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                    saved_models.append(output_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[37m# Log image\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                    idx = random.randrange(\u001b[36mlen\u001b[39;49;00m(val_dataset))\u001b[37m\u001b[39;49;00m\n",
      "                    val_img = val_dataset[idx][-\u001b[34m1\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "                    result = run_example(\u001b[33m\"\u001b[39;49;00m\u001b[33mDocVQA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mWhat do you see in this image?\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, val_dataset[idx][-\u001b[34m1\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "                    val_img_result = create_image_with_text(val_img, json.dumps(result))\u001b[37m\u001b[39;49;00m\n",
      "                    mlflow.log_image(val_img_result, key=\u001b[33m\"\u001b[39;49;00m\u001b[33mDocVQA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, step=step)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[37m# Manage to save only the most recent 3 checkpoints\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(saved_models) > \u001b[34m2\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                        old_model = saved_models.pop(\u001b[34m0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                        \u001b[34mif\u001b[39;49;00m os.path.exists(old_model):\u001b[37m\u001b[39;49;00m\n",
      "                            shutil.rmtree(old_model)\u001b[37m\u001b[39;49;00m\n",
      "                            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mRemoved old model: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mold_model\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# Validation phase\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            model.eval()\u001b[37m\u001b[39;49;00m\n",
      "            val_loss = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m torch.no_grad():\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[34mfor\u001b[39;49;00m (inputs, answers) \u001b[35min\u001b[39;49;00m tqdm(val_loader, desc=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mValidation Epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[37m \u001b[39;49;00m+\u001b[37m \u001b[39;49;00m\u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepochs\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                    input_ids = inputs[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "                    pixel_values = inputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mpixel_values\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "                    labels = processor.tokenizer(text=answers, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, padding=\u001b[34mTrue\u001b[39;49;00m, return_token_type_ids=\u001b[34mFalse\u001b[39;49;00m).input_ids.to(device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                    outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\u001b[37m\u001b[39;49;00m\n",
      "                    loss = outputs.loss\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                    val_loss += loss.item()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            avg_val_loss = val_loss / \u001b[36mlen\u001b[39;49;00m(val_loader)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "            mlflow.log_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mavg_val_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, avg_val_loss)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAverage Validation Loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mavg_val_loss\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Save model checkpoint\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model_dir = args.model_dir\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#os.makedirs(model_dir, exist_ok=True)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model.save_pretrained(model_dir)\u001b[37m\u001b[39;49;00m\n",
      "        processor.save_pretrained(model_dir)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        dependencies_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33mdependencies\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        shutil.copytree(dependencies_dir, model_dir, dirs_exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[32mmain\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    load_model(args.model_name_or_path)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load datasets\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    dataset = load_from_disk(args.train_dir)\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = DocVQADataset(dataset[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    val_dataset = DocVQADataset(dataset[\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Train model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_model(args, train_dataset, val_dataset)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#sys.argv = ['']\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    args = parse_args()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#args.model_name_or_path = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/poc-phi3/code/Users/model/Florence-2-base-ft\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\u001b[37m\u001b[39;49;00m\n",
      "    main(args)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src_train/train_mlflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Training\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create the compute cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compute cluster already exists! Reusing it for the current run\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "### Create the compute cluster\n",
    "try:\n",
    "    compute = ml_client.compute.get(azure_compute_cluster_name)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {azure_compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        tier = \"LowPriority\" if USE_LOWPRIORITY_VM else \"Dedicated\"\n",
    "        compute = AmlCompute(\n",
    "            name=azure_compute_cluster_name,\n",
    "            size=azure_compute_cluster_size,\n",
    "            tier=tier,\n",
    "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Start training job\n",
    "\n",
    "The `command` allows user to configure the following key aspects.\n",
    "\n",
    "-   `inputs` - This is the dictionary of inputs using name value pairs to the command.\n",
    "    -   `type` - The type of input. This can be a `uri_file` or `uri_folder`. The default is `uri_folder`.\n",
    "    -   `path` - The path to the file or folder. These can be local or remote files or folders. For remote files - http/https, wasb are supported.\n",
    "        -   Azure ML `data`/`dataset` or `datastore` are of type `uri_folder`. To use `data`/`dataset` as input, you can use registered dataset in the workspace using the format '<data_name>:<version>'. For e.g Input(type='uri_folder', path='my_dataset:1')\n",
    "    -   `mode` - Mode of how the data should be delivered to the compute target. Allowed values are `ro_mount`, `rw_mount` and `download`. Default is `ro_mount`\n",
    "-   `code` - This is the path where the code to run the command is located\n",
    "-   `compute` - The compute on which the command will run. You can run it on the local machine by using `local` for the compute.\n",
    "-   `command` - This is the command that needs to be run\n",
    "    in the `command` using the `${{inputs.<input_name>}}` expression. To use files or folders as inputs, we can use the `Input` class. The `Input` class supports three parameters:\n",
    "-   `environment` - This is the environment needed for the command to run. Curated (built-in) or custom environments from the workspace can be used.\n",
    "-   `instance_count` - Number of nodes. Default is 1.\n",
    "-   `distribution` - Distribution configuration for distributed training scenarios. Azure Machine Learning supports PyTorch, TensorFlow, and MPI-based distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "\u001b[32mUploading src_train (0.2 MBs): 100%|██████████| 202657/202657 [00:02<00:00, 81447.44it/s] \n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: magenta_bell_mbxzg1w48z\n",
      "Web View: https://ml.azure.com/runs/magenta_bell_mbxzg1w48z?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: magenta_bell_mbxzg1w48z\n",
      "Web View: https://ml.azure.com/runs/magenta_bell_mbxzg1w48z?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        # train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
    "        train_dir=Input(path=f\"{AZURE_DATA_NAME}@latest\"),  # Get data from Data asset\n",
    "        epoch=d[\"train\"][\"epoch\"],\n",
    "        train_batch_size=d[\"train\"][\"train_batch_size\"],\n",
    "        eval_batch_size=d[\"train\"][\"eval_batch_size\"],\n",
    "        model_dir=d[\"train\"][\"model_dir\"],\n",
    "    ),\n",
    "    code=\"./src_train\",  # local path where the code is stored\n",
    "    compute=azure_compute_cluster_name,\n",
    "    command=\"python train_mlflow.py --train_dir ${{inputs.train_dir}} --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}\",\n",
    "    # environment=\"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/77\", # Use built-in Environment asset\n",
    "    environment=f\"{azure_env_name}@latest\",\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        \"process_count_per_instance\": 1,  # For multi-gpu training set this to an integer value more than 1\n",
    "    },\n",
    ")\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>florence2-VQA</td><td>magenta_bell_mbxzg1w48z</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/magenta_bell_mbxzg1w48z?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec&amp;tid=16b3c013-d300-468d-ac64-7eda0820b6d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "Command({'parameters': {}, 'init': False, 'name': 'magenta_bell_mbxzg1w48z', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/butterluo/SLMWorkshopCN.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '3c7fbaf7cccbaa7bf383f5aad52b6b0052909ac8', 'azureml.git.dirty': 'False', '_azureml.ComputeTargetType': 'amlctrain', '_azureml.ClusterName': 'gpu-h100', 'ContentSnapshotId': '862912f2-883b-4ed8-bf84-e2f4916e92dc'}, 'print_as_yaml': False, 'id': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/jobs/magenta_bell_mbxzg1w48z', 'Resource__source_path': '', 'base_path': '/mnt/d/BT/SRC/NLP/LLM/SFT/SLMWorkshopCN/2_slm-fine-tuning-mlstudio/florence2-VQA', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f58006063e0>, 'serialize': <msrest.serialization.Serializer object at 0x7f58001b5ea0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <TraceLogger attr_dict (WARNING)>, 'display_name': 'magenta_bell_mbxzg1w48z', 'experiment_name': 'florence2-VQA', 'compute': 'gpu-h100', 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/magenta_bell_mbxzg1w48z?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'train_dir': {'type': 'uri_folder', 'path': 'lgds-sftflorence241201:1', 'mode': 'ro_mount'}, 'epoch': '1', 'train_batch_size': '8', 'eval_batch_size': '8', 'model_dir': './outputs'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.magenta_bell_mbxzg1w48z', 'mode': 'rw_mount'}}, 'inputs': {'train_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f58001b5cc0>, 'epoch': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f58001b6260>, 'train_batch_size': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f58001b5b40>, 'eval_batch_size': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f58001b6830>, 'model_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f58001b7ca0>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f58001b6560>}, 'component': CommandComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'magenta_bell_mbxzg1w48z', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/d/BT/SRC/NLP/LLM/SFT/SLMWorkshopCN/2_slm-fine-tuning-mlstudio/florence2-VQA', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f58006063e0>, 'serialize': <msrest.serialization.Serializer object at 0x7f58001b5f30>, 'command': 'python train_mlflow.py --train_dir ${{inputs.train_dir}} --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}', 'code': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/codes/00c0996b-ef42-41da-848b-67651f58e324/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/environments/llm-finetuning-florence-2024-11-05/versions/1', 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f58001b7b50>, 'resources': None, 'queue_settings': None, 'version': None, 'schema': None, 'type': 'command', 'display_name': 'magenta_bell_mbxzg1w48z', 'is_deterministic': True, 'inputs': {'train_dir': {'type': 'uri_folder', 'path': '/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec/data/lgds-sftflorence241201/versions/1', 'mode': 'ro_mount'}, 'epoch': {'type': 'string', 'default': '1'}, 'train_batch_size': {'type': 'string', 'default': '8'}, 'eval_batch_size': {'type': 'string', 'default': '8'}, 'model_dir': {'type': 'string', 'default': './outputs'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.magenta_bell_mbxzg1w48z', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': []}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/magenta_bell_mbxzg1w48z?wsid=/subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourcegroups/rg-slmwrkshp_9/workspaces/mlw-pgwgybluulpec&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f58006063e0>}, 'instance_id': '22359ae0-dea6-4582-86f7-551a2c103013', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': <azure.ai.ml.entities._job.distribution.PyTorchDistribution object at 0x7f58001b7b50>, 'environment_variables': {}, 'environment': 'llm-finetuning-florence-2024-11-05:1', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(returned_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if the `trained_model` output is available\n",
    "job_name = returned_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. (Optional) Create model asset and get fine-tuned LLM to local folder\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    model_name,\n",
    "    job_name,\n",
    "    model_dir=\"outputs\",\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    "):\n",
    "\n",
    "    try:\n",
    "        latest_model_version = max(\n",
    "            [int(m.version) for m in ml_client.models.list(name=model_name)]\n",
    "        )\n",
    "        if update:\n",
    "            raise ResourceExistsError(\"Found Model asset, but will update the Model.\")\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(\n",
    "                name=model_name, version=latest_model_version\n",
    "            )\n",
    "            print(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        model_path = f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"\n",
    "        run_model = Model(\n",
    "            name=model_name,\n",
    "            path=model_path,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type,  # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        print(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_model_name = d[\"serve\"][\"azure_model_name\"]\n",
    "model_dir = d[\"train\"][\"model_dir\"]\n",
    "model = get_or_create_model_asset(\n",
    "    ml_client,\n",
    "    azure_model_name,\n",
    "    job_name,\n",
    "    model_dir,\n",
    "    model_type=\"custom_model\",\n",
    "    update=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Get fine-tuned LLM to local folder\n",
    "\n",
    "You can copy it to your local directory to perform inference or serve the model in Azure environment. (e.g., real-time endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {local_model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model (this is optional)\n",
    "local_model_dir = \"./artifact_downloads\"\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "ml_client.models.download(\n",
    "    name=azure_model_name, download_path=local_model_dir, version=model.version\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
