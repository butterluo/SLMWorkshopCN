{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Get started to validate the setup\n",
    "\n",
    "This Jupyter notebook is recommended for workshop/education only.\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "1. Set up your computing environment\n",
    "2. Install the required library in your Python environment\n",
    "3. Select the correct kernel (`azureml_py310_sdkv2`) for your Jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Azure Open AI Test\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: python31014jvsc74a57bd01f90a0206bde5cf3732dab79adbbcc7570d5fab64b89fc69d46a8fe33664a709\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/BT/SRC/NLP/LLM/SFT/SLMWorkshopCN/.env\n",
      "https://cog-pgwgybluulpec.openai.azure.com/\n",
      "=== Initialized AzuureOpenAI client ===\n",
      "AZURE_OPENAI_ENDPOINT=https://cog-pgwgybluulpec.openai.azure.com/\n",
      "AZURE_OPENAI_API_VERSION=2024-07-01-preview\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "envpath = find_dotenv()\n",
    "load_dotenv(envpath)\n",
    "print(envpath)\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "print(aoai_api_endpoint)\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "if not aoai_api_version:\n",
    "    aoai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "if not aoai_deployment_name:\n",
    "    aoai_deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = aoai_api_endpoint,\n",
    "        api_key        = aoai_api_key,\n",
    "        api_version    = aoai_api_version\n",
    "    )\n",
    "    deployment_name = aoai_deployment_name\n",
    "    print(\"=== Initialized AzuureOpenAI client ===\")\n",
    "    print(f\"AZURE_OPENAI_ENDPOINT={aoai_api_endpoint}\")\n",
    "    print(f\"AZURE_OPENAI_API_VERSION={aoai_api_version}\")\n",
    "    print(f\"AZURE_OPENAI_DEPLOYMENT_NAME={aoai_deployment_name}\")   \n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Alpine Explorer Tent boasts several impressive features:\n",
      "\n",
      "- **Detachable Partition**: Ensures privacy for occupants. \n",
      "- **Mesh Windows**: Allows for excellent ventilation while keeping bugs out. \n",
      "- **Adjustable Vents**: Helps to control airflow for added comfort. \n",
      "- **Waterproof Design**: Keeps you dry in wet conditions. \n",
      "- **Built-In Gear Loft**: Perfect for storing your outdoor essentials.\n",
      "\n",
      "In short, it’s a cozy and convenient second home in nature! 🏕️\n",
      "\n",
      "By the way, why did the tent break up with the backpack? \n",
      "\n",
      "It felt too much pressure! 😂\n"
     ]
    }
   ],
   "source": [
    "# Create your prompt\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant that helps customers find information. As an assistant, you respond to questions in a concise and unique manner.\n",
    "You can use Markdown to answer simply and concisely, and add a personal touch with appropriate emojis.\n",
    "\n",
    "Add a witty joke starting with \"By the way,\" at the end of your response. Do not mention the customer's name in the joke part.\n",
    "The joke should be related to the specific question asked.\n",
    "For example, if the question is about tents, the joke should be specifically related to tents.\n",
    "\n",
    "Use the given context to provide a more personalized response. Write each sentence on a new line:\n",
    "\"\"\"\n",
    "context = \"\"\"\n",
    "    The Alpine Explorer Tent features a detachable partition to ensure privacy, \n",
    "    numerous mesh windows and adjustable vents for ventilation, and a waterproof design. \n",
    "    It also includes a built-in gear loft for storing outdoor essentials. \n",
    "    In short, it offers a harmonious blend of privacy, comfort, and convenience, making it a second home in nature!\n",
    "\"\"\"\n",
    "question = \"What are features of the Alpine Explorer Tent?\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "  temperature=0.7,\n",
    "  max_tokens=300\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) 2. Azure Document Inteligence Test\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initialized DocumentIntelligenceClient ===\n",
      "AZURE_DOC_INTELLIGENCE_ENDPOINT=https://cog-di-pgwgybluulpec.cognitiveservices.azure.com/\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\")\n",
    "\n",
    "try:\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=doc_intelligence_endpoint, \n",
    "        credential=AzureKeyCredential(doc_intelligence_key),\n",
    "        headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    "    )\n",
    "    print(\"=== Initialized DocumentIntelligenceClient ===\")\n",
    "    print(f\"AZURE_DOC_INTELLIGENCE_ENDPOINT={doc_intelligence_endpoint}\")    \n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)\n",
    "    \n",
    "raw_data_dir = \"../1_synthetic-qa-generation/raw_data\"\n",
    "file_path = f\"{raw_data_dir}/pdf/en-imagenet-training-wrote-by-daekeun.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      "\n",
      "<figure>\n",
      "</figure>\n",
      "\n",
      "\n",
      "# [Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\n",
      "\n",
      "Author: Daekeun Kim (daekeun@amazon.com)\n",
      "\n",
      "\n",
      "## Goal\n",
      "\n",
      "This document is for people who need distributed GPU training using Horovod for\n",
      "experimental purposes. Many steps are similar to what mentioned in Julien\n",
      "Simon's article (https://medium.com/@julsimon/imagenet-part-1-going-on-an-\n",
      "adventure-c0a62976dc72) and AWS\n",
      "\n",
      "Documentation(https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-\n",
      "horovod-tensorflow.html). So I recommend you to view these articles first. If there\n",
      "are some things that aren't going well (e.g., Downloading the dataset does not\n",
      "work, How to convert the raw data to the TFRecord feature set?, How to fix the\n",
      "error ModuleNotFoundError: No module named 'cv2'? ) please refer this\n",
      "document.\n",
      "\n",
      "\n",
      "## Introduction\n",
      "\n",
      "For data preparation and data transformation, we do not need to use a GPU\n",
      "instance such as p2 and p3. Instead, we can start much cheaper instances like\n",
      "t2. large instance with 1.0TB EBS volume.\n",
      "\n",
      "For distributed training, we need to use multiple GPU instances like p2, p3, g3 and\n",
      "g4.\n",
      "\n",
      "You can skip step 1 if you do not want to invent the wheel again because I have\n",
      "stored everything in my s3 bucket.\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"1/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      "· s3://dataset-image/ imagenet/raw (raw jpeg)\n",
      "\n",
      "· s3://dataset-image/imagenet/tfrecord (TFRecord before resizing)\n",
      "\n",
      "· s3://dataset-image/imagenet/tfrecord-resized (TFRecord after resizing to\n",
      "224x224)\n",
      "\n",
      "· s3://dataset-image/imagenet/recordio (RecordIO after resizing to\n",
      "256×256)\n",
      "\n",
      "o The reason I did not resize 224x224 is that the below article shows\n",
      "different validation accuracy for resizing strategy.\n",
      "\n",
      "· https://forums.fast.ai/t/impact-of-image-resizing-on-model-training-time-\n",
      "and-performance/1980\n",
      "\n",
      "Please let me know if you want to access the bucket because I did not grant any\n",
      "public access.\n",
      "\n",
      "\n",
      "## Step 1. Downloading and Transformation\n",
      "\n",
      "\n",
      "### Setting up an EC2 instance for Data Transformation\n",
      "\n",
      "· Create an EC2 instance for storing ImageNet dataset (Ubuntu 18.04 or 16.04.\n",
      "Linux is also available). t2. micro is also available, but t2. large is\n",
      "recommended due to memory size. Note that we do not need large storage\n",
      "size since we will make another EBS volume to attach the EC2 instance.\n",
      "\n",
      ". Create an EBS volume (1.0TB) for ImageNet dataset and then attach the\n",
      "volume it to your EC2 instance. ImageNet consists of 138GB for training set\n",
      "and 6.3GB for validation set, but we need an additional space since we need to\n",
      "extract tar files as well as need to transform it to the feature sets like TFRecord\n",
      "and RecordIO. Here is an example command using AWS CLI.\n",
      "\n",
      "$ aws ec2 create-volume | -- size 1000 | -- region\n",
      "[YOUR_AWS_REGION] \\ -- availability-zone [YOUR_AZ> \\ -- volume-\n",
      "type sc1 \\ -- tag-specifications 'ResourceType=volume, Tags=\n",
      "[{Key=Name, Value=ImageNet}]' $ aws ec2 attach-volume | -- volume-\n",
      "id vol-[YOUR_EC2_volume_id] \\ -- instance-id i-\n",
      "[YOUR_EC2_instance_id] \\ -- device /dev/sdf\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"2/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      ". Format the EBS volume, mount it on /data , and then change the owner to\n",
      "ec2-user: ec2-user . You may refer to\n",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-\n",
      "volumes.html if you do not know how to mount it.\n",
      "\n",
      ". Download MXNet repository and TensorFlow models repository.\n",
      "\n",
      "\\$ cd /data \\$ git clone https://github.com/tensorflow/models.git\n",
      "$ git clone https://github.com/apache/incubator-mxnet.git # or,\n",
      "you can just type `pip install mxnet'\n",
      "\n",
      "· [Optional] For your convenience, use symbolic link such that:\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th colspan=\"2\">[ec2-user@ip-172-31-34-246 data]$ ls -1</th>\n",
      "<th></th>\n",
      "<th colspan=\"3\" rowspan=\"2\"></th>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>합계 13864</th>\n",
      "<th></th>\n",
      "<th></th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x 4 ec2-user ec2-user</td>\n",
      "<td>37</td>\n",
      "<td>9월</td>\n",
      "<td colspan=\"3\">16 02:20 im2rec</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Lrwxrwxrwx 1 ec2-user ec2-user</td>\n",
      "<td>41</td>\n",
      "<td>9월</td>\n",
      "<td>10</td>\n",
      "<td colspan=\"2\">02:38 imagenet -&gt; /data/models/research/inception/inception</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x 7 ec2-user ec2-user</td>\n",
      "<td>249</td>\n",
      "<td>9%</td>\n",
      "<td colspan=\"3\">10 02:22 models</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Lrwxrwxrwx 1 ec2-user ec2-user</td>\n",
      "<td>44</td>\n",
      "<td>9월</td>\n",
      "<td>16</td>\n",
      "<td colspan=\"2\">00:29 mxnet -&gt; /usr/local/lib/python2.7/site-packages/mxnet</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw ------- 1 ec2-user ec2-user</td>\n",
      "<td>14193041</td>\n",
      "<td>9월</td>\n",
      "<td>17</td>\n",
      "<td colspan=\"2\">23:46 nohup . out</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x 4 ec2-user ec2-user</td>\n",
      "<td>98</td>\n",
      "<td>9월</td>\n",
      "<td>16</td>\n",
      "<td>00:52</td>\n",
      "<td>opencv</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">[ec2-user@ip-172-31-34-246 data]$</td>\n",
      "<td></td>\n",
      "<td colspan=\"3\"></td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      ". [Important Step] You need to install OpenCV also. (Both 3.x and 4.x work well).\n",
      "If you do not install OpenCV, then you cannot convert ImageNet raw data to\n",
      "RecordIO files since im2rec. py utilizes some OpenCV functions. You may\n",
      "refer to https://www.pyimagesearch.com/2018/08/15/how-to-install-opencv-\n",
      "4-on-ubuntu/.\n",
      "\n",
      ". [Caution] I strongly recommend to use Python2 instead of Python3 because\n",
      "many codes of Tensorflow models repository does not work on Python3.\n",
      "Please refer to https://stackoverflow.com/questions/38546672/inception-\n",
      "build-imagenet-data-py-typeerror-rgb-has-type-class-str-but-ex.\n",
      "\n",
      "\n",
      "### Downloading ImageNet\n",
      "\n",
      "Please note that ImageNet server is sometimes unstable so download speed is not\n",
      "fast, taking 4 to 5 days.\n",
      "\n",
      "\n",
      "### Method 1\n",
      "\n",
      "· Go to http://www.image-net.org/, sign up, and get your own username and\n",
      "access key.\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"3/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      ". You can use the TensorFlow's download\n",
      "script(https://github.com/tensorflow/models/blob/master/research/inception/in\n",
      "ception/data/download_imagenet.sh) by exporting your username and access\n",
      "key.\n",
      "\n",
      "\\$ export IMAGENET_USERNAME=[YOUR_USERNAME] \\$ export\n",
      "IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY] \\$ cd imagenet/data \\$ mv\n",
      "imagenet_2012_validation_synset_labels. txt synsets. txt $ nohup\n",
      "bash download_imagenet.sh . synsets. txt >& download. log &\n",
      "\n",
      "\n",
      "### Method 2 (Alternative method if Method 1 does not work)\n",
      "\n",
      ". Download ImageNet dataset manually.\n",
      "\n",
      "$ nohup wget http://www.image-\n",
      "net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar & $\n",
      "nohup wget http://www.image-\n",
      "net. org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_train_v2.tar.\n",
      "& $ nohup wget http://www.image-\n",
      "net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val. tar & $\n",
      "nohup wget http://www.image-\n",
      "net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_val_v3.tgz &\n",
      "\n",
      "· Extract the validation set\n",
      "\n",
      "\\$ mkdir validation \\$ mv ILSVRC2012_img_val. tar validation $ cd\n",
      "validation $ tar xf ILSVRC2012_img_val. tar\n",
      "\n",
      "· After extracting the validation set, move jpeg\n",
      "files(ILSVRC2012_val_00000001.JPEG, ... ,\n",
      "ILSVRC2012_val_00050000.JPEG) in 1,000 directories using the following\n",
      "script;\n",
      "https://github.com/juliensimon/aws/blob/master/mxnet/imagenet/build_vali\n",
      "dation_tree.sh (Each directory means the unique category like ).\n",
      "n01728572\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"4/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      ". Extract the training set\n",
      "\n",
      "\\$ mkdir train \\$ mv ILSVRC2012_img_train. tar train \\$ cd train \\$\n",
      "tar xf ILSVRC2012_img_train. tar $ find . - name \" *. tar\" | while\n",
      "read NAME ; do mkdir -p \"\\${NAME%. tar}\"; tar -xvf \"\\$ {NAME}\" -C\n",
      "\"$ {NAME%. tar}\"; rm -f \"$ {NAME}\"; done\n",
      "\n",
      "· After extracting the training set, check if the number of directories is 1,000\n",
      "(class 1 is n01728572 and class 1000 is n15075141).\n",
      "\n",
      "· Extract bounding boxes\n",
      "\n",
      "\\$ mkdir bounding_boxes \\$ mv ILSVRC2012_bbox_train_v2.tar.gz\n",
      "bounding_boxes \\$ mv ILSVRC2012_bbox_val_v3.tgz bounding_boxes \\$\n",
      "cd bounding_boxes \\$ tar xzf ILSVRC2012_bbox_val_v3.tgz \\$ mkdir\n",
      "train \\$ mv ILSVRC2012_bbox_train_v2. tar.gz train \\$ cd train $\n",
      "tar xzf ILSVRC2012_bbox_train_v2.tar.gz\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>-rw-rw-r --</th>\n",
      "<th>1</th>\n",
      "<th>ec2-user</th>\n",
      "<th>ec2-user</th>\n",
      "<th>19537608</th>\n",
      "<th>10 월</th>\n",
      "<th>1</th>\n",
      "<th>00:01</th>\n",
      "<th>ILSVRC2012_bbox_train_v2.tar.gz</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>2221290</td>\n",
      "<td>9%</td>\n",
      "<td>18</td>\n",
      "<td>2012</td>\n",
      "<td>ILSVRC2012_bbox_val_v3. tgz</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>147897477120</td>\n",
      "<td>6%</td>\n",
      "<td>14</td>\n",
      "<td>2012</td>\n",
      "<td>ILSVRC2012_img_train. tar</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>6744924160</td>\n",
      "<td>6%</td>\n",
      "<td>14</td>\n",
      "<td>2012</td>\n",
      "<td>ILSVRC2012_img_val. tar</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x</td>\n",
      "<td>1002</td>\n",
      "<td>ec2-user</td>\n",
      "<td>ec2-user</td>\n",
      "<td>32768</td>\n",
      "<td>9%</td>\n",
      "<td>17</td>\n",
      "<td>01:36</td>\n",
      "<td>bounding_boxes</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-r -- r --</td>\n",
      "<td>1</td>\n",
      "<td>root</td>\n",
      "<td>root</td>\n",
      "<td>29709928</td>\n",
      "<td>9%</td>\n",
      "<td>17</td>\n",
      "<td>06:02</td>\n",
      "<td>imagenet_2012_bounding_boxes. csv</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x</td>\n",
      "<td>1002</td>\n",
      "<td>ec2-user</td>\n",
      "<td colspan=\"2\">ec2-user 32768</td>\n",
      "<td>9월</td>\n",
      "<td>17</td>\n",
      "<td>02:29</td>\n",
      "<td>train</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>drwxrwxr-x</td>\n",
      "<td>1002</td>\n",
      "<td>ec2-user</td>\n",
      "<td colspan=\"2\">ec2-user 2691072</td>\n",
      "<td>9%</td>\n",
      "<td>17</td>\n",
      "<td>05:54</td>\n",
      "<td>validation</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"5\">(base) [ec2-user@ip-172-31-35-5 data]$</td>\n",
      "<td></td>\n",
      "<td colspan=\"3\"></td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "### Data Transformation\n",
      "\n",
      "\n",
      "#### RecordIO format\n",
      "\n",
      ". Use im2rec. py the same way Simon did.\n",
      "(https://medium.com/@julsimon/imagenet-part-1-going-on-an-adventure-\n",
      "c0a62976dc72). It takes 1.5 days on the t2. large instance. I think he did\n",
      "some typos (ImageNet baseline usually uses 224x224 size image, but he uses\n",
      "480×480).\n",
      "\n",
      "\n",
      "#### TFRecord format\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"5/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      "· Please refer to https://github.com/aws-samples/deep-learning-\n",
      "models/blob/master/utils/tensorflow/preprocess_imagenet.py (code) and\n",
      "https://docs.aws.amazon.com/ko_kr/dlami/latest/devguide/tutorial-horovod-\n",
      "tensorflow.html (document).\n",
      "\n",
      "\n",
      "<figure>\n",
      "\n",
      "o python preprocess_imagenet.py | -- local_scratch_dir= [YOUR\n",
      "DIRECTORY] \\ -- imagenet_username=[imagenet account] | --\n",
      "imagenet_access_key=[imagenet access key]\n",
      "\n",
      "o python tensorflow_image_resizer.py \\ -d imagenet \\ -i [PATH TO\n",
      "TFRECORD TRAINING DATASET] \\ -o [PATH TO RESIZED TFRECORD TRAINING\n",
      "DATASET] \\ -- subset_name train \\ -- num_preprocess_threads 60 | --\n",
      "num_intra_threads 2 \\ -- num_inter_threads 2\n",
      "\n",
      "</figure>\n",
      "\n",
      "\n",
      ". [Additional Notes] The original document uses the small number of intra-\n",
      "op(multiple threads within one op; for example, while doing matrix\n",
      "multiplication operation we can divide the op by multiple threads) and inter-\n",
      "op(thread-pool size per one executor) such that -num_intra_threads 2 \\ -\n",
      "-num_inter_threads 2 . But, you can give higher number of intra-op and inter-\n",
      "op.\n",
      "\n",
      "\n",
      "### Backing up and Copying to S3\n",
      "\n",
      ". After data transformation, create a new bucket and sync or copy feature sets\n",
      "to the bucket.\n",
      "\n",
      ". Create a snapshot of the EBS volume.\n",
      "\n",
      "\n",
      "### Step 2. Training ResNet-50 Model with Horovod\n",
      "\n",
      "[Before get started] If you just want to train on a single machine, you may refer to\n",
      "https://medium.com/@julsimon/imagenet-part-2-the-road-goes-ever-on-and-on-\n",
      "578f09a749f9 (RecordIO) and\n",
      "\n",
      "https://github.com/tensorflow/models/tree/master/official/r1/resnet (TFRecord)\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"6/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      "· Create an EC2 instance for Training (Deep Learning AMI (Ubuntu 16.04) or\n",
      "Deep Learning AMI (Amazon Linux)). p3.16xlarge or p3dn. 24xlarge is\n",
      "recommended if you need to do distributed GPU training using Uber's\n",
      "Horovod or Tensorflow's DistributedStrategy). Please also note that the\n",
      "default root volume size is 75GB, but I recommend you to increase 100GB\n",
      "since training logs and model checkpoints are stored in the root volume if you\n",
      "do not modify training configuration. If you not want to increase the volume\n",
      "size, then you can delete some conda environments such as Theano, Chainer,\n",
      "Caffe, and Caffe2 after logging in to the EC2 instance.\n",
      "\n",
      "· https://aws.amazon.com/ko/getting-started/tutorials/get-started-dlami/\n",
      "\n",
      ". If you want to train on distributed GPUs, then you need to create multiple GPU\n",
      "instances with the same setting. For example, the below figure shows 8\n",
      "p3dn. 24xlarge instances.\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th colspan=\"6\">Q ☒ search : POC_HU24 Add filter</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>☐</th>\n",
      "<th>Name</th>\n",
      "<th>Instance ID</th>\n",
      "<th>Instance Type</th>\n",
      "<th>Availability Zone</th>\n",
      "<th>Instance State</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_81</td>\n",
      "<td>i-089bb90e91fef7b09</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_82</td>\n",
      "<td>i-09be131f79506dcc1</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_83</td>\n",
      "<td>i-0c44553f8570af264</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_84</td>\n",
      "<td>i-0d8f1a29d7864e892</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_85</td>\n",
      "<td>i-0de84adf899462171</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_86</td>\n",
      "<td>i-0e56678cc29ad0de8</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_87</td>\n",
      "<td>i-Of4912fb1d7760a1b</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>☐</td>\n",
      "<td>POC_HU24_88</td>\n",
      "<td>i-Of4ddd1c5bbfd4dd7</td>\n",
      "<td>p3dn.24xlarge</td>\n",
      "<td>us-west-2c</td>\n",
      "<td>☒ running</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      ". Please refer to the website for the remaining steps;\n",
      "https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod-\n",
      "tensorflow.html. Note that all code and all feature sets(TFRecord and\n",
      "RecordIO) must be on the same path on each server.\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"7/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      ". After training, please check the training log and evaluation log by checking\n",
      "imagenet_resnet folder:\n",
      "\n",
      "\n",
      "<figure>\n",
      "</figure>\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<td colspan=\"2\">...</td>\n",
      "<td colspan=\"3\">ubuntu@ip-172-31-3-51: ~ /examples/horovod/tensorflow/imagenet_resnet (ssh)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"5\">ubuntu@ip-172-31-3-51 :~ /examples/horovod/tensorflow/imagenet_resnet$ 1s -1 total 646280</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">8 -rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td colspan=\"3\">89 Sep 23 02:54 checkpoint</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">-rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td>18880</td>\n",
      "<td colspan=\"2\">Oct 1 01:22 eval_hvd_train. log</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">-rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td>21227745</td>\n",
      "<td colspan=\"2\">Sep 23 03:01 events. out. tfevents . 1569199858. ip-172-31-3-51</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"2\">-rw-rw-r -- 1 ubuntu ubuntu</td>\n",
      "<td>9287777</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>graph. pbtxt</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>18880</td>\n",
      "<td>Sep 23 03:03</td>\n",
      "<td>hvd_train. log</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>8</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model.ckpt-0.data-00000-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>204668736</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model. ckpt-0.data-00001-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>17114</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model. ckpt-0. index</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>5709416</td>\n",
      "<td>Sep 23 00:51</td>\n",
      "<td>model. ckpt-0.meta</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>8</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model.ckpt-10000.data-00000-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>204668736</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model. ckpt-10000.data-00001-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>17114</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model. ckpt-10000. index</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>5709416</td>\n",
      "<td>Sep 23 01:53</td>\n",
      "<td>model. ckpt-10000.meta</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>8</td>\n",
      "<td>Sep 23 02:54</td>\n",
      "<td>model.ckpt-20000.data-00000-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>204668736</td>\n",
      "<td>Sep 23 02:54</td>\n",
      "<td>model. ckpt-20000.data-00001-of-00002</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>17114</td>\n",
      "<td colspan=\"2\">Sep 23 02:54 model. ckpt-20000. index</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>-rw-rw-r --</td>\n",
      "<td>1 ubuntu ubuntu</td>\n",
      "<td>5709416</td>\n",
      "<td colspan=\"2\">Sep 23 02:54 model. ckpt-20000.meta</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"5\">ubuntu@ip-172-31-3-51 :~ /examples/horovod/tensorflow/imagenet_resnet$</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "· vd_train_log (32 GPUS; 4 p3dn.24xlarge instances)\n",
      "\n",
      "\\- Step Epoch Speed Loss FinLoss LR - 0 0.0 952.2 6.923 8.262\n",
      "0.00100 - 1 0.0 2686.6 6.928 8.267 0.00305 - 50 0.3 22243.7\n",
      "\n",
      "6.586 7.919 0.10353 - .. - 14000 89.5 21021.1 0.750 1.152\n",
      "\n",
      "0.00012 - 14050 89.8 21818.7 0.583 0.985 0.00002 - Finished in\n",
      "5289.161954164505\n",
      "\n",
      "· eval_hvd_train.log (32 GPUS; 4 p3dn.24xlarge instances)\n",
      "\n",
      "ubuntu@ip-172-31-3-51 :~ /examples/horovod/tensorflow$ cat eval_hvd_train_gpu32. log\n",
      "PY3.6.5 |Anaconda, Inc. I (default, Apr 29 2018, 16:14:56)\n",
      "\n",
      "[GCC 7.2.0]TF1.13.1\n",
      "\n",
      "Horovod size: 8\n",
      "\n",
      "Using data from: /home/ubuntu/data1/tf-imagenet/\n",
      "Evaluating\n",
      "\n",
      "Validation dataset size: 50000\n",
      "\n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>step</th>\n",
      "<th>epoch</th>\n",
      "<th>top1</th>\n",
      "<th>top5</th>\n",
      "<th>loss</th>\n",
      "<th>checkpoint_time(UTC)</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>14075</td>\n",
      "<td>90.2</td>\n",
      "<td>75.821</td>\n",
      "<td>92.90</td>\n",
      "<td>0.92</td>\n",
      "<td>2019-09-20 07:50:57</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td colspan=\"6\">Finished evaluation</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"8/9\" -->\n",
      "<!-- PageBreak -->\n",
      "\n",
      "<!-- PageHeader=\"24. 7. 22. 오전 9:52\" -->\n",
      "<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\n",
      "\n",
      "· hvd_train_log (64 GPUS; 8 p3dn.24xlarge instances)\n",
      "\n",
      "\\- Step Epoch Speed Loss FinLoss LR - 0 0.0 1907.3 6.920 8.259\n",
      "0.00100 - 1 0.0 5164.9 6.935 8.274 0.00920 - 50 0.6 43926.5\n",
      "6.206 7.522 0.41119 - ... - 6950 88.9 43552.2 0.783 1.185\n",
      "0.00125 - 7000 89.5 41958.4 0.624 1.027 0.00023 - Finished in\n",
      "2685.1825189590454\n",
      "\n",
      "· eval_hvd_train.log (64 GPUS; 8 p3dn.24xlarge instances)\n",
      "\n",
      "<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\n",
      "<!-- PageNumber=\"9/9\" -->\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"rb\") as f:\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", \n",
    "        output_content_format=ContentFormat.MARKDOWN \n",
    "    )\n",
    "\n",
    "result = poller.result()\n",
    "md_content = result.content\n",
    "print(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Azure ML Test\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_SUBSCRIPTION_ID=49aee8bf-3f02-464f-a0ba-e3467e7d85e2\n",
      "AZURE_RESOURCE_GROUP=rg-slmwrkshp_9\n",
      "AZURE_WORKSPACE=mlw-pgwgybluulpec\n",
      "AZURE_DATA_NAME=lgds-sftdemo241201\n",
      "DATA_DIR=./dataset\n",
      "CLOUD_DIR=./cloud\n",
      "HF_MODEL_NAME_OR_PATH=microsoft/Phi-3.5-mini-instruct\n",
      "IS_DEBUG=True\n",
      "USE_LOWPRIORITY_VM=False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open('../2_slm-fine-tuning-mlstudio/phi3/config_prd.yml') as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
    "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
    "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
    "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
    "DATA_DIR = d['config']['DATA_DIR']\n",
    "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
    "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
    "IS_DEBUG = d['config']['IS_DEBUG']\n",
    "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
    "\n",
    "\n",
    "print(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "print(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "print(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "print(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
    "print(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "print(f\"IS_DEBUG={IS_DEBUG}\")\n",
    "print(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Azure ML Workspace: mlw-pgwgybluulpec\n",
      "Workspace Location: eastus\n",
      "Workspace ID: /subscriptions/49aee8bf-3f02-464f-a0ba-e3467e7d85e2/resourceGroups/rg-slmwrkshp_9/providers/Microsoft.MachineLearningServices/workspaces/mlw-pgwgybluulpec\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)\n",
    "\n",
    "# from azure.identity import ClientSecretCredential\n",
    "# credentials = ClientSecretCredential(\n",
    "#     client_id=client_id,\n",
    "#     client_secret=client_secret,\n",
    "#     tenant_id=tenant_id\n",
    "# )\n",
    "\n",
    "try:\n",
    "    workspace = ml_client.workspaces.get(name=AZURE_WORKSPACE)\n",
    "    print(f\"Connected to Azure ML Workspace: {workspace.name}\")\n",
    "    print(f\"Workspace Location: {workspace.location}\")\n",
    "    print(f\"Workspace ID: {workspace.id}\")\n",
    "except HttpResponseError as e:\n",
    "    print(f\"Failed to connect to Azure ML Workspace: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
